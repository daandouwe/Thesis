<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>lisa-mail.md - Grip</title>
  <link rel="icon" href="/__/grip/static/favicon.ico" />
  <link rel="stylesheet" href="/__/grip/asset/frameworks-df973073d880f28fbbae0263fb1ef62b.css" />
  <link rel="stylesheet" href="/__/grip/asset/github-2b520d809bcf76c745c815d9523f0a00.css" />
  <link rel="stylesheet" href="/__/grip/asset/site-0be82e42e6ce84ef34fecbf8469a45aa.css" />
  <link rel="stylesheet" href="/__/grip/static/octicons/octicons.css" />
  <style>
    /* Page tweaks */
    .preview-page {
      margin-top: 64px;
    }
    /* User-content tweaks */
    .timeline-comment-wrapper > .timeline-comment:after,
    .timeline-comment-wrapper > .timeline-comment:before {
      content: none;
    }
    /* User-content overrides */
    .discussion-timeline.wide {
      width: 920px;
    }
  </style>
</head>
<body>
  <div class="page">
    <div id="preview-page" class="preview-page" data-autorefresh-url="/__/grip/refresh/">

    

      <div role="main" class="main-content">
        <div class="container new-discussion-timeline experiment-repo-nav">
          <div class="repository-content">
            <div id="readme" class="readme boxed-group clearfix announce instapaper_body md">
              
                <h3>
                  <span class="octicon octicon-book"></span>
                  lisa-mail.md
                </h3>
              
              <article class="markdown-body entry-content" itemprop="text" id="grip-content">
                <p>Dear Lisa,</p>
<p>I am working on a PyTorch application (python) that I want to parallelize over (CPU) several nodes. (On CPU, because it has complicated serial computation that GPU offers no benefit for.) Currenlty I have code that works on one node. I want it to work on mutliple nodes. For example: spawn 32 processes on 2 nodes: 16 on one node and 16 on the other, and communicate as if it were just 32 on one node. Can this be done?</p>
<p>This is a pretty long and maybe complicated/confused question. I've tried to write it out clearly, but I could also come by for a short appointment if that is easier?</p>
<p><code>Note</code> I'm using PyTorch v0.4.0 that I installed myself using pip: <code>pip install --user torch torchvision</code> after <code>module load Python/3.6.3-foss-2017b</code>.</p>
<h2>
<a id="user-content-single-node" class="anchor" href="#single-node" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Single-node</h2>
<p>Currently, I have code that works on one node. It is completely synchronous. It spawns a number of processes, each of these computes gradients on a different example, and these gradients are then communicated and averaged.</p>
<p>My code looks roughly like this, and is adapted from this <a href="https://pytorch.org/tutorials/intermediate/dist_tuto.html" rel="nofollow">this PyTorch tutorial</a>. The code is self-sufficient, I hope, and I've teste it on a single Lisa node.</p>
<div class="highlight highlight-source-python"><pre><span class="pl-c"><span class="pl-c">#</span>!/usr/bin/env python</span>
<span class="pl-s"><span class="pl-pds">"""</span>run.py:<span class="pl-pds">"""</span></span>
<span class="pl-k">import</span> os

<span class="pl-k">import</span> torch
<span class="pl-k">import</span> torch.nn <span class="pl-k">as</span> nn
<span class="pl-k">from</span> torch.autograd <span class="pl-k">import</span> Variable
<span class="pl-k">import</span> torch.distributed <span class="pl-k">as</span> dist
<span class="pl-k">import</span> torch.nn.functional <span class="pl-k">as</span> F
<span class="pl-k">from</span> torch.multiprocessing <span class="pl-k">import</span> Process


<span class="pl-k">def</span> <span class="pl-en">run</span>(<span class="pl-smi">rank</span>, <span class="pl-smi">size</span>):
    <span class="pl-s"><span class="pl-pds">"""</span> Distributed Synchronous SGD Example <span class="pl-pds">"""</span></span>
    torch.manual_seed(<span class="pl-c1">1234</span>)
    <span class="pl-c"><span class="pl-c">#</span> Restrict the number of threads that each process uses.</span>
    torch.set_num_threads(<span class="pl-c1">1</span>)

    <span class="pl-c"><span class="pl-c">#</span> Random objective.</span>
    data, target <span class="pl-k">=</span> Variable(torch.ones(<span class="pl-c1">1</span>, <span class="pl-c1">500</span>).uniform_()), Variable(torch.ones(<span class="pl-c1">1</span>, <span class="pl-c1">500</span>).uniform_())
    <span class="pl-c"><span class="pl-c">#</span> Dummy model.</span>
    model <span class="pl-k">=</span> nn.Linear(<span class="pl-c1">500</span>, <span class="pl-c1">500</span>)  <span class="pl-c"><span class="pl-c">#</span> y = Wx + b</span>

    optimizer <span class="pl-k">=</span> torch.optim.SGD(model.parameters(),
                          <span class="pl-v">lr</span><span class="pl-k">=</span><span class="pl-c1">0.01</span>, <span class="pl-v">momentum</span><span class="pl-k">=</span><span class="pl-c1">0.5</span>)

    <span class="pl-k">for</span> epoch <span class="pl-k">in</span> <span class="pl-c1">range</span>(<span class="pl-c1">100000</span>):
        output <span class="pl-k">=</span> model(data)
        <span class="pl-c"><span class="pl-c">#</span> MSE loss.</span>
        loss <span class="pl-k">=</span> torch.mean((output <span class="pl-k">-</span> target)<span class="pl-k">**</span><span class="pl-c1">2</span>)

        optimizer.zero_grad()
        loss.backward()
        average_gradients(model)
        optimizer.step()
        <span class="pl-k">if</span> rank <span class="pl-k">==</span> <span class="pl-c1">0</span> <span class="pl-k">and</span> epoch <span class="pl-k">%</span> <span class="pl-c1">100</span> <span class="pl-k">==</span> <span class="pl-c1">0</span>:
            <span class="pl-c1">print</span>(<span class="pl-s">f</span><span class="pl-pds">'</span><span class="pl-s">Epoch </span><span class="pl-c1">{</span>epoch<span class="pl-k">:,</span><span class="pl-c1">}</span><span class="pl-s">: </span><span class="pl-c1">{</span>loss.item()<span class="pl-k">:.2e</span><span class="pl-c1">}</span><span class="pl-pds">'</span>)


<span class="pl-k">def</span> <span class="pl-en">average_gradients</span>(<span class="pl-smi">model</span>):
    <span class="pl-s"><span class="pl-pds">"""</span> Gradient averaging. <span class="pl-pds">"""</span></span>
    size <span class="pl-k">=</span> <span class="pl-c1">float</span>(dist.get_world_size())
    <span class="pl-k">for</span> param <span class="pl-k">in</span> model.parameters():
        dist.all_reduce(param.grad.data, <span class="pl-v">op</span><span class="pl-k">=</span>dist.reduce_op.<span class="pl-c1">SUM</span>)  <span class="pl-c"><span class="pl-c">#</span> This does the communication.</span>
        param.grad.data <span class="pl-k">/=</span> size


<span class="pl-k">def</span> <span class="pl-en">init_processes</span>(<span class="pl-smi">rank</span>, <span class="pl-smi">size</span>, <span class="pl-smi">fn</span>, <span class="pl-smi">backend</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>tcp<span class="pl-pds">'</span></span>):
    <span class="pl-s"><span class="pl-pds">"""</span> Initialize the distributed environment. <span class="pl-pds">"""</span></span>
    os.environ[<span class="pl-s"><span class="pl-pds">'</span>MASTER_ADDR<span class="pl-pds">'</span></span>] <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">'</span>127.0.0.1<span class="pl-pds">'</span></span>
    os.environ[<span class="pl-s"><span class="pl-pds">'</span>MASTER_PORT<span class="pl-pds">'</span></span>] <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">'</span>29500<span class="pl-pds">'</span></span>
    dist.init_process_group(backend, <span class="pl-v">rank</span><span class="pl-k">=</span>rank, <span class="pl-v">world_size</span><span class="pl-k">=</span>size)
    fn(rank, size)


<span class="pl-k">if</span> <span class="pl-c1">__name__</span> <span class="pl-k">==</span> <span class="pl-s"><span class="pl-pds">"</span>__main__<span class="pl-pds">"</span></span>:
    size <span class="pl-k">=</span> <span class="pl-c1">2</span>
    processes <span class="pl-k">=</span> []
    <span class="pl-k">for</span> rank <span class="pl-k">in</span> <span class="pl-c1">range</span>(size):
        p <span class="pl-k">=</span> Process(<span class="pl-v">target</span><span class="pl-k">=</span>init_processes, <span class="pl-v">args</span><span class="pl-k">=</span>(rank, size, run))
        p.start()
        processes.append(p)

    <span class="pl-k">for</span> p <span class="pl-k">in</span> processes:
        p.join()</pre></div>
<p>There is on side-question I have about this:</p>
<ol>
<li>I figured I should call only 16 processes, given that there are 16 cores on one CPU node. But I also tested with 32 processes, and this makes it almost twice as fast. How should I reason about this? The examples are of varying sizes (they are sentences), so some processes stop much earlier, and are left idle while waiting for the largest example to finish. Maybe this time is then filled with the other processes? How do I know the maximum number of processes to call? Experimentally?</li>
<li>Which <code>backends</code> are do you advice me to use on Lisa? I am using <code>tcp</code>, but I noticed that <code>mpi</code> is also supported when I <code>module load PyTorch/0.3.0-foss-2017b-Python-3.6.3-CUDA-9.0.176</code>. However, I prefer to use PyTorch 4, in which case mpi is not available, unless I compile from source with extra flags and options, which I'd like to avoid doing.</li>
<li>What are <code>export MASTER_ADDR=127.0.0.1</code> and <code>export MASTER_PORT=29500</code> used for? What values to choose for this? These numbers are totally out of the blue for me. Which values to use on Lisa?</li>
</ol>
<h2>
<a id="user-content-multi-node" class="anchor" href="#multi-node" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Multi-node</h2>
<p>I want to make the above work on multiple nodes.</p>
<p>I have a feeling that the key to this is in <code>init_processes</code>, and especially <code>dist.init_process_group</code>, correct? Unfortunately, I don't understand it at all. I'm reading the PyTorch documentation, but there is too much that I am completely ignorant of, like the IP adresses and Port and more. Pytorch also provides a <a href="https://pytorch.org/docs/master/distributed.html#launch-utility" rel="nofollow">launch utility</a>. There is an example for multinode training in the link, but I have a hard time understanding it:</p>
<ol>
<li>Again, what IP-address and port to use on Lisa?</li>
<li>How do I submit code to each of the different nodes, and make them communicate?</li>
</ol>
<h2>
<a id="user-content-sources" class="anchor" href="#sources" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Sources</h2>
<p>The following is a list of relevant PyTorch sources relating to this question:</p>
<ul>
<li>
<code>torch.distributed</code> package documentation: <a href="https://pytorch.org/docs/master/distributed.html" rel="nofollow">https://pytorch.org/docs/master/distributed.html</a>
</li>
<li>
<code>torch.distributed.init_process_group()</code> documentation: <a href="https://pytorch.org/docs/master/distributed.html#initialization" rel="nofollow">https://pytorch.org/docs/master/distributed.html#initialization</a>
</li>
<li>
<code>torch.distributed</code> launch utility documentation: <a href="https://pytorch.org/docs/master/distributed.html#launch-utility" rel="nofollow">https://pytorch.org/docs/master/distributed.html#launch-utility</a>
</li>
<li>Distributed implementations tutorial: <a href="https://pytorch.org/tutorials/intermediate/dist_tuto.html" rel="nofollow">https://pytorch.org/tutorials/intermediate/dist_tuto.html</a>
</li>
</ul>
<p>I hope my questions are clear and I hope that you can help me with this!</p>
<p>Best,
Daan van Stigt, ILLC
username: daanvans</p>

              </article>
            </div>
          </div>
        </div>
      </div>

    

  </div>
  <div>&nbsp;</div>
  </div><script>
    function showCanonicalImages() {
      var images = document.getElementsByTagName('img');
      if (!images) {
        return;
      }
      for (var index = 0; index < images.length; index++) {
        var image = images[index];
        if (image.getAttribute('data-canonical-src') && image.src !== image.getAttribute('data-canonical-src')) {
          image.src = image.getAttribute('data-canonical-src');
        }
      }
    }

    function scrollToHash() {
      if (location.hash && !document.querySelector(':target')) {
        var element = document.getElementById('user-content-' + location.hash.slice(1));
        if (element) {
           element.scrollIntoView();
        }
      }
    }

    function autorefreshContent(eventSourceUrl) {
      var initialTitle = document.title;
      var contentElement = document.getElementById('grip-content');
      var source = new EventSource(eventSourceUrl);
      var isRendering = false;

      source.onmessage = function(ev) {
        var msg = JSON.parse(ev.data);
        if (msg.updating) {
          isRendering = true;
          document.title = '(Rendering) ' + document.title;
        } else {
          isRendering = false;
          document.title = initialTitle;
          contentElement.innerHTML = msg.content;
          showCanonicalImages();
        }
      }

      source.onerror = function(e) {
        if (e.readyState === EventSource.CLOSED && isRendering) {
          isRendering = false;
          document.title = initialTitle;
        }
      }
    }

    window.onhashchange = function() {
      scrollToHash();
    }

    window.onload = function() {
      scrollToHash();
    }

    showCanonicalImages();

    var autorefreshUrl = document.getElementById('preview-page').getAttribute('data-autorefresh-url');
    if (autorefreshUrl) {
      autorefreshContent(autorefreshUrl);
    }
  </script>
</body>
</html>