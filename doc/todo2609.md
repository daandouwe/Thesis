# Todo list

## Data handling
- [X] Move `get_sent_dict` and `get_sentences` to a more sensible place, like `util.py`

## Syntax
- Train on unlabeled trees.
  - [ ] Remove labels during data loading.
  - [ ] Do not predict nonterminal class in forward.

## Generative
- [ ] Integrate softmax approximation.
- [ ] Decoding: implement importance sampling for inference (parsing and language modelling).
- [ ] Compute perplexity for language model.

## Bugs
Why get `nan` after some steps?

Found: Nan is not a result of parallel training! Also with simple training we get nan:
```
| step   2100/ 5001 (42%) | loss  25.000 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h10m33s | eta 0h14m35s
| step   2200/ 5001 (44%) | loss  23.869 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h11m00s | eta 0h14m01s
| step   2300/ 5001 (46%) | loss  24.049 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h11m29s | eta 0h13m29s
| step   2400/ 5001 (48%) | loss  24.770 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h11m59s | eta 0h12m59s
| step   2500/ 5001 (50%) | loss  25.254 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h12m30s | eta 0h12m30s
| step   2600/ 5001 (52%) | loss  19.592 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h12m56s | eta 0h11m56s
| step   2700/ 5001 (54%) | loss  26.543 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h13m29s | eta 0h11m29s
| step   2800/ 5001 (56%) | loss     nan | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h14m01s | eta 0h11m01s
| step   2900/ 5001 (58%) | loss     nan | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h14m30s | eta 0h10m30s
| step   3000/ 5001 (60%) | loss     nan | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h15m03s | eta 0h10m02s
```

## Training
- [ ] Redesign training into a trainer class. Look into others for inspiration like https://github.com/allenai/allennlp/blob/master/allennlp/training/trainer.py.
- [X] Redesign folder structure etc. I'm really annoyed with the way it is now.

## Prediction
- [X] Remove parse from model
- [X] Prediction with `GreedyDecoder`.

## Loss functions
- [X] Smoothly integrate ELBO objectives into training (with KL annealing)
- [X] Integrate temperature for discrete training.

## Experiment
- [ ] Run full discriminative (16 sents/sec = 45 mins ==> 10 epochs ~ 5 hours).
- [ ] Run full generative model.

## Latent variables
- [ ] Get latent-factors composition to work.
- [ ] Work on latent-attention.
