# Todo list

## Memory
Highest priority!
- [ ] Memory bug still there!
- [X] Use pympler (no resolve).
- [ ] Look into this lead: [Slight memory leak for LSTM](https://github.com/pytorch/pytorch/issues/3665).

## Parallel
- [ ] Email surfsara to figure out parallel and why no speed-up.
- [ ] Send them small example code.
- [ ] Want: parallel batches 16, 32, 48 (CPU), 4, 8, 12 (GPU).

## Generative RNNG
If memory issue solved: make generative.
- [ ] I have everything setup to have this done in two days. If only memory issue were solved!

## Softmax approx
- [ ] Implement Barber approx for neural N-gram

## Stochastic decoder
- [ ] Finish stochastic rnn language model.
- [ ] Replace history encoder with stochastic rnn.

## Discrete latent variables
- [ ] Replace Normal latent distribution with binary latent variable (inference with Concrete)

## Goal
Our goal: a (discrete) latent variable (generative) RNNG.
### Parsing
Experiment Disc/Gen RNNG +/- stochastic decoder +/- discrete variables:
- Parsing accuracy?
- Interpretation: what can we examine from (discrete) latent states?
### Parsing induction
Train on bracketed but unlabeled trees following [What Do RNNGs Learn About Syntax?](http://aclweb.org/anthology/E17-1117) and inspect representations produced by the composition function.
- [ ] Discrete latent states offer better interpretation: n binary values, so 2^n possible codes. Maybe we use this to discover per-dimension representations? No t-SNE projection, but per dimension inspection, and some combinations?
- [ ] Gated attention for composition function: make attention stochastic. Maybe even sparse? This forces hard head-rules.
### Language model
Experiment Gen RNNG +/- stochastic decoder +/- discrete variables:
- [ ] Perplexity?
- [ ] Evaluate on syntax-sensitive dependencies, like number agreement
### Brains
- [ ] Really cool ideas, but somewhat out of topic for this thesis.
- [ ] But: keep looking in this area for interesting linguistically motivated evaluation experiments: types of syntactic phenomena, weird sentences.
