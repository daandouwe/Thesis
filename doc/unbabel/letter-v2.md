# Cover letter Unbabel - Summer AI Research Internship

[Opening statement]

## 1. Why I want to work at Unbabel

Unbabel is precisely the company I want to work for. [Describe why...] I been following your company for some months now, and was very joyed to learn of the internship opportunity that you offer this summer. The internship was brought to my attention by my thesis supervisor Wilker Aziz, who was so very kind to mention me to his academic collaborator Andre Martins. I am convinced that I provide just the right skills to make this internship a success and hope that by the end of this letter you will think so too.

## 2. What I work on now
I am a master's student at the Institute for Logic Language and Computation at the University of Amsterdam. I am writing a master's thesis under supervision of Prof. Wilker Aziz, in which I investigate the question: What are effective ways of incorporating syntactic structure into neural language models?

I study a class of neural language models that explicitly model the hierarchical syntactic structure in addition to the sequence of words.

These models merges algorithms from transition-based parsing adapted for joint (generative) modeling, with (recurrent) neural networks to parametrize the transition model. Although these are fundamentally joint models, they can be evaluated as regular language models, modeling only words, by marginalizing over the latent syntactic structure.

I focus on one model in particular: Recurrent Neural Network Grammars (RNNG) as proposed by [Dyer et al. 2016]. Although exact marginalization is intractable for the RNNG, due to the rich parametrization of the statistical model, importance sampling provides an effective approximate method.

In my thesis I consider the RNNG and investigate:

1. The impact of the proposal samples on the approximate marginalization.

I propose a new discriminative chart-based neural parser that is trained with a global, Conditional Random Field (CRF), objective. I hypothesize that a globally trained model makes a more suitable proposal distribution than a locally trained transition based model. A global model has ready access to competing analyses that can be structurally very dissimilar but nonetheless close in probability, whereas we hypothesize that a locally trained model is more prone to produce locally corrupted structures that are nearby in transition-space.

2. Semi-supervised training by including unlabeled data.

To make these joint models competitive language models they need to make use of the vast amounts of unlabeled data that exists. I extend the training to unlabeled data by optimizing a variational lower bound on the marginal probability, which jointly optimizes the parameters of proposal model (also named the 'posterior' in this framework) and the joint model. Gradients for this objective are obtained using the score function estimator [Fu 2006], also known as REINFORCE [Williams 1992], and we introduce an effective baseline based on argmax decoding [Rennie et al. 2017], which significantly reduces the variance in this optimization procedure.

3. Alternative, simpler, models.

I investigate the added value of  syntax as a discrete latent variable by comparing the model with a robust and competitive alternative: multitask learning of a regular neural language model with a syntactic side objective. I propose a novel, minimal syntactic side objective inspired by the CRF parser's feature function, that is surprisingly successful.

4. Targeted syntactic evaluation.

I believe that to answer a subtle question we need subtle metrics. Perplexity, the usual performance metric language models, conflates multiple sources for success in a language model, and syntactically challenging sentences are relatively infrequent.
Instead we probe the syntactic abilities of language models by their grammatical acceptability judgements, a line of evaluation recently proposed by Linzen et al. Given two minimally differing sentences, the task is to assign a higher probability to the grammatical one. The dataset is consists of constructions generated by manually designed context free grammars and cover a range of syntactic phenomena.


## 3. What I have worked on before
<!-- This is repeating my CV! -->
<!-- I am currently writing a thesis for the MSc in Logic at the University of Amsterdam, where I followed courses on Theoretical Computer Science, Machine Learning and Natural Language Processing. I hold a BA in Liberal Arts and Sciences from the Amsterdam University College, a joint undergraduate college by the University of Amsterdam (UvA) and the Free University Amsterdam (VU), where I studied Philosophy and Linguistics. Between the BA and the MSc I partially completed a BSc in Mathematics. In the Master program I ventured into Theoretical Computer Science and Discrete Mathematics, after which I rejoined with my interest in language and linguistics through Machine Learning and especially Natural Language Processing (courses in the program of Artificial Intelligence). -->

[Briefly mention relevant courses]

I learned a lot from working on individual projects:

1. Graph-based and transition-based dependency parsing with different learning algorithms (neural network features, and a structured perceptron on manually defined features).

2. Latent Dirichlet Allocation with different inference methods: collapsed Gibbs sampling; (stochastic) Variational Inference; and amortized Varitional Inference (using a neural network).

3. A number of didactic projects, implemented with teaching in mind: CKY parsing with rules and probabilities estimated from a treebank; ngram language models with smoothing; neural ngram lanuage models; word embeddings in various ways (GloVe, SVD on a (weighted) cooccurence matrix).

In my projects I make regular use of the great libraries that the Python ecosystem has to offer: Dynet and PyTorch, and earlier some Tensorflow, wherever I need neural networks; Cython and Numpy for speed and general numerical computation; multiprocessing and joblib whenever I see the opportunity for parallel computation; matplotlib, seaborn, and pandas wherever I want to inspect, manipulate and visualize data.

I strive to make these projects well-organized, well-documented and self-contained and I increasingly learn to appreciate the role of bash scripts in this: to obtain and process data in transparent ways, to standardize training and evaluation, and to promote transparency and reproducibility in the process. I believe that the code for my thesis is the culmination of this learning process.

I have had the amazing opportunity to teach a large group of students at the master level in a course-long research project. The project had them implement state of the art NLP techniques from scratch, and write a report on their experimental findings - an experience that was a first for plenty of students. I supervised and taught almost entirely by myself; a bold initiative from my side that was rewarded with great satisfaction and immense experience. The student's appreciation of my efforts was evident in their consistent attendance to the weekly, non-compulsory, meetings, their impressive results, and the enthusiastic (anonymous) feedback I received afterwards. This experience has taught me that I have it in me to be a great teacher.


<!-- This is in my CV! -->
I have experience teaching students from the MSc in Artificial Intelligence. In the first year master's course Natural Language Processing 1, I supervised student projects in which they implemented a neural network graph-based dependency parser from scratch in Python using PyTorch. This was a semester-long project with 30 students that met twice per week. I prepared and presented mini-lectures on deep learning and dependency parsing, prepared code examples to teach PyTorch, and co-graded the final reports. Besides the projects I contributed to the course materials by designing programming practicals which where handed out in Jupyter Notebook. In teaching I was supported by Joost Bastings and the course was taught by Tejaswini Deoskar (see academic references in CV).


## 4. What future work interests me


[Closing statement]
