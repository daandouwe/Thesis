# Todo list

## Memory
Figure out the memory problem.
- [X] Use pympler.
 * It's not the classes Word, Item, Action, or Node.
 * The number of torch.Tensors increases each minibatch computation.

## Parallel
- [X] Email surfsara to figure out parallel and why no speed-up.

## Generative
If memory issue solved and parallel is working: make generative.

## Gated attention for composition
- [X] Add attention to the composition function
- [X] Make sure attention vectors can be accessed easily for visualization during decoding.

## Data
- [ ] Make data reading more streamlined for other data than the constituency ptb.
  * Get rid of `--name-template`!

## Decode
- [X] Make decode work with tree input.
- [X] Make decode score tree input.

## The beam search paper
- [X] Read it again and try to get it.
- [X] Try to see if we can easily recreate it: is the brain-data online?
  * Data is online at https://sites.lsa.umich.edu/cnllab/2016/06/11/data-sharing-fmri-timecourses-story-listening/
  * Understand how to use and read the TextGrid data.
- [ ] Implement the word-synchronous beam search decoder.
- [ ] Setup decoder so necessary embeddings can be easily stored.
- [ ] For regression: bayesian linear regression with scikit-learn or PyMC3 :).

## Fancy stuff
Switching linear dynamical systems (SLDS): https://github.com/mattjj/pyslds
- [ ] Use as analysis for learned representations of nn parser: apply to the sequence of representations and infer discrete states for SLDS.
  * Interesting in combination with the beam-search paper: more fancy statistical analysis than linear regression.
- [ ] Use SLDS encoder instead of stack LSTM. Options:
  * Fix all embeddings, then encoding is latent state of SLDS. External word embedding is data.
  * Use SLDS to approx. the representations produced by trained RNNG.

## Encoder
- [ ] Replace StackLSTM with 'StackTansformer'
  * Easy for buffer encoder
  * Understand sequential Transformer for Stack and History LSTM

## Dependency
Make a version that works for dependency trees. Need to change:
- [ ] Get oracles from conll file (maybe spacy?)
- [ ] Data reading from oracles is very much the same
- [ ] Change actions: SHIFT, RIGHT, LEFT, (with labels)
  * `RIGHT(nsubj`), with `nsubj = Label(nsubj)`, where `Label` is like `Nonterminals` and `RIGHT` is a like `NT`.
- Change composition function from rnn to mlp (page 338 Dyer et al. 2015):
- [ ] F1 eval using eval.pl.
- [ ] Reading and writing predicted trees to conll file with Joost's code.
