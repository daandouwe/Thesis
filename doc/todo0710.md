# Todo list

## Autobatching
There are two PyTorch add-ons that can help do autobatching:
- [TorchFold](https://github.com/nearai/torchfold)
- [Matchbox](https://github.com/salesforce/matchbox)
Open a new branch and try to use one of the two tools to make your code autobatcheable. Try to see if this can be done in 1-2 days.

## Semi-supervised
Three ways of training the RNNG semi-supervised have come to my attention:
- Wake-sleep following [A Structured Variational Autoencoder for Contextual Morphological Inflection](https://arxiv.org/pdf/1806.03746.pdf)
- Score-gradient with baselines following [StructVAE](https://arxiv.org/abs/1806.07832)
- Score-gradient with baselines following [A Generative Parser with a Discriminative Recognition Algorithm](https://arxiv.org/pdf/1708.00415.pdf)
- Both score-gradient methods refer to [Neural Variational Inference for Text Processing](https://arxiv.org/pdf/1511.06038.pdf)).
- RNNG mentions: "Finally, although we considered only the supervised learning scenario, RNNGs are joint models that could be trained without trees, for example, using expectation maximization."

## Latent Z
W can let the discriminative model be:
```
p(t|x) = \int p(z|x) p(t|x,z) dz
```
How is `z` incorporated into the discriminative parser? Let with `a = oracle(t)`, then the model is
```
p(t|x,z) = \prod p(a_i|a_{<i}, z).
```
We can choose to let `u' = cat(u, z)` where u is the feature vector of the stack. Or let `u' = u + z`?

(What if we use ```p(t|x) = \int p(z|x) p(t|z) dz```?)

### Prior
We learn can learn the prior:
```
p(z|x) = N(z|mu(x), sigma(x)^2)
```
Or a mixture prior.
```
p(z|x) = \sum_{i=1}^K N(z|mu_i(x), sigma_i(x)^2)
```

### Inference
We do posterior inference with a tree encoder
```
q(z|x, t) = N(z|mu(x,t), sigma(x,t)^2)
```

### Posterior sampling
We can do posterior sampling in another way now:
Compute `p(z|x)`, then sample `z ~ p(z|x)`. Then decode greedily with `p(t|x,z)`. Drawing different samples `z` will expect to give different trees.


## Discrete latent-variables
Fix `LatentFactorsComposition` so we do not get negative KL (which is a obvious bug.)

## Bugs
Why get `nan` after some steps? Found: Nan is not a result of parallel training! Also with simple training we get nan:
```
| step   2100/ 5001 (42%) | loss  25.000 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h10m33s | eta 0h14m35s
| step   2200/ 5001 (44%) | loss  23.869 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h11m00s | eta 0h14m01s
| step   2300/ 5001 (46%) | loss  24.049 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h11m29s | eta 0h13m29s
| step   2400/ 5001 (48%) | loss  24.770 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h11m59s | eta 0h12m59s
| step   2500/ 5001 (50%) | loss  25.254 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h12m30s | eta 0h12m30s
| step   2600/ 5001 (52%) | loss  19.592 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h12m56s | eta 0h11m56s
| step   2700/ 5001 (54%) | loss  26.543 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h13m29s | eta 0h11m29s
| step   2800/ 5001 (56%) | loss     nan | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h14m01s | eta 0h11m01s
| step   2900/ 5001 (58%) | loss     nan | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h14m30s | eta 0h10m30s
| step   3000/ 5001 (60%) | loss     nan | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h15m03s | eta 0h10m02s
```

## Cleanup
Keep cleaning up trainer class.
