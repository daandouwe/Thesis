# Todo list

## Autobatching
There are two PyTorch add-ons that can help do autobatching:
- [TorchFold](https://github.com/nearai/torchfold)
- [Matchbox](https://github.com/salesforce/matchbox)
Open a new branch and try to use one of the two tools to make your code autobatcheable. Try to see if this can be done in 1-2 days.

## Semi-supervised
Three ways of training the RNNG semi-supervised have come to my attention:
- Wake-sleep following [A Structured Variational Autoencoder for Contextual Morphological Inflection](https://arxiv.org/pdf/1806.03746.pdf)
- Score-gradient with baselines following [StructVAE](https://arxiv.org/abs/1806.07832)
- Score-gradient with baselines following [A Generative Parser with a Discriminative Recognition Algorithm](https://arxiv.org/pdf/1708.00415.pdf)
- Both score-gradient methods refer to [Neural Variational Inference for Text Processing](https://arxiv.org/pdf/1511.06038.pdf)).

## Discrete latent-variables
Fix `LatentFactorsComposition` so we do not get negative KL (which is a obvious bug.)

## Bugs
Why get `nan` after some steps?

Found: Nan is not a result of parallel training! Also with simple training we get nan:
```
| step   2100/ 5001 (42%) | loss  25.000 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h10m33s | eta 0h14m35s
| step   2200/ 5001 (44%) | loss  23.869 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h11m00s | eta 0h14m01s
| step   2300/ 5001 (46%) | loss  24.049 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h11m29s | eta 0h13m29s
| step   2400/ 5001 (48%) | loss  24.770 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h11m59s | eta 0h12m59s
| step   2500/ 5001 (50%) | loss  25.254 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h12m30s | eta 0h12m30s
| step   2600/ 5001 (52%) | loss  19.592 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h12m56s | eta 0h11m56s
| step   2700/ 5001 (54%) | loss  26.543 | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h13m29s | eta 0h11m29s
| step   2800/ 5001 (56%) | loss     nan | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h14m01s | eta 0h11m01s
| step   2900/ 5001 (58%) | loss     nan | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h14m30s | eta 0h10m30s
| step   3000/ 5001 (60%) | loss     nan | lr 8.0e-04 |  3.3 sents/sec | elapsed 0h15m03s | eta 0h10m02s
```

Why does distributed not work inside the `Trainer` class?
