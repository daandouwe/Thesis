Perhaps the most basic yet profound task in probabilistic modelling of language is to assign probabilities to sentences---such probability distribution is called a language model. This thesis studies how such distributions can be extended to incorporate that which is not observed in words alone: the sentence's syntactic structure.

In this thesis we are interested in distributions that assign probabilities $p(\x, \y)$ to \textit{pairs} of observations---both the sentence $x$ and its syntactic structure $y$. Rules of probability then gives us a language model for free because for any joint probability distribution
\begin{align*}
  p(\x) = \sum_{\y \in \yieldx} p(\x, \y).
\end{align*}
`For free', because for any combinatorial structure of any interest the sum over $y$ will be daunting, and can generally be computed exactly only for models that factorize $p(\x, \y)$ along significant independence assumptions. Approximations are  in place when $p$ is too expressive. The above marginalization is the core subject of this thesis: the spread of probability $p(\x)$ over the many probabilities $p(\x, \y)$, each describing how sentence $\x$ and structure $\y$ cooccur. How does this spread make $p$ a better model of sentences $\x$? How can we approximate the sum over $\y$ when the model $p$ is too expressive? And how can we obtain estimate probabilities $p(\x, \y)$ when only $\x$ is ever observed?

We ask these questions for one joint model in particular: the recurrent neural network grammar (RNNG) \citep{dyer2016rnng}. The RNNG models this joint distribution as a sequential generation process that generates words together with their phrase structure, merging generative transition-based parsing with recurrent neural networks (RNNs). The model factorizes the joint probability as the product of probabilities of actions in a shift-reduce parser. It makes no independence assumptions about this sequential process: at each step, the model can condition on the entire derivation constructed thus far, which is summarized by a syntax-dependent RNN.

As a joint model, the RNNG also defines a language model by marginalization of the structured variable $\y$, but the lack of independence assumptions precludes the efficient computation of this sum, a phenomenon that we alluded to above. Importance sampling provides a tractable alternative, approximating this sum with the help of a separately trained discriminative proposal model $q(\y \mid \x)$ that provides samples of trees $\y$ for the sentence $\x$. The proposal model acts as an approximation to the intractable true posterior $p(y \mid x)$. \citet{dyer2016rnng} show how a discrminative formulation of the RNNG can be used for this role. Supervised RNNGs are trained on labeled data, requiring both the sentence $\x$ and the its syntactic analysis $\y$ to be known, for which the Penn Treebank \citep{marcus1993penn} is used. They are a strong model of language that can outperform RNN language models in terms of perplexity \citep{dyer2016rnng} and on a targeted syntactic agreement task \citep{kuncoro2018learn}.

The approximate marginalization is central in the application of the RNNG as language model, and the supervised learning requires labeled data. In this thesis we study this marginalization, to see if we can move beyond labeled data. Our central contribution is the introduction of a neural Conditional Random Field (CRF) constituency parser that can act as an alternative approximate posterior for the RNNG. The CRF parser can be used as proposal model for a trained RNNG, but we also experiment with the CRF as approximtate posterior in variational learning of the RNNG, in which we jointly learn the RNNG and CRF by optimizing a lower bound on the marginal likelihood. This opens the door to semisupervised and unsupervised learning of the RNNG. The CRF formulation of the parser allows the exact computation of key quantities involved in the computation of the lower bound, and the global normalization provides a robust distribution for the sampling based gradient estimation.

To evaluate how the joint formulation differentiates the RNNG from neural language models that model only $\x$ we perform targeted syntactic evaluation of these models on the dataset of \citep{linzen2018targeted}. A competitive alternative to the full joint model of the RNNG are RNN language models that receive syntactic supervision during training in the form of multitask learning. We compare the RNNG to regular RNN language models, and RNN language models trained with multitask learning.

The organization of this thesis is as follows.

\begin{description}
  \item[Chapter \ref{02-background}]
    In chapter 2 we introduce the background to this thesis. We describe the fundamentals of phrase structure, motivating why we would need it for a characterization of language. We describe syntactic parsing, emphasizing the difference between globally normalized and locally normalized models. Similarly, we describe language modelling, emphasizing relevant related models. We conclude with a review of the neural networks used in this thesis.

  \item[Chapter \ref{03-rnng}]
    In this chapter we review the RNNG. We describe the exact probabilistic model, the neural parametrization, the training, and the approximate inference. We report results with our own implementation, and analyze the approximate inference with the discriminative RNNG.

  \item[Chapter \ref{04-crf}]
    This chapter contains our core contribution: the neural CRF parser. We present a span factored neural CRF parser, by borrowing the span factored approach and neural scoring function from \citet{stern2017minimal} and deriving custom exact inference algorithms from general inside and outside recursions \citep{goodman1999semiring}. We show how these algorithms are used to solve a number of inference problems, including the computation of the entropy, which plays an important role in chapter \ref{05-semisupervised}. We describe a slight caveat in the way our model deals with $n$-ary trees which leads to derivational ambiguity, and provide solutions in the form of alternative inference algorithms. The derivational ambiguity is not an problem for the application of the CRF as a parser, but does become one in its application as posterior distribution in the approximate inference.

  \item[Chapter \ref{05-semisupervised}]
    In this chapter we adress the semisupervised and unsupervised learning of the RNNG, focussing on the CRF as approximate posterior. We derive a variational lower bound on the marginal probability and show how this can be optimized by sampling based gradient estimation. We perform experiments with semisupervised, and unsupervised objectives, for labeled and unlabeled trees, but a full exploration of the CRF in this role is halted by the derivational ambiguity. Preliminary results with unlabeled trees suggest the potential of this approach for unsupervised $n$-ary tree induction, and we formulate future work towards this goal.


  \item[Chapter \ref{06-syneval}]
    In the final chapter we perform syntactic evaluation using the dataset of \citet{linzen2018targeted}. We compare the supervised RNNG with RNN language models that are trained with a syntactic side objective, a type of multitask learning. We propose a novel side objective inspired by the scoring function of the CRF.

\end{description}


Our implementations are available at \url{https://github.com/daandouwe/thesis}.
