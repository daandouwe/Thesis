\bibliography{../src/bibliography.bib}

I investigate the question: What are effective ways of incorporating syntactic structure into neural language models?

Central in this research is a class of neural language models that explicitly model the hierarchical syntactic structure in addition to the sequence of words \citep{Dyer+2016:RNNG}, \cite{Buys+2015:neural-gen-dep} \cite{Buys+2018}. These models merge algorithms from transition based parsing ('shift-reduce parsing') adapted for joint (generative) modeling, with (recurrent) neural networks that parametrize the transition model.

The syntactic structure that decorates the words can be latent, and marginalized over, or can be given explicitly, for example as the prediction of an external parser. Although these are fundamentally joint model, they can be evaluated as regular language models (modeling only words) by (approximate) marginalization of the syntactic structure. In the case of the RNNG (Dyer et al. 2016), exact marginalization is intractable due to the parametrization of the statistical model, but importance sampling provides an effective approximate method. An externally trained discriminative parser is used to obtain proposal samples. Other models provide exact marginalization, but this typically comes at the cost of a less expressive parametrization, for example one in which the features cannot be structure-dependent (Buys \& Blunsom 2018).

I study the RNNG and investigate:

\paragraph{The impact of the proposal samples on the approximate marginalization}
I propose a new discriminative chart-based neural parser that is trained with a global, Conditional Random Field (CRF), objective. The parser is an adaptation of the minimal neural parser proposed in (Stern et al. 2017) which is trained with a margin-based objective. This contrast with the typical choice for a transition-based parser as proposal (a discrminatively trained RNNG). The rationale in this research is that we posit that a globally trained model is a better proposal distribution than a locally trained transition based model. A global model has ready access to competing analyses that can be structurally dissimilar but close in probability, whereas we hypothesize that a locally trained model is prone to produce locally corrupted structures that are nearby in transition-space. To promote more diverse samples, the transition distributions are flattened, causing as a downside for the model to visit . This is a general challenge for greedy transition based models that is typically answered to train dynamic oracles (Golberg \& Nivre 2012) (also called 'exploration' (Ballesteros 2016; Stern et al. 2017), instances of imitation learning (Vlachos 2012; Eisner et al. 2012)), a direction which we do not consider in this research.

\paragraph{Semi-supervised training by including unlabeled data}
A major drawback of these syntactic language models is that they require annotated data to be trained, and preciously little of such data exists. To make these joint models competitive language models they need to make use of the vast amounts of unlabeled data that exists. We extend the training to the unsupervised domain by optimizing a variational lower bound on the marginal probabilities that jointly optimizes the parameters of proposal model ('posterior' in this framework) with the joint model. We obtain gradients for this objective using the score function estimator (Fu 2006), also known as REINFORCE (Williams 1992), which is widely used in the field of deep reinforcement learning, and we introduce an effective baseline based on argmax decoding (Rennie et al. 2017), which significantly reduces the variance in this optimization procedure. Our CRF parser particularly excels in the role of posterior thanks the independence assumptions that allow for efficient exact computation of key quantities: the entropy term in the lower bound can be computed exactly using Inside-Outside algorithm, removing one source of variance from the gradient estimation, and the argmax decoding can be performed exactly thanks to Viterbi, making the argmax baseline even more effective.

\paragraph{Alternative, simpler, models}
Multitask learning of a neural language model with a syntactic side objective is a competitive and robust alternative method to infuse neural language models with syntactic knowledge. Training the syntactic model on data that mixes gold trees with predicted 'silver' trees for unlabeled data is a competitive and robust alternative to fully principled semi-supervised learning. We consider these alternatives in order to quantify significance of the latent structure, and the semisupervised training on the other hand, as measured by some external performance metric. We propose a simple multitask neural language model that predicts labeled spans from the RNN hidden states, using a feature function identical identical to that used in the CRF parser. A similar strategy has recently proposed in work on semantic parsing and is called a `syntactic scaffold'
(Swayamdipta et al. 2018).

\paragraph{Targeted syntactic evaluation}
TBA
