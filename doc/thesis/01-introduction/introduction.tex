Perhaps the most basic yet profound task in probabilistic modelling of language is to assign probabilities to sentences---such probability distribution is called a language model. This thesis studies how such distributions can be extended to incorporate that which is not observed in words alone: the sentence's syntactic structure.

In this thesis we are interested in distributions that assign probabilities $p(\x, \y)$ to \textit{pairs} of observations---both the sentence $x$ and its syntactic structure $y$. Rules of probability then gives us a language model for free because for any joint probability distribution
\begin{align*}
  p(\x) = \sum_{\y \in \yieldx} p(\x, \y).
\end{align*}
`For free', because for any combinatorial structure of any interest the sum over $y$ will be daunting, and can generally be computed exactly only for models that factorize $p(\x, \y)$ along significant independence assumptions. Approximations are  in place when $p$ is too expressive. The above marginalization is the core subject of this thesis: the spread of probability $p(\x)$ over the many probabilities $p(\x, \y)$, each describing how sentence $\x$ and structure $\y$ cooccur. How does this spread make $p$ a better model of sentences $\x$? How can we approximate the sum over $\y$ when the model $p$ is too expressive? And how can we estimate probabilities $p(\x, \y)$ when only $\x$ is ever observed?

We ask these questions for one joint model in particular: the recurrent neural network grammar (RNNG) \citep{dyer2016rnng}. The RNNG models this joint distribution as a sequential generation process that generates words together with their phrase structure. It merges generative transition-based parsing with recurrent neural networks (RNNs), factorizing the joint probability as the product of probabilities of actions in a transition system that builds trees top-down. It makes no independence assumptions about this sequential process: at each step, the model can condition on the entire derivation constructed thus far, which is summarized by a syntax-dependent RNN.

As a joint model, the RNNG also defines a language model by marginalization of the structured variable $\y$, but the lack of independence assumptions precludes the efficient computation of this sum, an instance of the phenomenon that we alluded to above. Importance sampling provides a tractable alternative, approximating this sum with the help of a separately trained discriminative proposal model $q(\y \mid \x)$ that provides samples of trees $\y$ for the sentence $\x$. The proposal model acts as an approximation to the intractable true posterior $p(y \mid x)$, and \citet{dyer2016rnng} show how a discrminative formulation of the RNNG can be used for this role. Supervised RNNGs are trained on annotated data, requiring both the sentence $\x$ and the its syntactic analysis $\y$ to be known, for which the Penn Treebank \citep{marcus1993penn} is used. They are a strong model of language that can outperform RNN language models in terms of perplexity \citep{dyer2016rnng} and on a targeted syntactic agreement task \citep{kuncoro2018learn}.

The approximate marginalization is central in the application of the RNNG as language model, and the supervised learning requires annotated data. In this thesis we study this marginalization, to see if we can extend estimation to data without annotation.

Our central contribution is the introduction of a neural conditional random field (CRF) constituency parser that can act as an alternative approximate posterior for the RNNG. The CRF parser can be used as proposal model for a trained RNNG, but we also experiment with the CRF as approximtate posterior in variational learning of the RNNG, in which we jointly learn the RNNG and CRF by optimizing a lower bound on the marginal log-likelihood. This opens the door to semisupervised and unsupervised learning of the RNNG. The CRF formulation of the parser allows the exact computation of key quantities involved in the computation of the lower bound, and the global normalization provides a robust distribution for the sampling based gradient estimation.

To evaluate how the joint formulation differentiates the RNNG from neural language models that model only $\x$ we perform targeted syntactic evaluation of these models on the dataset of \citep{linzen2018targeted}. A competitive alternative to the full joint model of the RNNG are RNN language models that receive syntactic supervision during training in the form of multitask learning. We compare the RNNG to these models, as well as to an RNN language that is trained without any additional syntactic supervision.

The organization of this thesis is as follows.

\begin{description}
  \item[Chapter \ref{02-background}]
    In this chapter we describe the background to this thesis. We first describe the fundamentals of phrase structure, motivating why we might need it for a characterization of language. We then describe syntactic parsing, emphasizing the difference between globally and locally normalized models. Similarly, we describe language modelling, emphasizing relevant related models. We conclude with a review of the neural networks used in this thesis.

  \item[Chapter \ref{03-rnng}]
    In this chapter we review the RNNG. We describe the exact probabilistic model, the neural parametrization, the supervised training, and the approximate inference. We report results with our own implementation, and analyze the approximate inference with the discriminative RNNG.

  \item[Chapter \ref{04-crf}]
    This chapter contains part one of our core contribution: the neural CRF parser. We present a neural CRF parser by borrowing the span factored approach and neural scoring function from \citet{stern2017minimal} and by deriving custom exact inference algorithms from general inside and outside recursions \citep{goodman1999semiring}. We show how these algorithms are used to solve a number of inference problems, including sampling and the computation of the entropy, which plays an important role in chapter \ref{05-semisupervised}. We report results of supervised training, showing that it is a strong parser, and proceed to use it in the importance sampling inference of the supervised RNNG. This experiment brings to light a slight caveat in the way our model deals with $n$-ary trees which leads to derivational ambiguity. The derivational ambiguity is not a direct problem for the application of the CRF as a parser, but does become one in its application as posterior distribution in the approximate inference. We provide solutions in the form of alternative inference algorithms, and preliminary results show that these are easy to implement and resolve the ambiguity.

  \item[Chapter \ref{05-semisupervised}]
    This chapter contains part two of our core contribution: we address the semisupervised and unsupervised learning of the RNNG, focussing on the CRF as approximate posterior. We derive a variational lower bound on the marginal log likelihood and show how this can be optimized by sampling based gradient estimation. We perform experiments with semisupervised, and unsupervised objectives, for labeled and unlabeled trees, but a full exploration of the CRF in this role is halted by the derivational ambiguity. Preliminary results with unlabeled trees suggest the potential of this approach for unsupervised $n$-ary tree induction, and we formulate future work towards this goal.

  \item[Chapter \ref{06-syneval}]
    In the final chapter we perform syntactic evaluation using the dataset of \citet{linzen2018targeted}. We compare the supervised RNNG with RNN language models that are trained with a syntactic side objective, a type of multitask learning. We additionally propose a novel side objective inspired by the scoring function of the CRF.

  \item[Conclusion]
    We summarize our work and list our main contributions, and finish with suggestions for future work that departs from where our investigation leaves off.

\end{description}


Our implementations are available at \url{https://github.com/daandouwe/thesis}.
