% \setlength{\epigraphwidth}{4in}
% % \epigraph{Ya se sabe: por una línea razonable o una recta noticia hay leguas de insensatas cacofonías, de fárragos verbales y de incoherencias. \\ \textit{This much is already known: for every sensible line of straighforward statement, there are leagues of senseless cacaphonies, verbal jumbles and incoherences.}}{--- Jorge Luis Borges (\textit{The Library of Babel}; trans. J. E. Irby)}
% \epigraph{\textit{This much is already known: for every sensible line of straighforward statement, there are leagues of senseless cacaphonies, verbal jumbles and incoherences.}}{--- Jorge Luis Borges (\textit{The Library of Babel}; trans. J. E. Irby)}
%
% \setlength{\epigraphwidth}{2.5in}
% \epigraph{\textit{King Lear is just English words put in order!}}{--- David Mitchell}

Perhaps the most basic yet profound task in probabilistic modelling of language is to assign probabilities to sentences---such probability distribution is called a language model. This thesis studies how such distributions can be extended to incorporate that which is not observed in words alone: the sentence's syntactic structure.

% General stuff on sentences, and our assumptions as realizations of random
  % Sentences are everywhere: in books and newspapers, on the Web and in the streets; we utter them and we hear them. I can open any book and find one, turn on the radio and hear one, and look back and see that I have just created one. In fact, this is one right here. One gross simplification is that a sentence is just words put in order---yet there is some truth to it. From a probabilistic perspective, we can consider sentences sequences of words, in order, generated by some unknown random process. But why model them as outcomes a random process? This is a view of linguistic behaviour that few practitioners would ascribe to. I, for instance am currently not rolling dice---maybe weighted to reflect my personal word preference---and writing down the corresponding words from a list.\footnote{This process does create a terribly effective password---take it you use \textit{fair} dice. See \url{http://world.std.com/~reinhold/diceware.html}.} But as an ensemble, the linguistic production of humans taken together makes a probabilistic description more compelling: the first page of an arbitrary Wikipedia is a random observation, letting a book fall open gives another.
  %
  % This idea is made precise in a probabilistic formulation: a sentence $x$ is any sequence of words $\langle x_1, \cdots, x_m \ranlge$, with each from a vocabulary $\mathcal{X}$, and over the space of all those sentences is defined some probability distribution. Given a vocabulary of finite size and a restriction on maximum length, the set of `sentences' in that space is finite---dropping one of these restrictions makes this set countably infinite. Yet with these restrictions, even for modest vocabularies and length restrictions this set is unfathomably large. \footnote{Namely, of size $O(\lvert \matcal{X} \rvert^m)$, exponential in the sentence maximum sentence length $m$ with base the size of the vocabulary.} All possible sentences that can be constructed with the words $\mathcal$ are there.
  %
  % Assuming some fixed vocabulary---say the most frequent English words---we can make the situation tangible with an example.
  %
  % [Tower of babel here]
  %
  % [Example with probabilities of sentences here]
  %
  % Such is the challenge that is faced by a language model: to move the probability mass \textit{away} from those strings that are word soup, and \textit{towards} those that constitute comprehensible sentences. All that based on a number of  observations that is tiny compared to the entire space.

% Start of technical discussion.
  In this thesis we are interested in distributions that assign probabilities $p(\x, \y)$ to \textit{pairs} of observations---both the sentence $x$ and its syntactic structure $y$. Rules of probability then gives us a language model for free because for any joint probability distribution
  \begin{align*}
    p(\x) = \sum_{\y \in \yieldx} p(\x, \y).
  \end{align*}
  `For free', because for any combinatorial structure of any interest the sum over $y$ will be daunting, and can generally be computed exactly only for models that factorize $p(\x, \y)$ along significant independence assumptions. Approximations are  in place when $p$ is too expressive. The above marginalization is the core subject of this thesis: the spread of probability $p(\x)$ over the many probabilities $p(\x, \y)$, each describing how sentence $\x$ and structure $\y$ cooccur. How does this spread make $p$ a better model of sentences $\x$? How can we approximate the sum over $\y$ when the model $p$ is too expressive? And how can we obtain estimate probabilities $p(\x, \y)$ when only $\x$ is ever observed?

  % The statistical model that we have just introduced from the particular case of words and structures is an instance of a more general kind called \textit{generative models}.

  % [Generative models here]

% General introduction, one paragpraph: language modelling, generative models, generative models with syntax,
  % \begin{itemize}
  %   \item Library of Babel Borges example: contains all orderings of letters; only parts of the books contain stories; english language model should assigns high probability to those that are english and low (or none) to the sentences not in English.
  %
  %   \item Generative models are nice, show classifier image?, describe $p(x, y, z, \dots)$ as the true model of the world (sentence $x$, with structure $y$, and meaning $z$, from speaker $a$, to speaker $b$, etc.), and that $p(x) = \sum_y \sum_x \cdots \sum_z p(x, y, z, \dots)$ is the probability in isolation of the sentence. Maybe some example, $p(x, speaker = Wilker) >> p(x, speaker = Daan)$ (maybe something like that?), and $p(x) = \sum_{speaker} p(x, speaker)$. We call $y, z, \dots$ latent variabels: they help expressiveness in the model.
  %
  %   \item We choose one such latent variable: syntactic structure, expressed in constituency trees.
  % \end{itemize}

% RNNG: joint model of words with syntax, one paragraph
  We ask these questions for one joint model in particular: the recurrent neural network grammar (RNNG) \citep{dyer2016rnng}. The RNNG models this joint distribution as a sequential generation process that generates words together with their phrase structure, merging generative transition-based parsing with recurrent neural networks (RNNs). The model factorizes the joint probability as the product of probabilities of actions in a shift-reduce parser. It makes no independence assumptions about this sequential process: at each step, the model can condition on the entire derivation constructed thus far, which is summarized by a syntax-dependent RNN.

  As a joint model, the RNNG also defines a language model by marginalization of the structured variable $\y$, but the lack of independence assumptions precludes the efficient computation of this sum, a phenomenon that we alluded to above. Importance sampling provides a tractable alternative, approximating this sum with the help of a separately trained discriminative proposal model $q(\y \mid \x)$ that provides samples of trees $\y$ for the sentence $\x$. The proposal model acts as an approximation to the intractable true posterior $p(y \mid x)$. \citet{dyer2016rnng} show how a discrminative formulation of the RNNG can be used for this role. Supervised RNNGs are trained on labeled data, requiring both the sentence $\x$ and the its syntactic analysis $\y$ to be known, for which the Penn Treebank \citep{marcus1993penn} is used. [RNNGs are strong language models]

  The approximate marginalization is central in the application of the RNNG as language model, and the supervised learning requires labeled data. In this thesis we study this marginalization, to see if we can move beyond labeled data. Our central contribution is the introduction of a neural Conditional Random Field (CRF) constituency parser that can act as an alternative approximate posterior for the RNNG. The CRF parser can be used as proposal model for a trained RNNG, but we also experiment with the CRF as approximtate posterior in variational learning of the RNNG, in which we jointly learn the RNNG and CRF by optimizing a lower bound on the marginal likelihood. This opens the door to semisupervised and unsupervised learning of the RNNG. The CRF formulation of the parser allows the exact computation of key quantities involved in the computation of the lower bound, and the global normalization provides a robust distribution for the sampling based gradient estimation.

  To evaluate how the joint formulation differentiates the RNNG from neural language models that model only $\x$ we perform targeted syntactic evaluation of these models on the dataset of \citep{linzen2018targeted}. A competitive alternative to the full joint model of the RNNG are RNN language models that receive syntactic supervision during training in the form of multitask learning. We compare the RNNG to regular RNN language models, and RNN language models trained with multitask learning.

  % \paragraph{Neural CRF parser}
  %    % We introduce a neural Conditional Random Field (CRF) constituency parser. The parser is a probabilitic formulation of the margin trained chart parser proposed in \citet{stern2017minimal}, borowing the span factorization and the neural scoring architecture. We derive the inference algorithms required to train and show
  %
  %     % \item This contrast with the choice of \citet{dyer2016rnng} for a transition-based parser as proposal, a discrminatively trained RNNG.
  %
  %     % \item We posit that a globally trained model is a better proposal distribution than a locally trained transition based model: a global model has ready access to competing analyses that can be structurally dissimilar but close in probability, whereas we hypothesize that a locally trained model is prone to produce locally corrupted structures that are nearby in transition-space. In a transition based parser more diverse samples can be obtained by flattening the transition distributions, which causes the model to be less confident in its predictions, but with the downside is that the model now explores parts of the probability space which it has not encountered during training.
  %
  %   \end{itemize}
  %
  % \paragraph{Syntactic evaluation of the RNNG}
  %   We are interested to know whether the joint modelling
  %
  % \paragraph{Semisupervised and unsupervised learning of the RNNG}
  %   The supervised training limits the flexibiliy of Unlike language models, that can use the vast amounts of unlabeled data that exists, the RNNG is limited to those few texts that have been annotated with trees.
  %   \begin{itemize}
  %
  %     \item A major drawback of these syntactic language models is that they require annotated data to be trained, and preciously little of such data exists.
  %
  %     \item We extend the learning to the unsupervised situation, in which the maximize the marginal likelihood $\sum_y p( \x, \y)$ directly. We derive a variational lower bound on the marginal probabilities that jointly optimizes the parameters of proposal model ('posterior' in this framework) with the joint model.
  %
  %     \item We obtain gradients for this objective using the score function estimator \citep{fu2006gradient}, also known as REINFORCE \citep{williams1992reinforce}, which is widely used in the field of deep reinforcement learning, and we introduce an effective baseline based on argmax decoding \citep{rennie2017argmax}, which significantly reduces the variance in this optimization procedure.
  %
  %     \item Our CRF parser particularly excels in the role of posterior thanks the independence assumptions that allow for efficient exact computation of key quantities: the entropy term in the lower bound can be computed exactly using Inside-Outside algorithm, removing one source of variance from the gradient estimation, and the argmax decoding can be performed exactly thanks to Viterbi, making the argmax baseline even more effective.
  %
  %   \end{itemize}
  %
  % \paragraph{Comparison to multitask learning}
  %   There are alternatives to the methods that this thesis investigates.
  %   \begin{itemize}
  %
  %     \item Multitask learning of a neural language model with a syntactic side objective is a competitive and robust alternative method to infuse neural language models with syntactic knowledge.
  %
  %     \item We propose a simple multitask neural language model that predicts labeled spans from the RNN hidden states, using a feature function identical identical to that used in the CRF parser.
  %
  %     \item  and the semisupervised training as measured by some external performance metric.
  %
  %   \end{itemize}

  The organization of this thesis is as follows.

  \begin{description}
    \item[Chapter \ref{02-background}]
      In chapter 2 we introduce the background to this thesis. We describe the fundamentals of phrase structure, motivating why we would need it for a characterization of language. We describe syntactic parsing, emphasizing the difference between globally normalized and locally normalized models. Similarly, we describe language modelling, emphasizing relevant related models. We conclude with a review of the neural networks used in this thesis.

    \item[Chapter \ref{03-rnng}]
      In this chapter we review the RNNG. We describe the exact probabilistic model, the neural parametrization, the training, and the approximate inference. We report results with our own implementation, and analyze the approximate inference with the discriminative RNNG.

    \item[Chapter \ref{04-crf}]
      This chapter contains our core contribution: the neural CRF parser. We present a span factored neural CRF parser, by borrowing the span factored approach and neural scoring function from \citet{stern2017minimal} and deriving custom exact inference algorithms from general inside and outside recursions \citep{goodman1999semiring}. We show how these algorithms are used to solve a number of inference problems, including the computation of the entropy, which is used in the variational inference in \ref{05-semisupervised}. We describe a slight caveat in the way our model deals with $n$-ary trees which leads to derivational ambiguity, and provide solutions in the form of alternative inference algorithms. The derivational ambiguity is not an problem for the application of the CRF as a parser, but does become one in its application as posterior distribution in the approximate inference.

    \item[Chapter \ref{06-syneval}]
      We review the syntactic evaluation dataset of \citep{linzen2018targeted} and use it to evaluate the supervised RNNG. We compare the RNNG with RNN language models that are trained with a syntactic side objective, and propose a novel side objective inspired by the scoring function of the CRF.

    \item[Chapter \ref{05-semisupervised}]
      In the final chapter, we adress the semisupervised and unsupervised learning of the RNNG, focussing on the CRF as approximate posterior. We derive a variational lower bound on the marginal probability and show how this can be optimized by sampling based gradient estimation. We perform experiments with semisupervised, and unsupervised objectives, for labeled and unlabeled trees, but a full exploration of the CRF in this role is halted by the derivational ambiguity. Preliminary results with unlabeled trees suggest the potential of this approach for unsupervised $n$-ary tree induction, and we formulate future work towards this goal.

  \end{description}

% Oranization of chapters

  % The RNNG is an model from a larger class of neural language models that merge generative transition-based parsing with recurrent neural networks \citep{dyer2016rnng,buys2015generative,buys2018exact}. In contrast  Other recent models provide exact marginalization, but this typically comes at the cost of a less expressive parametrization, for example one in which the features cannot depend on the structure of the model \citep{buys2018exact}.

Our implementations are available at \url{https://github.com/daandouwe/thesis}.
