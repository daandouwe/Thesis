% \bibliography{../src/bibliography}

This thesis investigates the question: \textit{What are effective ways of incorporating syntactic structure into a neural language model?}

We study a class of neural language models that explicitly model the hierarchical syntactic structure in addition to the sequence of words \citep{dyer2016rnng,buys2015generative,buys2018exact}. These models merges generative transition-based parsing with recurrent neural networks in order to model sentences together with their latent syntactic structure. The syntactic structure that decorates the words can be latent, and marginalized over, or can be given explicitly, for example as the prediction of an external parser. Although these are fundamentally joint model, they can be evaluated as regular language models (modeling only words) by (approximate) marginalization of the syntactic structure. In the case of the RNNG \citep{dyer2016rnng}, exact marginalization is intractable due to the parametrization of the statistical model, but importance sampling provides an effective approximate method. An externally trained discriminative parser is used to obtain proposal samples. Other models provide exact marginalization, but this typically comes at the cost of a less expressive parametrization, for example one in which the features cannot be structure-dependent \citep{buys2018exact}.

In this thesis I study the RNNG \citep{dyer2016rnng} and investigate:

\paragraph{The approximate marginalization} I propose an alternative proposal distribution and investigate the impact.
\begin{itemize}

  \item I propose a new discriminative chart-based neural parser that is trained with a global, Conditional Random Field (CRF), objective. The parser is an adaptation of the minimal neural parser proposed in \citet{stern2017minimal}, which is trained with a margin-based objective.

  \item This contrast with the choise of \citet{dyer2016rnng} for a transition-based parser as proposal, a discrminatively trained RNNG.

  \item We posit that a globally trained model is a better proposal distribution than a locally trained transition based model: a global model has ready access to competing analyses that can be structurally dissimilar but close in probability, whereas we hypothesize that a locally trained model is prone to produce locally corrupted structures that are nearby in transition-space. In a transition based parser more diverse samples can be obtained by flattening the transition distributions, which causes the model to be less confident in its predictions. The downside is that the model now explores parts of the probability space which it has not encountered during training.

\end{itemize}


\paragraph{Semi-supervised training by including unlabeled data} To make joint models competitive language models they need to make use of the vast amounts of unlabeled data that exists.
\begin{itemize}

  \item A major drawback of these syntactic language models is that they require annotated data to be trained, and preciously little of such data exists.

  \item We extend the training to the unsupervised domain by optimizing a variational lower bound on the marginal probabilities that jointly optimizes the parameters of proposal model ('posterior' in this framework) with the joint model.

  \item We obtain gradients for this objective using the score function estimator \citep{fu2006gradient}, also known as REINFORCE \citep{williams1992reinforce}, which is widely used in the field of deep reinforcement learning, and we introduce an effective baseline based on argmax decoding \citep{rennie2017argmax}, which significantly reduces the variance in this optimization procedure.

  \item Our CRF parser particularly excels in the role of posterior thanks the independence assumptions that allow for efficient exact computation of key quantities: the entropy term in the lower bound can be computed exactly using Inside-Outside algorithm, removing one source of variance from the gradient estimation, and the argmax decoding can be performed exactly thanks to Viterbi, making the argmax baseline even more effective.

\end{itemize}

\paragraph{Alternative, simpler, models} There are alternatives to the methods that this thesis investigates.
\begin{itemize}

  \item Multitask learning of a neural language model with a syntactic side objective is a competitive and robust alternative method to infuse neural language models with syntactic knowledge.

  \item Training the syntactic model on data that mixes gold trees with predicted `silver' trees for unlabeled data is a competitive and robust alternative to fully principled semi-supervised learning.

  \item We propose a simple multitask neural language model that predicts labeled spans from the RNN hidden states, using a feature function identical identical to that used in the CRF parser.

  \item We consider these alternatives in order to quantify significance of the latent structure and the semisupervised training as measured by some external performance metric.
  
\end{itemize}

\paragraph{Targeted syntactic evaluation}
TBA
