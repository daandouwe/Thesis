% \bibliography{../src/bibliography}

This appendix reports all details about the implementation of the models, including the datasets and their pre-processing, optimization, and hyperparameters. We have made some deliberate choices which we will motivate. As a general rule, we choose simplicity over maximal performance: we use a minimal scheme for dealing with unknown words, we use only word embeddings, which we learn from scratch, and use regular SGD optimization. The reason: to compare the models in a minimal setting so as to maximally focus on the essential modelling differences between the models. The implementation is publicly available at \url{github.com/daandouwe/thesis}.

\section{Data}

\subsection{Datasets}

\paragraph{Penn Treebank}
We use a publicly available version of the Penn Treebank (PTB) that was preprocessed and published by \citet{cross2016span}, and that has since been used in the experiments of \citet{stern2017minimal,kitaev2018benepar}. The data is divided along the standard splits of sections 2-21 for training, section 22 for development, and section 23 for testing. The dataset comes with predicted tags, which is for training neural parsers, but note, however, that none of our models use these tags. The dataset is availlable at \url{https://github.com/jhcross/span-parser/data}.\footnote{Although I do not know how it is possible to make the PTB public, given the licensing restrictions of the LDC, I am very thankful that it was done. Now, all the data used in this thesis is publicly availlable.}

\paragraph{One Billion Word Benchmark}
In the experiments on semisupervised learning we use the One Billion Word Benchmark (OBW) dataset \citep{chelba2013one} to obtain unlabeled data. This is a common dataset for large-scale language modelling \citep{jozefowicz2016exploring}, and has the advantage that it has sentences seperated by a newline; a requirement for the RNNG language model, which should only be applied to entire sentences and not to longer or shorter segments.\footnote{For this reason we cannot use the otherwise appealing Wikitext dataset \citep{merity2016pointer}: this dataset has sentences grouped into paragraphs.} The dataset in its entirety is far too large for our purposes, so instead we select sentences from the first section of the training data\footnote{Section \texttt{news.en-00001-of-00100}.} by selecting the first 100,000 sentences that have at most 40 words. Because the OBW uses slightly different tokenization and standardization of characters than the Penn Treebank we perform a number of processing steps to smooth out these difference. First, we escape all brackets following the Penn Treebank convention ($\eg$ replacing \texttt{(} with \texttt{-LRB-}) and do the same for the quotation marks ($\eg$ replacing \texttt{"} with \texttt{``}). Finally, tokenization of negation is handled differently in the OBW, and we change this to the PTB convention ($\eg$ replacing \texttt{don 't} with \texttt{do n't}). These simple changes together avoid a lot of incoherences when combining this dataset with the PTB. The dataset is publicly availlable at \url{http://www.statmt.org/lm-benchmark/}, and scripts for preprocessing are availlable at \url{github.com/daandouwe/thesis/scripts}.

\paragraph{CCG supertags}
For the multitask language model with CCG supertagging we use the CCGBank \citep{hockenmaier2007ccgbank} processed by \citet{enguehard2017multitask} into a word-tag format. This is the dataset also used by \citet{linzen2018targeted}. It follows the same splits of the Penn Treebank as described above, and restricts the size of the tagset from the original 1363 different supertags to 452 supertags that occurred at least ten times, replacing the rest of the tags with a dummy token. The dataset is publicly availlable at \url{https://github.com/BeckyMarvin/LM_syneval/tree/master/data/ccg_data}.


\subsection{Vocabularies}
We use two types of vocabularies corresponding to the two types of models that we study this thesis: a vocabulary for the discriminative models and a vocabulary for the generative models. The vocabulary used in the discrminative models contains all words in the training data, whereas the vocabulary used in the generative models only includes words that occur at least 2 times in the training data\footnote{This makes training of the generative model faster, because the softmax normalization involves less terms in the sum, and additionaly avoids the statistical difficulty related to predicting words that occur just once. The discrminative model has neither of these problems, since the words are only conditioned on}. Finally, in the semisupervised models we construct the vocabulary from the labeled and unlabeled datasets combined. To keep the vocabulary size manageable in this larger dataset we restrict the vocabulary of the generative model to words that occur at leest 3 times.

\paragraph{Unknown words} We use a single token for unknown words, and during training replace each word $w$ by this token with probability $1 / (1 + \text{freq}(w))$, where $\text{freq}(w)$ is the frequency of $w$ in the training data. In this we follow \citet{stern2017minimal}, and this is a different approach from \citet{dyer2016rnng} who use a set of almost 50 tokens each with detailed lexical information about the unknown word in question.\footnote{This approach is taken from the Berkeley parser \citep{petrov2006learning}.} This elaborate approach is common in parsing but certainly not in language modelling \citep{dyer2016rnng}, for which reason we opt for the simpler scheme of a single token.

\paragraph{Embeddings}
All word embeddings are learned from scratch: the embeddings are considered part of the model's parameters and are optimized jointly with the rest of them, starting from random initialization \citep{glorot2010understanding}. While we surmise that more elaborate embeddings should improve performance of the models\footnote{See for example the impact of ELMo embeddings \citep{peters2018elmo} on the performance of the parser in \citet{kitaev2018attentive} (an adaptation of the chart parser in \citet{stern2017minimal}).}, such investigation is othogonal to our work. We furthermore do not experiment with any kind of subword information in the embeddings and, as noted before, make no use of tags in any of the models. The influence of such embeddings on the discriminate parser of \citet{stern2017minimal} is analysed extensively by \citet{stern2018analyis}, who investigate all combinations of word, tag, and character-LSTM embeddings, and find that the best model\footnote{A tie between the model that uses all embeddings concatenated and the model that uses just the character-LSTM, both with an F1 of 92.24.} improves on the worst model\footnote{Using only word embeddings, at an F1 of 91.44.} by only 0.8 F1, which is a relative improvement of just 1\%. We believe this justifies our basic approach.

\begin{table}
  \caption{Vocabularies}
  \label{tab:vocabularies}
  % Dataset statistics (number of sentences and number of tokens) and vocabulary sizes
\end{table}


\section{Implementation}
All our models are implemented in python using the Dynet neural network library \citep{neubig2017dynet}, and use automatic batching \citep{neubig2017fly}. Automabatching enables efficient training of ours model, for which manual batching is not possible.

\paragraph{Optimization}
All our models are optimized with stochastic gradient-based methods, in which we use mini-batches to compute stochastic approximations of the model's gradient on the entire dataset. We use mini-batches subsampled from the dataset and let Dynet compute the gradients using automatic differentiation \citep{neubig2017dynet,baydin2018automatic}. For all our supervised models we use regular stochastic gradient descent (SGD), with an initial learning rate of 0.1, and anneal this based on performance on a development set. This follows the recomendations of \citet{wilson2017marginal}, who show that on models and datasets similar to ours this method finds good solutions and is more robust against overfitting than methods with adaptive learning rates. This also follows \citet{dyer2016rnng}, who obtain their best RNNG models using this optimizer and learning rate. For our semisupervised model we do rely on adaptive gradient methods, and use Adam \citep{kingma2014adam} with the default learning rate\footnote{To be precise, this is the value $\alpha$ in \citet{kingma2014adam}.} of 0.001. Adaptive learning are considered more suitable for dealing with the dramatic variability in magnitude of the surrogate obective \citep{ranganath2014black,klein2018reinforce}. We use minibatches

We normalize the learning signal: \say{This variability makes training an inference network using a fixed learning rate difficult. We address this issue by dividing the centered learning signal by a running estimate of its standard deviation. This normalization ensures that the signal is approximately unit variance, and can be seen as a simple and efficient way of adapting the learning rate.} \citep{mnih2014nvil}.

\begin{table}
  \caption{Optimization}
  \label{tab:optimization}
  % Optimizers, learning rates, batch sizes.
\end{table}

\item Surrogate objective and gradient blocking \cite{schulman2015gradient}.
\end{itemize}

\paragraph{Hyperparameters}
For the RNNG we follow exactly the hyperparameters from the published papers. For the CRF parser
\begin{itemize}
  \item Dropout of ...
\end{itemize}

\begin{table}
  \caption{Model parameters}
  \label{tab:model-parameters}

\end{table}
