This appendix reports all details about the implementation of the models, including the datasets and their pre-processing, optimization, and hyperparameters. We have made some deliberate choices which we will motivate. As a general rule, we choose simplicity over maximal performance: we use a minimal scheme for dealing with unknown words, we use only word embeddings---which are learned from scratch---and use regular SGD optimization. The reason: to compare the models in a minimal setting so as to maximally focus on the essential modelling differences between the models. All code is available at \url{github.com/daandouwe/thesis}.

\section{Data}

  \subsection{Datasets}
    We use three datasets: the Penn Treebank for the parsing models; CCG supertags for the RNNLM-CCG multitask model; and (part of) the One Billion Word Benchmark for unlabeled data in the semisupervised training.

    \paragraph{Penn Treebank}
    We use a publicly available version of the Penn Treebank (PTB) that was preprocessed and published by \citet{cross2016span}, and that has since been used in the experiments of \citet{stern2017minimal,kitaev2018attentive}. The data is divided along the standard splits of sections 2-21 for training, section 22 for development, and section 23 for testing. The dataset comes with predicted tags, which is a requirement for neural parsers to avoid overfitting, but note however that none of our models use tag information. The dataset is availlable at \url{https://github.com/jhcross/span-parser/data}.\footnote{Although I do not know how it is possible to make the PTB public, given the licensing restrictions of the LDC, I am very thankful that it was done. Now, all the data used in this thesis is publicly availlable.}

    \paragraph{CCG supertags}
    For the multitask language model with CCG supertagging we use the CCGBank \citep{hockenmaier2007ccgbank} processed by \citet{enguehard2017multitask} into a word-tag format. This is the dataset also used by \citet{linzen2018targeted}. It follows the same splits of the Penn Treebank as described above, and restricts the size of the tagset from the original 1363 different supertags to 452 supertags that occurred at least ten times, replacing the rest of the tags with a dummy token. The dataset is publicly availlable at \url{https://github.com/BeckyMarvin/LM_syneval/tree/master/data/ccg_data}.

    \paragraph{One Billion Word Benchmark}
    In the experiments on semisupervised learning we use the One Billion Word Benchmark (OBW) dataset \citep{chelba2013one} to obtain unlabeled data. This is a common dataset for large-scale language modelling \citep{jozefowicz2016exploring}, and has the advantage that it has sentences seperated by a newline; a requirement for the RNNG language model, which should only be applied to entire sentences and not to longer or shorter segments.\footnote{For this reason we cannot use the otherwise appealing Wikitext dataset \citep{merity2016pointer}: this dataset has sentences grouped into paragraphs.} The dataset in its entirety is far too large for our purposes, so instead we select sentences from the first section of the training data\footnote{Section \texttt{news.en-00001-of-00100}.} by selecting the first 100,000 sentences that have at most 40 words. Because the OBW uses slightly different tokenization and standardization of characters than the Penn Treebank we perform a number of processing steps to smooth out these difference. First, we escape all brackets following the Penn Treebank convention (replacing \texttt{(} with \texttt{-LRB-}) and do the same for the quotation marks (replacing \texttt{"} with \texttt{``}). Finally, tokenization of negation is handled differently in the OBW, and we change this to the PTB convention (replacing \texttt{don 't} with \texttt{do n't}). These simple changes together avoid a lot of incoherences when combining this dataset with the PTB. The dataset is publicly availlable at \url{http://www.statmt.org/lm-benchmark/}, and scripts for preprocessing are availlable at \url{github.com/daandouwe/thesis/scripts}.

  \subsection{Vocabularies}
    We use two types of vocabularies corresponding to the two types of models that we study this thesis: a vocabulary for the discriminative models and a vocabulary for the generative models. The vocabulary used in the discrminative models contains all words in the training data, whereas the vocabulary used in the generative models only includes words that occur at least 2 times in the training data\footnote{This makes training of the generative model faster, because the softmax normalization involves less terms in the sum, and additionaly avoids the statistical difficulty related to predicting words that occur just once. The discrminative model has neither of these problems, since the words are just conditioned on.}. Finally, in the semisupervised models we construct the vocabulary from the labeled and unlabeled datasets combined. To keep the vocabulary size manageable in this larger dataset we restrict the vocabulary of the generative model to words that occur at leest 3 times.

    \paragraph{Unknown words} We use a single token for unknown words, and during training replace each word $w$ by this token with probability
    \begin{align*}
      \frac{1}{1 + \text{freq}(w)},
    \end{align*}
    where $\text{freq}(w)$ is the frequency of $w$ in the training data. In this we follow \citet{stern2017minimal}. We deviate from \citet{dyer2016rnng}, who use a set of almost 50 tokens each with detailed lexical information about the unknown word in question.\footnote{An approach taken from the Berkeley parser \citep{petrov2006learning}.} This elaborate approach is common in parsing but certainly not in language modelling \citep{dyer2016rnng}, for which reason we opt for the simpler scheme of a single token.

    \paragraph{Embeddings}
    \label{sec:impl-embedding}
    All word embeddings are learned from scratch: the embeddings are considered part of the model's parameters and are optimized jointly with the rest of them, starting from random initialization \citep{glorot2010understanding}. We surmise that more elaborate embeddings should improve performance of the models, but such investigation is in principle othogonal to our work.\footnote{See for example the impact of ELMo embeddings \citep{peters2018elmo} on the performance of the parser in \citet{kitaev2018attentive} (an adaptation of the chart parser in \citet{stern2017minimal}).} We furthermore do not experiment with any kind of subword information in the embeddings and, as noted before, make no use of tags in any of the models. The influence of such embeddings on the discriminate parser of \citet{stern2017minimal} is analysed extensively by \citet{stern2018analyis}, who investigate all combinations of word, tag, and character-LSTM embeddings, and find that the best model\footnote{A tie between the model that uses all embeddings concatenated and the model that uses just the character-LSTM, both with an F1 of 92.24.} improves on the worst model\footnote{Using only word embeddings, at an F1 of 91.44.} by only 0.8 F1, which is a relative improvement of just 1\%. We believe this justifies our basic approach.

    % \begin{table}
    %   \caption{Vocabularies}
    %   \label{tab:vocabularies}
    %   % Dataset statistics (number of sentences and number of tokens) and vocabulary sizes
    % \end{table}

    \begin{table}[h]
    \center
      \begin{tabular}{l|r}
          Model  & Parameters \\ \hline
          Discriminative RNNG & 798,078  \\
          Generative RNNG & 9,610,736  \\
          CRF & 762,752  \\
          CRF (big) & 2,337,282  \\
          RNNLM & 9,308,979 \\
          RNNLM-span & 9,420,172 \\
          RNNLM-CCG & 9,498,986
      \end{tabular}
      \caption{Number of parameters of all models used.}
      \label{tab:num-params}
    \end{table}


\section{Implementation}
  All our models are implemented in python using the Dynet neural network library \citep{neubig2017dynet}, and use automatic batching \citep{neubig2017fly}. Autobatching enables efficient training of our models, for which manual batching is too difficult.

  \paragraph{Optimization}
    All our models are optimized with stochastic gradient-based methods, in which we use mini-batches to compute stochastic approximations of the model's gradient on the entire dataset, and let Dynet compute the gradients using automatic differentiation \citep{neubig2017dynet,baydin2018automatic}. For all our supervised models we use regular stochastic gradient descent (SGD), with an initial learning rate of 0.1, and anneal this by a factor of 2 when the performance on the development set fails to improve. This follows the recommendations of \citet{wilson2017marginal}, who show that on models and datasets similar to ours this method finds good solutions and is more robust against overfitting than methods with adaptive learning rates. This also follows \citet{dyer2016rnng}, who obtain their best RNNG models using this optimizer and learning rate. For our semisupervised model we do rely on adaptive gradient methods, and use Adam \citep{kingma2014adam} with the default learning rate\footnote{To be precise, this is the value $\alpha$ in \citet{kingma2014adam}.} of 0.001. Adaptive learning are considered more suitable for dealing with the dramatic variability in magnitude of the surrogate obective \citep{ranganath2014black,klein2018reinforce}. For all supervised models we use minibatches of size 10.
