We considered neural network language models that incorporate syntactic structure. Central to our discussion was the RNNG, in which the syntactic structure is modelled explicitly in a joint distribution $p( \x, \y )$ and a language model $p(\x)$ is obtained through approximate marginalization over $\y$ using a discriminative proposal model. The approximate marginalization is central in the application of the RNNG as language model, and we have introduced a neural CRF parser to investigate the impact of an alternative proposal distribution. As a side gain we obtained a competitive probabilistic parser, and with that a globally normalized probability distribution over trees that has aplications beyond what we discussed. We showed how the RNNG and CRF can be combined in semisupervised and unsupervised learning, but must leave the full exploration of those ideas for future work. We briefly discussed neural language models that only model $\x$ but receive syntactic supervision during training in the form of multitask learning. To gain insight in their comparative syntactic abilities we performed targeted syntactic evaluation using the Syneval dataset, which gave us a detailed breakdown.

We now list the main constributions of this thesis and follow this with suggestions for future work that depart from them.

\section{Main contributions}
  The main contributions of this thesis are: (1) the neural CRF parser introduced in chapter \ref{04-crf}; (2) the semisupervised and unsupervised learning of the RNNG guided by a CRF posterior in chapter \ref{05-semisupervised}; (3) the syntactic evaluation of all our models and (4) the comparison of the RNNG to syntactic multitask models in chapter \ref{06-syneval}.

  \begin{description}
    \item[Neural CRF parser]
      We presented a neural CRF parser by borrowing the span factored approach and neural scoring function from \cite{stern2017minimal} and deriving custom inference algorithms from general inside and outside recursions. We showed that the model is a competitive parser that outperforms a discriminative RNNG of the same size by a large margin and moreover appears to deal with the absence of tag information much better. We used the CRF as a proposal distribution for the RNNG, but noted that there is a subtle mismatch between the space of trees modelled by the CRF and the RNNG. This is caused by derivational ambiguity which in turn is caused by the way we deal with the dummy label $\varnothing$ in the CRF. We showed how the parse forest of the CRF can be altered to deal with this but must leave the full evaluation to future work. Finally, the prohibitively slow training is a drawback of the model, but we noted that pruning the labelset---removing mostly labels corresponding to rarely occuring unary chains---results in great speedup. The pruning will inevitably affect the accuracy of the model, and future research can investigate this.

    \item[Semisupervised learning of RNNG]
      We formulated semisupervised and unsupervised estimation of the RNNG using variational inference. We showed how the CRF posterior allows us to derive a particularly satisfying version of the ELBO in which the entropy term can be computed exactly, and have argued that the global normalization makes for a distribution that is more amenable to sampling based gradient estimation. Our first attempt at semisupervised learning from pretrained models turned out unsuccesful: the CRF is too slow, and with the RNNG we could not prevent the posterior from degenerating to pathological trees. The unlabeled setting proved more promising, but the derivational ambiguity in the CRF prevented us from optimizing the proper objective. We have described the solutions to this, and preliminary results show that they fix the problem. Concurrent work shows that this approach works for binary trees---given appropriate tuning---and

    \item[Syntactic evaluation of RNNG]
      We performed targeted syntactic evaluation of the RNNG on the dataset of \citet{linzen2018targeted}. This gave us a detailed breakdown of performance. However, the finegrained evaluation also poses an interpretative challenge, and we additionally opted for a broader view provided by averaging the results.

    \item[Comparison with multitask learning]
      We proposed a novel multitask language model with a side objective inspired by the span scoring function of the CRF, and described a previous model based on CCG supertagging. This method provides an alternative way to bias language models towards syntax, and in at least one category of the Syneval dataset this method outperformed the RNNG (on average).

  \end{description}

\section{Future work}
  We see two directions for future work, both on unsupervised learning of the RNNG with the CRF as posterior. The first continues with the unsupervised learning where this thesis left off, following through with our proposed refinements of the CRF. The second direction that we propose takes the RNNG and CRF into the direction of sparse structured inference.

  \begin{description}

    \item[Unsupervised RNNG with CRF posterior]
      The line of work that departs from where this thesis left off: the further investigation of our CRF as approximate posterior for unsupervised learning of the RNNG. That this direction is fruitful has recently been shown by \citet{kim2019unsupervised}. But our work differs from theirs because we do not restrict to binary trees and use the original formulation of the RNNG. We have made the first steps in this direction, but were halted by a fundamental mismatch between the support of the RNNG and the CRF. We have described the steps that need to be taken to overcome this mismatch. We can learn from the insights of \cite{kim2019unsupervised} and follow the training strategy that found was required to, including separate optimizers, annealing the posterior, and early stopping.

    \item[SparseMAP inference]
      Another direction of interest is to perform unsupervised learning of the generative RNNG with CRF posterior using the recently proposed SparseMAP inference \citep{niculae2018sparsemap}. SparseMAP allows sparse structured inference with neural networks by inducing sparse posterior distributions over the latent strucutre, so that sums over that space can be computed as exact sums over only the small numer of structures with nonzero probability. This allows fully differentiable training of neural networks with discrete latent structure, and has been used effectively for unsupervised induction of dependency trees \citep{niculae2018towards}. The key requirement for sparseMAP is that the posterior model permits efficient and exact MAP inference: this is precicely what is provided for our CRF parser by the Viterbi algorithm. And in contrast with the approaches that we have discussed in chapter \ref{05-semisupervised} SparseMAP requires no approximation by sampling. A first sketch of our approach would be to optimize a kind of autoencoding objective
      \begin{align*}
        \sum_{ \y \in \yieldx } p_{\theta}(\x \mid \y) q_{\lambda}( \y \mid \x ),
      \end{align*}
      with trees as discrete latent structure. Here $q$ would be the CRF model, and $p$ an adaptation of the joint RNNG in which only the words are modelled, and the structure $\y$ is conditioned on.\footnote{A straightforward adaptation of the transition system achieves this: all other actions are provided, and only the word in the $\gen$ action needs to be predicted.}. SparseMAP makes the sum over $\yieldx$ tractable by inducing sparsity in $q$, such that most of the terms in the sum are zero. Only for the small number of trees for which the posterior is nonzero do we need to compute $p_{\theta}(\x \mid \y)$. This training objective is not probabilistic, and the two conditional models $p$ and $q$ together do not define a language model, but it can be used to for unsupervised tree induction, and as such provide interesting comparison with the approaches based on variational inference.

    \end{description}
