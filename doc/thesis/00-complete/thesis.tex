% !Mode:: "TeX:DE:UTF-8:Main"

\documentclass[examplefnt,biber]{../src/nowfnt}

\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb,amsfonts,latexsym,dsfont,xspace}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{dirtytalk}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz-qtree}
\usepackage{linguex}

\usetikzlibrary{arrows,calc}

\graphicspath{{../figures/}}

%% abrevitions
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\etc}{\emph{etc}}
\newcommand{\cf}{\emph{cf.}}
%% neural network functions
\newcommand{\embed}{\textsc{E}}
\newcommand{\ff}{\textsc{Ffn}}
\newcommand{\rnn}{\textsc{Rnn}}
%% RNNG actions
\newcommand{\reduce}{\textsc{reduce}}
\newcommand{\open}{\textsc{open}}
\newcommand{\shift}{\textsc{shift}}
\newcommand{\gen}{\textsc{gen}}
\newcommand{\discactions}{\mathcal{A}_D}
\newcommand{\genactions}{\mathcal{A}_G}
%% blocking gradient in computation graph
\newcommand{\blockgrad}{\textsc{BlockGrad}}
%% probability models
\newcommand{\ptheta}{p_{\theta}}
\newcommand{\qlambda}{q_{\lambda}}
%% math symbols
\newcommand{\reals}{\ensuremath{\mathbb{R}}}  % real numbers
\newcommand{\dataset}{\ensuremath{\mathcal{D}}}  % dataset
\newcommand{\defeq}{\ensuremath{\triangleq}}  % define something with an equation
%% miscelaneous
\newcommand{\x}{\ensuremath{x}}  % sentence
\newcommand{\y}{\ensuremath{y}}  % tree
\newcommand{\yieldx}{\mathcal{Y}(x)}  % yield of a sentence x
\newcommand{\vecx}{\ensuremath{\mathbf{x}}}  % sentence
\newcommand{\vecy}{\ensuremath{\mathbf{y}}}  % tree
\newcommand{\h}{\ensuremath{\mathbf{h}}}  % hidden vector
\newcommand{\fw}{\ensuremath{\mathbf{f}}}  % forward lstm feature
\newcommand{\bw}{\ensuremath{\mathbf{b}}}  % backward lstm feature
\newcommand{\veca}{\ensuremath{\mathbf{a}}}
\newcommand{\vecb}{\ensuremath{\mathbf{b}}}
\newcommand{\vecr}{\ensuremath{\mathbf{r}}}
\newcommand{\vecs}{\ensuremath{\mathbf{s}}}
\newcommand{\vecu}{\ensuremath{\mathbf{u}}}
\newcommand{\vecv}{\ensuremath{\mathbf{v}}}
\newcommand{\vecw}{\ensuremath{\mathbf{w}}}
\newcommand{\vecR}{\ensuremath{\mathbf{R}}}
\newcommand{\vecS}{\ensuremath{\mathbf{S}}}
\newcommand{\vecV}{\ensuremath{\mathbf{V}}}
\newcommand{\vecW}{\ensuremath{\mathbf{W}}}

\DeclareMathOperator*{\argmax}{arg\,max}  % argmax
\DeclareMathOperator{\indicator}{\mathbf{1}}  % indicator function
\DeclareMathOperator{\expect}{\mathbb{E}} % expectation
\DeclareMathOperator{\var}{Var} % variance
\DeclareMathOperator{\cov}{Cov} % covariance
\DeclareMathOperator{\corr}{corr} % covariance
\DeclareMathOperator{\objective}{\mathcal{L}}  % objective function
\DeclareMathOperator{\elbo}{\mathcal{E}}  % elbo objective


%ARTICLE SUB-TITLE
\subtitle{Neural language models with latent syntax}

\maintitleauthorlist{
  Daan van Stigt \\
  Institute for Logic, Language and Computation \\
}

%BIBLIOGRAPHY FILE
\addbibresource{../src/bibliography.bib}

\begin{document}

\makeabstracttitle

\begin{abstract}
In this thesis I investigate the question: \textit{What are effective ways of incorporating syntactic structure into neural language models?}

In this thesis I:
\begin{itemize}
  \item study a class of neural language models that merges generative transition-based parsing with recurrent neural networks in order to model sentences together with their latent syntactic structure;
  \item propose a new globally trained chart-based parser as an alternative proposal distribution used in the approximate marginalization;
  \item propose effective methods for semisupervised learning, making the syntactic structure a latent variable;
  \item perform targeted syntactic evaluation and compare the model's performance with that of alternative models that are based on multitask learning.
\end{itemize}
\end{abstract}

% end of main matter
\begin{acknowledgements}
\end{acknowledgements}

\input{../tables/notation.tex}


\chapter{Introduction}
\label{01-introduction}
\input{../01-introduction/introduction}


\chapter{Background}
\label{02-background}
\input{../02-background/background}


\chapter{Recurrent Neural Network Grammars}
\label{03-rnng}
\input{../03-rnng/rnng}


\chapter{Conditional Random Field parser}
\label{04-crf}
\input{../04-crf-parser/crf-parser}


\chapter{Semisupervised learning}
\label{05-semisupervised}
\input{../05-semisupervised/semisupervised}


\chapter{Syntactic evaluation}
\label{06-syneval}
\input{../06-syneval/syneval}


\chapter{Conclusion}
\label{07-conclusion}
\input{../07-conclusion/conclusion}


\appendix
\chapter{Figures}
\label{A1-figures}
\input{../08-app-figures/figures.tex}


\chapter{Implementation}
\label{A2-implementation}
\input{../09-app-implementation/implementation.tex}


\chapter{Semiring parsing}
\label{A3-crf}
\input{../10-app-crf/crf.tex}


\chapter{Score function estimator}
\label{A4-vi}
\input{../11-app-vi/vi.tex}


\chapter{Syneval dataset}
\label{A5-syneval}
\input{../12-app-syneval/syneval.tex}


\printbibliography


%BACKMATTER SEE DOCUMENTATION
\backmatter  % references, restarts sample


\end{document}
