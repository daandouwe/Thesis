% !Mode:: "TeX:DE:UTF-8:Main"

\documentclass{uvamath}
\usepackage{illcmolthesis}

\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsfonts,latexsym,dsfont,xspace}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{dirtytalk}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz-qtree}
\usepackage{linguex}

\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{bbm}
\usepackage{bm}
\usepackage{float}

\usepackage{natbib}
\usepackage{todonotes}
\usepackage{mathtools}
\usepackage{appendix}
\usepackage{url}
\usepackage{nameref}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\usepackage{palatino}

\usepackage{epigraph}

% \documentclass{memoir}
% \epigraphfontsize{\small\itshape}
% \setlength\epigraphwidth{8cm}
% \setlength\epigraphrule{0pt}


\usetikzlibrary{arrows,calc}

\graphicspath{{../figures/}}

%% abrevitions
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\etc}{\emph{etc}}
\newcommand{\cf}{\emph{cf.}}
%% neural network functions
\newcommand{\embed}{\textsc{E}}
\newcommand{\ff}{\textsc{Ffn}}
\newcommand{\rnn}{\textsc{Rnn}}
%% RNNG actions
\newcommand{\reduce}{\textsc{reduce}}
\newcommand{\open}{\textsc{open}}
\newcommand{\shift}{\textsc{shift}}
\newcommand{\gen}{\textsc{gen}}
\newcommand{\discactions}{\mathcal{A}_D}
\newcommand{\genactions}{\mathcal{A}_G}
%% blocking gradient in computation graph
\newcommand{\blockgrad}{\textsc{BlockGrad}}
%% probability models
\newcommand{\ptheta}{p_{\theta}}
\newcommand{\qlambda}{q_{\lambda}}
%% math symbols
\newcommand{\reals}{\ensuremath{\mathbb{R}}}  % real numbers
\newcommand{\dataset}{\ensuremath{\mathcal{D}}}  % dataset
% \newcommand{\defeq}{\ensuremath{\triangleq}}  % define something with an equation
\newcommand{\defeq}{\ensuremath{:=}}  % define something with an equation
%% miscelaneous
\newcommand{\x}{\ensuremath{x}}  % sentence
\newcommand{\y}{\ensuremath{y}}  % tree
\newcommand{\yieldx}{\mathcal{Y}(x)}  % yield of a sentence x
\newcommand{\vecx}{\ensuremath{\mathbf{x}}}  % sentence
\newcommand{\vecy}{\ensuremath{\mathbf{y}}}  % tree
\newcommand{\h}{\ensuremath{\mathbf{h}}}  % hidden vector
\newcommand{\fw}{\ensuremath{\mathbf{f}}}  % forward lstm feature
\newcommand{\bw}{\ensuremath{\mathbf{b}}}  % backward lstm feature
\newcommand{\veca}{\ensuremath{\mathbf{a}}}
\newcommand{\vecb}{\ensuremath{\mathbf{b}}}
\newcommand{\vecr}{\ensuremath{\mathbf{r}}}
\newcommand{\vecs}{\ensuremath{\mathbf{s}}}
\newcommand{\vecu}{\ensuremath{\mathbf{u}}}
\newcommand{\vecv}{\ensuremath{\mathbf{v}}}
\newcommand{\vecw}{\ensuremath{\mathbf{w}}}
\newcommand{\vecR}{\ensuremath{\mathbf{R}}}
\newcommand{\vecS}{\ensuremath{\mathbf{S}}}
\newcommand{\vecV}{\ensuremath{\mathbf{V}}}
\newcommand{\vecW}{\ensuremath{\mathbf{W}}}

\DeclareMathOperator*{\argmax}{arg\,max}  % argmax
\DeclareMathOperator{\indicator}{\mathbf{1}}  % indicator function
\DeclareMathOperator{\expect}{\mathbb{E}} % expectation
\DeclareMathOperator{\var}{Var} % variance
\DeclareMathOperator{\cov}{Cov} % covariance
\DeclareMathOperator{\corr}{corr} % covariance
\DeclareMathOperator{\entropy}{H} % entropy
\DeclareMathOperator{\kl}{KL} % kl
\DeclareMathOperator{\objective}{\mathcal{L}}  % objective function
\DeclareMathOperator{\elbo}{\mathcal{E}}  % elbo objective

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\theoremstyle{definition}
\newtheorem{counterexample}[theorem]{Counterexample}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{definition}
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{condition}{Condition}


\title{Neural language models with latent syntax}
\author{Daan van Stigt}
\birthdate{August 17, 1992}
\birthplace{Amsterdam, The Netherlands}
\defensedate{May 24, 2019}
\supervisor{Dr. Wilker Aziz}
\committeemember{Dr. Wilker Aziz}
\committeemember{Dr. Caio Corro}
% \committeemember{Prof. Dr. Khalil Sima`an}
\committeemember{Dr. Ekaterina Shutova}
\committeemember{Prof. Dr. Yde Venema}
\committeemember{Dr. Willem Zuidema}


\begin{document}
\maketitle


% We study neural language models that are obtained by approximate marginalization of the recurrent neural network grammar (RNNG), a joint model of sentences and their syntactic structure.

\begin{abstract}
  We study neural language models that are obtained by marginalizing tree latent variables in the recurrent neural network grammar (RNNG), a joint model of sentences and their syntactic structure. [Problem] Supervised RNNGs produce competitive language models, but the intractable sum over the structured variable complicates learning from unlabeled data in which the structure is latent. [The effectiveness of approximate learning with variational inference will depend on the quality of the posterior approximation.] [How] We introduce a neural conditional random field (CRF) constituency parser that can be used as alternative proposal model for approximate inference of a supervised RNNG, and experiment with the CRF as approximate posterior in variational learning, in which we learn both models jointly by optimizing a lower bound on the marginal likelihood. This opens the door to semisupervised and unsupervised learning. [Why] The CRF formulation of the parser allows the exact computation of key quantities involved in the computation of the lower bound, and the global normalization provides a robust distribution for the sampling based gradient estimation. [Results] Preliminary results with unlabeled trees suggest the potential of this approach for unsupervised $n$-ary tree induction, and we formulate future work towards this goal. Finally, to evaluate how the joint formulation differentiates the RNNG from other neural language models we perform targeted syntactic evaluation and compare its performance with that of alternative models that are based on multitask learning.


\end{abstract}



{\hypersetup{linkcolor=black}
% or \hypersetup{linkcolor=black}, if the colorlinks=true option of hyperref is used
\tableofcontents
}

\chapter*{Notation}
\input{../tables/notation.tex}


\chapter{Introduction}
\label{01-introduction}
\input{../01-introduction/introduction}


\chapter{Background}
\label{02-background}
\input{../02-background/background}


\chapter{Recurrent Neural Network Grammars}
\label{03-rnng}
\input{../03-rnng/rnng}


\chapter{Conditional Random Field parser}
\label{04-crf}
\input{../04-crf-parser/crf-parser}


\chapter{Semisupervised learning}
\label{05-semisupervised}
\input{../05-semisupervised/semisupervised}


\chapter{Syntactic evaluation}
\label{06-syneval}
\input{../06-syneval/syneval}


\chapter{Conclusion}
\label{07-conclusion}
\input{../07-conclusion/conclusion}


\appendix

\chapter{Implementation}
\label{A2-implementation}
\input{../09-app-implementation/implementation.tex}


\chapter{Semiring parsing}
\label{A3-crf}
\input{../10-app-crf/crf.tex}


\chapter{Variational inference}
\label{A4-vi}
\input{../11-app-vi/vi.tex}


\chapter{Syneval dataset}
\label{A5-syneval}
\input{../12-app-syneval/syneval.tex}


% \printbibliography
\bibliographystyle{plainnat}
\bibliography{../src/bibliography.bib}


\end{document}
