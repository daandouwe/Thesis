% !Mode:: "TeX:DE:UTF-8:Main"
%
%
%JOURNAL CODE  SEE DOCUMENTATION

\documentclass[examplefnt,biber]{../src/nowfnt} % creates the journal version, needs biber version
%wrapper for book and ebook are created automatically.

\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb,amsfonts,latexsym,dsfont,xspace}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz-qtree}



\graphicspath{{../figures/}}

% a few definitions that are *not* needed in general:
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\etc}{\emph{etc}}

% neural network functions
\newcommand{\ff}{\textsc{Ff}}
\newcommand{\rnn}{\textsc{Rnn}}
\newcommand{\lstm}{\textsc{Lstm}}
\newcommand{\rnng}{\textsc{Rnng}}
\newcommand{\birnn}{\textsc{BiRnn}}
\newcommand{\bilstm}{\textsc{BiLstm}}

% blocking gradient in computation graph
\newcommand{\blockgrad}{\textsc{BlockGrad}}

% probability models
\newcommand{\ptheta}{p_{\theta}}  % sentence
\newcommand{\qlambda}{q_{\lambda}}  % sentence

%% math symbols
\DeclareMathOperator{\reals}{\mathbf{R}}  % real numbers

\DeclareMathOperator{\dataset}{\mathcal{D}}  % dataset

\DeclareMathOperator{\x}{\mathbf{x}}  % sentence
\DeclareMathOperator{\y}{\mathbf{y}}  % tree
\DeclareMathOperator{\fw}{\mathbf{f}}  % forward lstm feature
\DeclareMathOperator{\bw}{\mathbf{b}}  % backward lstm feature

% \DeclareMathOperator{\expect}{E} % expectation
% \DeclareMathOperator{\expect}{\rm I\kern-.3em E} % expectation (thicker letter)
\DeclareMathOperator{\expect}{\mathbf{E}} % expectation
\DeclareMathOperator{\var}{\mathbf{var}} % variance
\DeclareMathOperator{\cov}{\mathbf{cov}} % covariance
\DeclareMathOperator{\corr}{\mathbf{corr}} % covariance

\DeclareMathOperator{\objective}{\mathcal{L}}  % objective function
\DeclareMathOperator{\elbo}{\mathcal{E}}  % elbo objective
\DeclareMathOperator{\yieldx}{\mathcal{Y}(\x)}  % yield of a sentence x

\DeclareMathOperator{\rev}{\text{reverse}}  % reverse a sequeunce
\DeclareMathOperator{\concat}{\circ}  % reverse a sequeunce


%ARTICLE TITLE
\title{Latent structure in deep generative models of language}


%ARTICLE SUB-TITLE
% \subtitle{Language models with latent syntactic structure}


%AUTHORS FOR COVER PAGE
% separate authors by \and, item by \\
% Don't use verbatim or problematic symbols.
% _ in mail address should be entered as \_
% Pay attention to large mail-addresses ...

%if there are many author twocolumn mode can be activated.
%\booltrue{authortwocolumn} %SEE DOCUMENTATION
\maintitleauthorlist{
  Daan van Stigt \\
  Institute for Logic, Language and Computation \\
}

%BIBLIOGRAPHY FILE
\addbibresource{../src/bibliography.bib}

\begin{document}

\makeabstracttitle

\begin{abstract}
In this thesis I investigate the question: \textit{What are effective ways of incorporating syntactic structure into neural language models?}

In this thesis I:
\begin{itemize}
  \item study a class of neural language models that merges generative transition-based parsing with recurrent neural networks in order to model sentences together with their latent syntactic structure;
  \item propose a new globally trained chart-based parser as an alternative proposal distribution used in the approximate marginalization;
  \item propose effective methods for semisupervised learning, making the syntactic structure a latent variable;
  \item perform targeted syntactic evaluation and compare the model's performance with that of alternative models that are based on multitask learning.
\end{itemize}
I find that:
\begin{itemize}
  \item ...
  \item ...
\end{itemize}
\end{abstract}

\chapter{Notes}
The plan for next week: \textbf{hand in a fully outlines and bullet-pointed thesis}

This is a todo list:
\begin{itemize}
  \item Do not describe the CRF parser or inside-outside algorithm (`value recursion`) in the exact framework of graphical models! These are hard to reconcile!
  \item Use the general value recursion formulation and derive from there.
  \item Move formula derivations for the VI gradient and variance reduction to VI appendix.
  \item Move gradient blocking and surrogate objective to implementation appendix.
  \item Outline the chapter on RNNGs. Expand on sections on brains and syntax.
  \item Outline the chapter on syntactic evaluation.
  \item Describe the multitask models.
\end{itemize}


\chapter{Introduction}
\label{01-introduction}
\input{../01-introduction/introduction}


\chapter{Background}
\label{02-background}
\input{../02-background/background}


\chapter{Recurrent Neural Network Grammars}
\label{03-rnng}
\input{../03-rnng/rnng}


\chapter{Conditional Random Field parser}
\label{04-crf}
\input{../04-crf-parser/crf-parser}


\chapter{Semisupervised learning}
\label{05-semisupervised}
\input{../05-semisupervised/semisupervised}


\chapter{Syntactic evaluation}
\label{06-syneval}
\input{../06-syneval/syneval}


\chapter{Conclusion}
\label{07-conclusion}
\input{../07-conclusion/conclusion}


% end of main matter
\begin{acknowledgements}

\end{acknowledgements}

\appendix
\chapter{Figures}
\label{A1-figures}
\input{../08-app-figures/figures.tex}

\chapter{Implementation}
\label{A2-implementation}
\input{../09-app-implementation/implementation.tex}

\chapter{Conditional Random Fields}
\label{A3-crf}
\input{../10-app-crf/crf.tex}

\chapter{Variational Inference}
\label{A4-vi}
\input{../11-app-vi/vi.tex}


%BACKMATTER SEE DOCUMENTATION
\backmatter  % references, restarts sample

\printbibliography

\end{document}
