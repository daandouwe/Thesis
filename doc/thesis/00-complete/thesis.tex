% !Mode:: "TeX:DE:UTF-8:Main"
%
%
%JOURNAL CODE  SEE DOCUMENTATION

\documentclass[examplefnt,biber]{../src/nowfnt} % creates the journal version, needs biber version
%wrapper for book and ebook are created automatically.

\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb,amsfonts,latexsym,dsfont,xspace}

\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{dirtytalk}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz-qtree}
% \usepackage{qtree}
% \usepackage{linguex}
% \newsavebox{\partbox}

\graphicspath{{../figures/}}

% a few definitions that are *not* needed in general:
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\etc}{\emph{etc}}

% neural network functions
\newcommand{\ff}{\textsc{ff}}
\newcommand{\rnn}{\textsc{rnn}}
\newcommand{\lstm}{\textsc{Lstm}}
\newcommand{\rnng}{\textsc{Rnng}}
\newcommand{\birnn}{\textsc{BiRnn}}
\newcommand{\bilstm}{\textsc{BiLstm}}

% RNNG actions
\newcommand{\reduce}{\textsc{reduce}}
\newcommand{\open}{\textsc{open}}
\newcommand{\shift}{\textsc{shift}}
\newcommand{\gen}{\textsc{gen}}
\newcommand{\discactions}{A_{\mathcal{D}}}
\newcommand{\genactions}{A_{\mathcal{G}}}

\newcommand{\syneval}{\textsc{syneval}}


% blocking gradient in computation graph
\newcommand{\blockgrad}{\textsc{BlockGrad}}

% probability models
\newcommand{\ptheta}{p_{\theta}}  % sentence
\newcommand{\qlambda}{q_{\lambda}}  % sentence

%% math symbols
\newcommand{\reals}{\ensuremath{\mathbf{R}}}  % real numbers
\newcommand{\dataset}{\ensuremath{\mathcal{D}}}  % dataset

\newcommand{\defeq}{\ensuremath{\triangleq}}  % define something with an equation

\newcommand{\x}{\ensuremath{\mathbf{x}}}  % sentence
\newcommand{\y}{\ensuremath{\mathbf{y}}}  % tree
\newcommand{\h}{\ensuremath{\mathbf{h}}}  % hidden vector
\newcommand{\fw}{\ensuremath{\mathbf{f}}}  % forward lstm feature
\newcommand{\bw}{\ensuremath{\mathbf{b}}}  % backward lstm feature

\newcommand{\veca}{\ensuremath{\mathbf{a}}}
\newcommand{\vecb}{\ensuremath{\mathbf{b}}}
\newcommand{\vecr}{\ensuremath{\mathbf{r}}}
\newcommand{\vecs}{\ensuremath{\mathbf{s}}}
\newcommand{\vecu}{\ensuremath{\mathbf{u}}}
\newcommand{\vecv}{\ensuremath{\mathbf{v}}}
\newcommand{\vecw}{\ensuremath{\mathbf{w}}}

\newcommand{\vecR}{\ensuremath{\mathbf{R}}}
\newcommand{\vecS}{\ensuremath{\mathbf{S}}}
\newcommand{\vecV}{\ensuremath{\mathbf{V}}}
\newcommand{\vecW}{\ensuremath{\mathbf{W}}}


\DeclareMathOperator{\indicator}{\mathbf{1}}  % indicator function

% \DeclareMathOperator{\expect}{E} % expectation
% \DeclareMathOperator{\expect}{\rm I\kern-.3em E} % expectation (thicker letter)
\DeclareMathOperator{\expect}{\mathbf{E}} % expectation
\DeclareMathOperator{\var}{\mathbf{var}} % variance
\DeclareMathOperator{\cov}{\mathbf{cov}} % covariance
\DeclareMathOperator{\corr}{\mathbf{corr}} % covariance

\DeclareMathOperator{\objective}{\mathcal{L}}  % objective function
\DeclareMathOperator{\elbo}{\mathcal{E}}  % elbo objective
\DeclareMathOperator{\yieldx}{\mathcal{Y}(\x)}  % yield of a sentence x

\DeclareMathOperator{\rev}{\text{reverse}}  % reverse a sequeunce
\DeclareMathOperator{\concat}{\circ}  % concatenate vectors


%ARTICLE TITLE
\title{Latent syntax in a deep generative model of language}


%ARTICLE SUB-TITLE
% \subtitle{Language models with latent syntactic structure}


%AUTHORS FOR COVER PAGE
% separate authors by \and, item by \\
% Don't use verbatim or problematic symbols.
% _ in mail address should be entered as \_
% Pay attention to large mail-addresses ...

%if there are many author twocolumn mode can be activated.
%\booltrue{authortwocolumn} %SEE DOCUMENTATION
\maintitleauthorlist{
  Daan van Stigt \\
  Institute for Logic, Language and Computation \\
}

%BIBLIOGRAPHY FILE
\addbibresource{../src/bibliography.bib}

\begin{document}

\makeabstracttitle

\begin{abstract}
In this thesis I investigate the question: \textit{What are effective ways of incorporating syntactic structure into neural language models?}

In this thesis I:
\begin{itemize}
  \item study a class of neural language models that merges generative transition-based parsing with recurrent neural networks in order to model sentences together with their latent syntactic structure;
  \item propose a new globally trained chart-based parser as an alternative proposal distribution used in the approximate marginalization;
  \item propose effective methods for semisupervised learning, making the syntactic structure a latent variable;
  \item perform targeted syntactic evaluation and compare the model's performance with that of alternative models that are based on multitask learning.
\end{itemize}
I find that:
\begin{itemize}
  \item ...
  \item ...
\end{itemize}
\end{abstract}

% end of main matter
\begin{acknowledgements}
\end{acknowledgements}


\chapter{Introduction}
\label{01-introduction}
\input{../01-introduction/introduction}


\chapter{Background}
\label{02-background}
\input{../02-background/background}


\chapter{Recurrent Neural Network Grammars}
\label{03-rnng}
\input{../03-rnng/rnng}


\chapter{Conditional Random Field parser}
\label{04-crf}
\input{../04-crf-parser/crf-parser}


\chapter{Semisupervised learning}
\label{05-semisupervised}
\input{../05-semisupervised/semisupervised}


\chapter{Syntactic evaluation}
\label{06-syneval}
\input{../06-syneval/syneval}


\chapter{Conclusion}
\label{07-conclusion}
\input{../07-conclusion/conclusion}


\appendix
\chapter{Figures}
\label{A1-figures}
\input{../08-app-figures/figures.tex}


\chapter{Implementation}
\label{A2-implementation}
\input{../09-app-implementation/implementation.tex}


\chapter{Conditional Random Fields}
\label{A3-crf}
\input{../10-app-crf/crf.tex}


\chapter{Variational Inference}
\label{A4-vi}
\input{../11-app-vi/vi.tex}


\printbibliography


%BACKMATTER SEE DOCUMENTATION
\backmatter  % references, restarts sample


\end{document}
