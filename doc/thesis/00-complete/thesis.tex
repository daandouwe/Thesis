% !Mode:: "TeX:DE:UTF-8:Main"
%
%
%JOURNAL CODE  SEE DOCUMENTATION

\documentclass[examplefnt,biber]{../src/nowfnt} % creates the journal version, needs biber version
%wrapper for book and ebook are created automatically.

\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb,amsfonts,latexsym,dsfont,xspace}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{todonotes}

% a few definitions that are *not* needed in general:
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\etc}{\emph{etc}}
\newcommand{\now}{\textsc{now}}



%ARTICLE TITLE
\title{Neural language models with syntax}


%ARTICLE SUB-TITLE
% \subtitle{Smart Things as Said by Me}


%AUTHORS FOR COVER PAGE
% separate authors by \and, item by \\
% Don't use verbatim or problematic symbols.
% _ in mail address should be entered as \_
% Pay attention to large mail-addresses ...

%if there are many author twocolumn mode can be activated.
%\booltrue{authortwocolumn} %SEE DOCUMENTATION
\maintitleauthorlist{
Daan van Stigt \\
Institute for Logic, Language and Computation (ILLC) \\
}

%BIBLIOGRAPHY FILE
\addbibresource{../src/bibliography.bib}
% \usepackage[authoryear]{natbib}
% \bibliographystyle{apa}
% \bibliography{../src/bibliography}

\begin{document}

\makeabstracttitle

\begin{abstract}
In this thesis I investigate the question: \textit{What are effective ways of incorporating syntactic structure into neural language models?}

In this thesis I:
\begin{itemize}
  \item study a class of neural language models that merges generative transition-based parsing with recurrent neural networks in order to model sentences together with their latent syntactic structure;
  \item propose a new globally trained chart-based parser as an alternative proposal distribution used in the approximate marginalization;
  \item propose effective methods for semisupervised learning, making the syntactic structure a latent variable;
  \item perform targeted syntactic evaluation and compare the model's performance with that of alternative models that are based on multitask learning.
\end{itemize}
I find that:
\begin{itemize}
  \item ...
  \item ...
\end{itemize}
\end{abstract}


\chapter{Introduction}
\label{01-introduction}
\input{../01-introduction/introduction}


\chapter{Background}
\label{02-background}
\input{../02-background/background}


\chapter{Recurrent Neural Network Grammars}
\label{03-rnng}
\input{../03-rnng/rnng}


\chapter{Conditional Random Field parser}
\label{04-crf}
\input{../04-crf-parser/crf-parser}


\chapter{Semisupervised learning}
\label{05-semisupervised}
\input{../05-semisupervised/semisupervised}


\chapter{Multitask learning}
\label{06-multitask}
\input{../06-multitask/multitask}


\chapter{Syntactic evaluation}
\label{07-syneval}
\input{../07-syneval/syneval}


\chapter{Conclusion}
\label{08-conclusion}
\input{../08-conclusion/conclusion}


% end of main matter
\begin{acknowledgements}

\end{acknowledgements}

\appendix
\chapter{Figures}
\label{A1-figures}
\input{../09-app-data/data.tex}

%BACKMATTER SEE DOCUMENTATION
\backmatter  % references, restarts sample

\printbibliography

\end{document}
