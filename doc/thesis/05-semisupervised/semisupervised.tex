\bibliography{../src/bibliography.bib}

In this chapter we show how the RNNG can be trained on unlabeled data. Together with the regular, supervised, objective, this derives a way to perform semisupervised training.
\begin{itemize}
  \item I formulate an unsupervised objective for the RNNG that we can combine with the supervised objective to perfom semisupervised training.
  \item I introduce an approximate posterior in the form of a discriminative parser and derive a variational lower bound on the unsupervised objective.
  \item I show how to obtain gradients for this lowerbound by rewrinting the gradient into a form that is called the score function estimator \citep{Williams1992:REINFORCE,Fu2006}.
\end{itemize}

\paragraph{Notation} In this chapter we write $x$ for a sentence, $y$ for a (latent) constituency tree, and $\mathcal{Y}(x)$ for the \textit{yield} of $x$, all trees that can be assigned to $x$. Furthermore, let $\mathcal{D}_L = \{ (\mathbf{x}_i, \mathbf{y}_i)\}_{i=1}^N$ be a labeled dataset of sentences $\mathbf{x}$ with gold trees $\mathbf{y}$, and let $\mathcal{D}_U = \{ \mathbf{x}_i \}_{i=1}^M$ be an unlabeled dataset consisting of just sentences $x$. We denote our generative RNNG with $p_{\theta}$ and when we write $q_{\lambda}$ we will mean either the discriminative RNNG or the CRF parser.

\section{Objective}
We define the following general semi-supervised objective
\begin{align*}
  \mathcal{L}(\theta, \lambda) \triangleq \mathcal{L}_{S}(\theta) + \mathcal{L}_{U}(\theta, \lambda).
\end{align*}
The supervised objective $\mathcal{L}_{S}$ is optimized over $\mathcal{D}_L$ and $\mathcal{L}_{U}$ the unsupervised objective optimized over $\mathcal{D}_U$. We introduce $\alpha \in \mathbb{R}_{\geq 0}$ as an arbitrary scalar controlling the contribution of the unsupervised objective.

\paragraph{Supervised objective}
We define the supervised objective $\mathcal{L}_{S}(\theta)$ as
\begin{align*}
  \mathcal{L}_{S}(\theta)
    &\triangleq \sum_{(\mathbf{x}, \mathbf{y}) \in \mathcal{D}_L} \log p_{\theta} (\mathbf{x}, \mathbf{y}) \\
\end{align*}
This objective is optimized as usual using stochastic gradient estimates:
\begin{align*}
  \nabla_{\theta} \mathcal{L}_{S}(\theta)
    &\approx \frac{N}{K} \sum_{(\mathbf{x}, \mathbf{y}) \in \mathcal{B}} \nabla_{\theta} \log p_{\theta} (\mathbf{x}, \mathbf{y}), \\
\end{align*}
where $\mathcal{B} \subseteq \mathcal{D}_U$ is a mini-batch of size $K$ sampled uniformly from the dataset. We rely on automatic differentiation to compute $\nabla_{\theta} \log p_{\theta}(\mathbf{x}, \mathbf{y})$ \citep{Baydin+2017:AD}.

\paragraph{Unsupervised objective}
We define the unsupervised objective $\mathcal{L}_{U}(\theta, \lambda)$ as
\begin{subequations}
\begin{align*}
  \mathcal{L}_{U}(\theta, \lambda)
    &\triangleq \sum_{\mathbf{x} \in \mathcal{D}_U} \log p (\mathbf{x}) \\
    &= \sum_{\mathbf{x} \in \mathcal{D}_U} \log \sum_{ \mathbf{y} \in \mathcal{Y}(\mathbf{x})} p_{\theta}(\mathbf{x}, \mathbf{y})
\end{align*}
\end{subequations}
This is a language modelling objective, in which we treat $\mathbf{y}$ as latent. We have noted the a consequence the lack of independence assumptions of the RNNG is that the sum over trees $\mathbf{y}$ is not tractable. To optimize this objective we must thus fall back on approximate methods.

\section{Variational approximation} We optimize the objective using variational inference \citep{Blei+2016:VI}. We introduce a posterior $q_{\lambda}(\mathbf{y} | \mathbf{x})$ parametrised by $\lambda$ and use Jensen's inequality to derive a variational lower bound on the objective $\mathcal{L}_U(\theta, \lambda)$. First we bound the probability of one $\mathbf{x}$ in $\mathcal{D}_U$
\begin{align}
  \log p (\mathbf{x})
    &= \log \sum_{\mathbf{y}  \in \mathcal{Y}(\mathbf{x})} q_{\lambda}(\mathbf{y} |\mathbf{x}) \frac{p_{\theta}(\mathbf{x},\mathbf{y} )}{q_{\lambda}(\mathbf{y} | \mathbf{x})} \\
    &= \log \mathbb{E}_{q} \bigg[\frac{p_{\theta}(\mathbf{x},\mathbf{y} )}{ q_{\lambda}(\mathbf{y} | \mathbf{x})} \bigg] \\
    &\geq \mathbb{E}_{q} \bigg[\log \frac{p_{\theta}(\mathbf{x},\mathbf{y} )}{q_{\lambda}(\mathbf{y} | \mathbf{x})} \bigg] \\
    &= \mathbb{E}_{q} \big[\log p_{\theta}(\mathbf{x},\mathbf{y} )  - \log q_{\lambda}(\mathbf{y} | \mathbf{x}) \big]
  \label{eq:lowerbound}
\end{align}
and write
\begin{align*}
  \mathcal{E}(\theta, \lambda)
   &\triangleq \sum_{\mathbf{x} \in \mathcal{D}_U} \mathbb{E}_{q} \big[\log p_{\theta}(\mathbf{x},\mathbf{y} )  - \log q_{\lambda}(\mathbf{y} | \mathbf{x}) \big] \\
   &\leq \sum_{\mathbf{x} \in \mathcal{D}_U} \log p (\mathbf{x}) \\
   &= \mathcal{L}_{U}(\theta, \lambda)
\end{align*}
as a lower bound on our true objective $\mathcal{L}_{U}$. This is a particular instance of the expectation lower bound (ELBO) \citep{Blei+2016:VI}. The quantity can be rewritten to reveal an entropy term H($q$)
\begin{align}
  \mathbb{E}_{q} \big[\log p_{\theta}(\mathbf{x},\mathbf{y} )  - \log  q_{\lambda}(\mathbf{y} | \mathbf{x}) \big]
    &=\mathbb{E}_{q} \big[\log p_{\theta}(\mathbf{x},\mathbf{y} ) \big]  - \mathbb{E}_{q} \big[\log q_{\lambda}(\mathbf{y} | \mathbf{x}) \big] \\
    &= \mathbb{E}_{q} \big[\log p_{\theta}(\mathbf{x},\mathbf{y} ) \big]  + \text{H}(q).
  \label{eq:expectation}
\end{align}
This has an intuitive interpretation because $\cdots$

\paragraph{Posterior}
The posterior $q$ can be any kind of models, with the only condition that
\begin{equation}
  p(\mathbf{x}, \mathbf{y}) > 0 \Rightarrow q( \mathbf{y}| \mathbf{x} ) > 0 \qquad \text{for all $ \mathbf{x} $ and $ \mathbf{y} \in \mathcal{Y}( \mathbf{x} )$}.
\end{equation}
This condition is fulfilled by any discriminatively trained parser with the same support as the joint RNNG $p$. We have two obvious choices at hand: the discriminatively trained RNNG, and the CRF parser that we introduced in chapter \ref{04-crf}.

Their is an interesting advantage of CRF parser: in the case that $q$ is the CRF parser, we \textit{can} compute the entropy H$(q)$ exactly. This contrasts with the discriminative RNNG, where H$(q)$ can only be approximated.


\section{Optimization}
Just like the supervised objective $\mathcal{L}_U$ we optimize the lower bound $\mathcal{E}$ by gradient optimization. This means that we need to compute the gradients $\nabla_{\theta} \mathcal{E}(\theta, \lambda)$ and $\nabla_{\lambda} \mathcal{E}(\theta, \lambda)$.

\paragraph{Gradients of joint parameters} The first gradient is easy and permits a straightforward Monte-Carlo estimate:
\begin{subequations}
\begin{align*}
    \nabla_{\theta} \mathcal{E}(\theta, \lambda)
        &= \nabla_{\theta} \mathbb{E}_{q} \big[\log p_{\theta}(x,y)  - \log q_{\lambda}(y | x) \big] \\
        &= \mathbb{E}_{q} \big[ \nabla_{\theta} \log p_{\theta}(x,y)  -  \nabla_{\theta} \log q_{\lambda}(y | x) \big] \\
        &= \mathbb{E}_{q} \big[ \nabla_{\theta} \log p_{\theta}(x,y) \big] \\
        &\approx \frac{1}{n}\sum_{i=1}^n  \nabla_{\theta} \log p_{\theta}(x,y^{(i)})
\end{align*}
\end{subequations}
where $y_i \sim q_{\lambda}(y | x)$ for $i=1,\dots,n$. We can move the gradient inside the expectation (second equality) because $q$ does not depend on $\theta$.

\paragraph{Gradients of posterior parameters}
The second gradient is harder and requires us to rewrite the objective:
\begin{subequations}
\begin{align*}
\nabla_{\lambda} \mathcal{E}(\theta, \lambda)
        &= \nabla_{\lambda} \mathbb{E}_{q} \big[\log p_{\theta}(x,y)  - \log q_{\lambda}(y | x) \big] \\
        &= \nabla_{\lambda} \sum_{y} \big\{ q_{\lambda}(y | x) \log p_{\theta}(x,y) - q_{\lambda}(y | x)\log q_{\lambda}(y | x) \big\} \\
        &= \sum_{y} \big\{ \nabla_{\lambda} q_{\lambda}(y | x) \log p_{\theta}(x,y) \\
        &\qquad\qquad - \nabla_{\lambda} q_{\lambda}(y | x)\log q_{\lambda}(y | x) \\
        &\qquad\qquad -  q_{\lambda}(y | x)\nabla_{\lambda}\log q_{\lambda}(y | x) \big\} \\
        &= \sum_{y} \big\{ \nabla_{\lambda} q_{\lambda}(y | x) \log p_{\theta}(x,y) - \nabla_{\lambda} q_{\lambda}(y | x)\log q_{\lambda}(y | x) \big\} \\
        &= \sum_{y} \big\{(\log p_{\theta}(x,y) - \log q_{\lambda}(y | x))\nabla_{\lambda} q_{\lambda}(y | x) \big\} \\
        &= \sum_{y} \big\{(\log p_{\theta}(x,y) - \log q_{\lambda}(y | x)) q_{\lambda}(y | x)\nabla_{\lambda} \log q_{\lambda}(y | x) \big\}   \\
        &= \mathbb{E}_{q} \big[ (\log p_{\theta}(x,y) - \log q_{\lambda}(y | x))  \nabla_{\lambda} \log q_{\lambda}(y | x) \big\} \\
        &= \mathbb{E}_{q} \big[ l(x,y) \nabla_{\lambda} \log q_{\lambda}(y | x) \big]
\end{align*}
\end{subequations}
Where we've defined a learning signal $l(x,y) \triangleq \log p_{\theta}(x,y) - \log q_{\lambda}(y | x)$.

In this derivation we used the identity
\begin{align*}
    \nabla_{\lambda}\log q_{\lambda}(y | x) &= \frac{\nabla_{\lambda} q_{\lambda}(y | x)}{q_{\lambda}(y | x)}
\end{align*}
or equivalently
\begin{align*}
    \nabla_{\lambda} q_{\lambda}(y | x) &= q_{\lambda}(y | x)\nabla_{\lambda}\log q_{\lambda}(y | x)
\end{align*}
and the fact that
\begin{subequations}
\begin{align*}
    \sum_{y} q_{\lambda}(y | x)\nabla_{\lambda}\log q_{\lambda}(y | x)
        &= \sum_{y}  q_{\lambda}(y | x) \frac{\nabla_{\lambda} q_{\lambda}(y | x)}{q_{\lambda}(y | x)}  \\
        &= \sum_{y} \nabla_{\lambda} q_{\lambda}(y | x) \\
        &= \nabla_{\lambda} \sum_{y} q_{\lambda}(y | x)\\
        &= \nabla_{\lambda} 1 \\
        &= 0. \\
\end{align*}
\end{subequations}

This rewritten objective permits a straightforward MC estimate:
\begin{align}
\label{eq:score-function-estimator}
    \mathbb{E}_{q} \big[ l(\mathbf{x}, \mathbf{y}) \nabla_{\lambda} \log q_{\lambda}(y | x) \big]
        &\approx \frac{1}{n}\sum_{i=1}^n  l(\mathbf{x}, \mathbf{y}^{(i)})\nabla_{\lambda} \log q_{\lambda}(x|y^{(i)})
\end{align}
where $y_i \sim q_{\lambda}(y | x)$ for $i=1,\dots,n$.

This estimator has been derived (in slightly different forms) in among others \citep{Williams1992:REINFORCE}, \citep{Paisley+2012:VISS}, \citep{Mnih+2014:NVIL}, \citep{Ranganath+2014:BBVI}, and \citep{Miao+2016} and is known as the \textsc{reinforce} estimator \citep{Williams1992:REINFORCE}, or score function estimator (after the score function $\nabla_{\theta} \log p_{\theta}(x)$) \citep{Fu2006}.

\section{Variance reduction}
We introduce the two baselines:
\begin{itemize}
  \item Feedforward baseline \citep{Miao+2016}.
  \item Argmax baseline from \citet{Rennie+2017:argmax-baseline} which is exact in the CRF, and approximate in the RNNG.
  \item The CRF has no variance in estimating the entropy.
\end{itemize}

\section{Experiments}
\begin{itemize}
  \item Experiments with the two baselines and the two posteriors.
  \item Compare to a simple baseline: supervised learing on mixed gold-silver trees (partially predicted).
  \item Analyze the variance reduction provided by the different baselines.
\end{itemize}


\section{Related work}
\begin{itemize}
  \item Discrete latent variables in neural models \citep{Miao+2016,yin2018structvae}.
\end{itemize}
