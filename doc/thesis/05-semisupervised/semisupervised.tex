The generative RNNG defines a joint distribution over trees and sentences. In chapter \ref{03-rnng} we showed how this distribution can be estimated from labeled data; in this chapter we show how estimation can be extended to unlabeled data. We optimize a tractable lower bound on the marginal log likelihood using amortized variational inference with an approximate posterior parametrized by a neural network---a procedure reminiscent of the importance sampling inference, and both the discriminative RNNG as the CRF can be used. To estimate gradients we use samples from the posterior, and the CRF particularly excels in this role: the entropy term in the lower bound can be computed exactly, and the global normalization of the distribution makes the model a well behaved sampler. The lower bound allows us to formulate a semisupervised training objective. This in principle allows us to add any amount of unlabeled data to the already existing labeled data, thus extending the training to both more data and different domains. A further question is whether the RNNG can be estimated from unlabeled data alone. This is particularly challenging because the RNNG makes no independence assumptions and as such it is questionable whether a model so expressive possesses enough inductive bias to induce any consistent structure without supervision. Very recently, concurrent work by \citet{kim2019unsupervised} has shown the success of this approach for unlabeled binary trees in an approach that is remarkably similar to ours, also making use of a CRF inference model.

In this chapter:
\begin{itemize}
  \item We describe how the RNNG can be used to learn from unlabeled data using amortized variational inference with an approximate posterior.
  \item We show how to obtain estimates of the gradients of the lowerbound by using the score function estimator, and show how to reduce the variance of this estimator using a parsing based baseline.
  \item We describe how the discriminative RNNG and the CRF parser introduced in chapter 4 both can be used as approximate posterior, but emphasize how a globally normalized CRF excels in this role.
  \item We describe experiments with semisupervised and unsupervised training---with both the CRF and RNNG as posteriors, for both labeled an unlabeled trees, with and without supervised pretraining---and obtain no definitive results, but analyze our preliminary findings.
  \item We conclude that with the appropriate changes to the CRF posterior, the unsupervised and unlabeled learning of the RNNG could prove succesful, indicated by the very recent success of \citet{kim2019unsupervised} with this approach for unlabeled binary trees. Preliminary work shows that the changes solve the problem, but that inducing non-trivial trees will require more finetuned optimization strategies, possibly using the methods used by \citet{kim2019unsupervised}.
\end{itemize}

\section{Unsupervised learning}
  We first describe how the joint distribution of the RNNG can be estimated from unlabeled data by maximizing the marginal likelihood, making the trees a latent variable. We derive the lower bound and describe how the choice of approximate posterior affects the optimization of it.

  \subsection{Variational approximation}
    The joint log likelihood $\log p_{\theta}(\x, \y)$ of the generative RNNG defines a marginal likelihood
    \begin{align*}
      \log \ptheta(\x) = \log \sum_{\y \in \yieldx} \ptheta(\x, \y).
    \end{align*}
    Optimizing this with respect to $\theta$ directly is intractable due to the sum over trees and so we must resort to an approximate method. We use variational inference \citep{jordan1999vi,blei2016vi} and introduce a posterior $\qlambda(\y | \x)$ parametrised by $\lambda$ and use Jensen's inequality to derive a variational lower bound on the marginal likelihood:
    \begin{align}
      \label{eq:lowerbound}
      \log p (\x)
        &= \log \sum_{\y  \in \yieldx} \qlambda(\y |\x) \frac{\ptheta(\x,\y )}{\qlambda(\y | \x)} \nonumber  \\
        &= \log \expect_{\qlambda} \bigg[ \frac{\ptheta(\x,\y )}{ \qlambda(\y | \x)} \bigg] \nonumber  \\
        &\geq \expect_{\qlambda} \bigg[ \log \frac{\ptheta(\x,\y )}{\qlambda(\y | \x)} \bigg].  \nonumber \\
        &= \expect_{\qlambda} [\log \ptheta(\x,\y )  - \log \qlambda(\y | \x) ].
    \end{align}
    This is called the evidence lower bound (ELBO) \citep{blei2016vi}, and we define it as a function of parameters $\theta$ and $\lambda$ given a single datapoint $\x$ as
    \begin{align}
      \elbo(\theta, \lambda; \x) = \expect_{\qlambda} \bigg[ \log \frac{\ptheta(\x,\y )}{\qlambda(\y | \x)} \bigg].
    \end{align}

    The objective is optimized with respect to both the generative and inference parameters. We can rewrite it in two ways, each providing different perspective on this optimization procedure. On the one hand we can write the as \citep{blei2016vi}
    \begin{align*}
      \elbo(\theta, \lambda; \x) &= \log \ptheta(\x) - \kl(\qlambda(\y \mid \x) \rvert\lvert \ptheta(\y \mid \x)).
    \end{align*}
    This reveals that maximizing the ELBO is equivalent to minimizing the KL divergenge between the approximate posterior $\qlambda(\y \mid \x)$ and the true posterior $\ptheta(\y \mid \x)$, while maximizing the marginal log likelihood $\log \ptheta(\x)$.\footnote{The KL divergence is a measure of divergence between two probability distributions. It is asymetric and positive, and $\kl(q \rvert\lvert p) = 0$ only when $p = q$.} Because the KL is positive, this equation tells us that the gap between the ELBO and the true log marginal $\ptheta(x)$ is given by how well $\qlambda$ approximates the true posterior. The gap would vanish if these equal each other, but this is an unlikely event, and can be impossible in the case that the support of $\qlambda$ is strictly smaller than that of $\ptheta$.

    Because the true posterior $\ptheta(\y \mid \x)$ cannot be computed efficiently---for the same reason that we cannot compute the marginalization---this formulation has no purposes for us other than theoretical insight. The alternative formulation will have practical purpose:\footnote{We move to a slightly different notation for the entropy, emphasizing the distribution, and write $\entropy(\qlambda(\y \mid \x)) \defeq - \sum_{\y \in \yieldx} \qlambda(\y \mid \x) \log \qlambda(\y \mid \x)$.}
    \begin{align}
      \elbo(\theta, \lambda; \x)
        &= \expect_{\qlambda} [ \log \ptheta( \x, \y ) ] - \expect_{\qlambda} [ \log \qlambda(\y | \x) ]  \nonumber \\
        &= \expect_{\qlambda} [ \log \ptheta( \x, \y ) ] + \entropy(\qlambda(\y \mid \x)).
    \end{align}
    The first term is the expectation of the joint model under the the posterior distribution, and the second is the entropy of that distribution. This reveals that the objective is twofold: the posterior is encouraged to put its mass on those trees that have likelihood under the joint model, while the entropy regularizes $\qlambda$ from overly concentrating probability mass. The first part of the objective is now in the form of an expectation which we can approximate using samples from the our approximate posterior. Whether the entropy can be computed depends on the choice of posterior, and can otherwise be estimated with samples as well.

  \subsection{Approximate posterior}
    As approximate posterior we can use both the RNNG and CRF: both models satisfy the condition that their support is a subset of the support of the true posterior $p(\y \mid \x)$, which is required for the ELBO optimization \citep{kucukelbir2017automatic}. The support of the RNNGs match up. The support of the CRF is a strict subset, because although it can handle common unary chains, it cannot handle the arbitrary chains that are in the support of the generative RNNG. Because this is a marginal phenomenon the space outside the reach of the CRF will be low density anyhow.

\section{Training}
  We use the ELBO to formulate a semisupervised and an unsupervised optimization objective. Let $\dataset_L = \{ (\x^{(n)}, \y^{(n)})\}_{n=1}^N$ be the familiar dataset from the supervised training, and let $\dataset_U = \{ \x^{(m)} \}_{m=1}^M$ be an unlabeled dataset consisting merely of sentences $\x$. The unsupervised objective is then to maximize
  \begin{align}
    \elbo(\theta, \lambda) = \sum_{ x \in \dataset_U } \elbo(\theta, \lambda; \x),
  \end{align}
  while the semisupervised objective combines the supervised and the unsupervised objective into one as
  \begin{align*}
    \objective(\theta, \lambda) = \objective_{S}(\theta) + \elbo(\theta, \lambda),
  \end{align*}
  where $\objective_{S}(\theta) = \sum_{ (\x, \y) \in \dataset } \log \ptheta(\x, \y)$ is the supervised objective.

  The objectives are optimized with gradient based optimization, which means that we need to compute the gradients $\nabla_{\theta} \elbo(\theta, \lambda)$ and $\nabla_{\lambda} \elbo(\theta, \lambda)$. These gradients are estimated with samples from the posterior, and the form of those estimates will depend on the posterior model used. With the CRF as posterior, we write ELBO as
  \begin{align}
    \elbo_{\text{CRF}}(\theta, \lambda; \x)
      &= \expect_{\qlambda} [ \log \ptheta( \x, \y ) ] + \entropy(\qlambda(\y \mid \x)),
  \end{align}
  to emphasize that we can compute the entropy exactly and that only the first part of the sum needs to be estimated. With the RNNG as posterior we write the ELBO as
  \begin{align}
    \elbo_{\text{RNNG}}(\theta, \lambda; \x)
      &= \expect_{\qlambda} [ \log \ptheta( \x, \y ) - \log \qlambda(\y | \x) ],
  \end{align}
  emphasizing that the entire quantity needs to be estimated. We stress however, that this is a purely practical distinction for the objectives are analytically identical.

  \subsection{Gradients of generative model}
    Computing the gradient with respect to $\theta$ is easy and permits a straightforward Monte-Carlo estimate:
    \begin{align}
      \nabla_{\theta} \elbo_{\text{CRF}}(\theta, \lambda; \x)
        &= \nabla_{\theta} \expect_{\qlambda} [ \log \ptheta(\x, \y) ]  + \nabla_{\theta} \entropy(\qlambda(\y \mid \x))  \nonumber \\
        &= \expect_{\qlambda} [ \nabla_{\theta} \log \ptheta(\x, \y) ]  \nonumber \\
        &\approx \frac{1}{K}\sum_{k=1}^K  \nabla_{\theta} \log \ptheta(\x, \y^{(k)})
    \end{align}
    where $\y^{(k)}$ are independent samples from the approximate posterior $\qlambda( \cdot | \x)$. We can move the gradient inside the expectation because $q$ does not depend on $\theta$ and for the same reason the gradient of the entropy is zero. In the case of the RNNG posterior we end up with the same estimator:
    \begin{align}
      \nabla_{\theta} \elbo_{\text{RNNG}}(\theta, \lambda; \x)
        &= \nabla_{\theta} \expect_{\qlambda} [ \log \ptheta(\x, \y) ]  - \log \qlambda(\y | \x) ]  \nonumber \\
        &= \expect_{\qlambda} [ \nabla_{\theta} \log \ptheta(\x, \y) ]  \nonumber \\
        &\approx \frac{1}{K}\sum_{k=1}^K  \nabla_{\theta} \log \ptheta(\x, \y^{(k)})
    \end{align}

  \subsection{Gradients of inference model}
    Computing the gradient with respect to $\lambda$ is less straightforward. The difference will be in the expectation: where in the previous derivations we could freely exchange gradients and expectations now we cannot. This requires us to rewrite the gradient as the \textit{score function estimator} \citep{fu2006gradient}.  We will first derive the estimator for the RNNG ELBO and define the \textit{learning signal} as everything that is inside the expectation
    \begin{equation}
      L_{\lambda}(\x, \y) = \log \ptheta(\x, \y) - \log \qlambda(\y | \x).
    \end{equation}
    We then use equality \ref{eq:score-function-estimator} to derive
    \begin{align}
      \nabla_{\lambda} \elbo_{\text{RNNG}}(\theta, \lambda)
        &= \nabla_{\lambda} \expect_{\qlambda} [ L_{\lambda}(\x, \y) ]  \nonumber \\
        &= \expect_{\qlambda} [ L_{\lambda}(\x, \y) \nabla_{\lambda} \log \qlambda(\y | \x) ]
    \end{align}
    In this rewritten form the gradient is an expectation, which does permit a straightforward MC estimate:
    \begin{align}
      \expect_{\qlambda} [ L_{\lambda}(\x, \y) \nabla_{\lambda} \log \qlambda(\y | \x) ]
        &\approx \frac{1}{K}\sum_{k=1}^K  L_{\lambda}(\x, \y^{(k)})\nabla_{\lambda} \log \qlambda(\x | \y^{(k)})
    \end{align}
    where again $y^{(k)}$ are independent samples from $\qlambda(\cdot | \x)$.

    For the CRF posterior we need the score function estimator only for the part inside the expectation---the gradient of the entropy can be computed exactly. We thus define
    \begin{equation}
      L_{\lambda}(\x, \y) = \log \ptheta(\x, \y),
    \end{equation}
    and derive
    \begin{align}
      \nabla_{\lambda} \elbo_{\text{CRF}}(\theta, \lambda)
        &= \nabla_{\lambda} \expect_{\qlambda} [ L_{\lambda}(\x, \y) ] +  \nabla_{\lambda} \entropy(\qlambda(\y \mid \x))  \nonumber  \\
        &= \expect_{\qlambda} [ L_{\lambda}(\x, \y) \nabla_{\lambda} \log \qlambda(\y | \x) ] + \nabla_{\lambda} \entropy(\qlambda(\y \mid \x)).
    \end{align}
    The computation of $\entropy(\qlambda(\y \mid \x))$ is fully differentiable ($\cf$ \ref{eq:crf-entropy}), and so we can rely on automatic differentiation to compute $\nabla_{\lambda} \entropy(\qlambda(\y \mid \x))$.

    Estimators of this form have been derived by \citet{williams1992reinforce,paisley2012viss,mnih2014nvil,ranganath2014black} and \citet{mnih2016variational}, and is also known as the \textit{reinforce estimator} \citep{williams1992reinforce}.

  \subsection{Variance reduction}
    The score function estimator is unbiased but is known to have high variance---often too much to be useful in practice  \citep{paisley2012viss}. To reduce variance we use a data dependent baseline $b(\x)$ and redefine the estimator as
    \begin{align}
      \frac{1}{K}\sum_{k=1}^K  (L_{\lambda}(\x, \y^{(k)}) - b(\x))\nabla_{\lambda} \log \qlambda(\x | \y^{(k)}).
    \end{align}
    The more the value $b(\x)$ correlates with with $L_{\lambda}(\x, \y^{(k)})$ the greater the reduction in variance, but the baseline cannot depend on the samples $\y^{(k)}$ ($\cf$ appendix \ref{A4-vi}). We use a clever baseline introduced by \citet{rennie2017argmax} which is based on the argmax decoding of the posterior, that \texit{almost} depends on the samples without being a control variate. Letting $\hat{\y} = \argmax_y \qlambda(\y \mid \x)$ we define
    \begin{align}
      b(x) = L_{\lambda}(\x, \hat{\y}).
    \end{align}
    The reasoning is elegant: the samples will tend to look like $\hat{\y}$ when the mass of the $\qlambda$ concentrates, in turn making $L_{\lambda}(\x, \hat{\y})$ close to $L_{\lambda}(\x, \y^{(k)})$. And while this is a data dependent baseline it involves no additional parameters, in contrast with baselines based on feedforward networks \citep{mnih2014nvil, miao2016discrete} or RNN language models \citep{yin2018structvae}. For the RNNG, we compute use the gready approximation $\y^*$ but with the CRF we can obtain $\hat{\y}$ exactly.

\section{Experiments}
  The above training objectives give us a range of options to investigate, and we explore them all. We have two posteriors at our disposal, labeled and unlabeled data, and the option to work with unlabeled trees. Yet unfortunately, none of the experiments we describe will lead to any defnitive results. But while we show how some approaches do not work for reasons practical and theoretical, other directions look promising. One stands out: to use the CRF as posterior, on unlabeled trees, in the semisupervised and even unsupervised learning setting. In fact concurrent work by \citet{kim2019unsupervised}---recently published and remarkably similar to ours---shows that, with the proper optimization strategies, such an approach can be made to work with unlabeled, binary trees. Our approach differs from theirs because we dot not restrict our CRF to binary trees, and use the original formulation of the RNNG and not a simplified architecture. Unfortunately---as we will describe---the derivational ambiguity in the CRF comes back with a vengeance, and our attempts at this approach strand. For now.

  We now describe the experiments that we performed, and the preliminary results that we obtained. We use the same model architectures as before, and details about optimization and unlabeled data are in appendix \ref{A2-implementation}. We finish on a positive and describe the future work that will make it likely for our CRF approach to succeed.

  \subsection{RNNG posterior}
    We experimented with the discriminate RNNG posterior in the semisupervised setting, using the One Billion Word benchmark to obtain unlabeled data. Altough we use pretrained models we do not obtain succes. We consistenly find that the model deteriorates to pathological trees, typically producing endless unary chains of the same symbol. This automatically stops the training by causing problems with numerical stability in the action distributions. Although we could invest more time in trying to make the transition-based posterior more stable---the results of a similar setup for semantic parsing also with a transition based posterior by \citet{yin2018structvae} did show succes---we decide against further exploration, and focuss on the CRF posterior instead.

  \subsection{CRF posterior}
    Initial experiments with a pretrained CRF on the semisupervised objective---analog to the RNNG experiment---show us the practical limitations of the CRF posterior: this combination is strikingly slow. We fit not even one epoch in 48 hours, and decide against further investigation.

    On unlabeled trees the CRF is a lot faster.\footnote{The unlabeled CRF is over 8 times as fast as the CRF with the full 100+ PTB labelset (this includes all the labels resulting from our treatment of unary chains as unique lables). With the speed of the CRF, this is the difference between a few days and a few weeks.} Using the CRF for unlabeled parsing is straightforward. We use two labels: the dummy label and a label $X$ that all constituents start with. We experiment with the CRF for unlabeled trees and observe promising stability that we did not observe with the RNNG, even without any pretraining.

    We run into the problem of the derivational ambiguity, however, which halts this investigation. We have noted the problem in chapter \ref{04-crf}, but here the problem is even more pronounced because there is just one label besides the dummy label. In this case the problem boils down to the computation of the entropy: with the current setup of the parser forest, we compute the entropy over \textit{derivations}, not the entropy over \textit{trees}. Our optimization exploits this error in a brilliant way: the CRF posterior collapses to a single trivial \textit{tree}---with all the leaves directly under a single root node---while it learns a perfectly uniform distribution\footnote{Easily achieved by the CRF by assigning the exact same score to each labeled span.} over the \textit{many derivations} that collapse to that trivial tree.\footnote{This set corresponds to the number of binary trees over that sentence (all of which have the same dummy label). For a sentence of length $n$, the size of that set is given by the $(n-1)$th Catalan number $C_{n-1}$, where $C_n = \frac{1}{n+1} \binom{2n}{n}$ is defined in terms of binomial coefficients. Note that for even modest $n$, this numer is \textit{enormous}: for $n=26$  (a sentence of modest length), $C_{n-1} = 4,861,946,401,452$.}. Because that number is very large, the entropy over derivations is in this way very high, but since we sample the same (collapsed) tree the entropy over trees is in fact 0. Note though, that altough this breaks our computation of the entropy, the \textit{sampled trees} obtained from the posterior (note: not their probabilities) still reflect the proper probability distribution: the sampling procedure implictly marginalized over the derivations.

    The way forward from here is clear---we have already discussed the core solution in chapter \ref{04-crf}. Altering the inference algorithms will remove the derivational ambiguity and allows us to compute the correct entropy term. Furthermore, we can learn from the optimization details of \citet{kim2019unsupervised}, who found that their approach required considerable finetuning to avoid the posterior to collapse to trivial trees.\footnote{Binary right branching trees in their case.} This required separate optimizers for the generative and inference models, annealing the posterior entropy, and freezing the posterior after two epochs \citep{kim2019unsupervised}. This could turn out to be necessary for our approach as well.

\section{Related work}
  Our approach is informed by work in (neural) variational inference with (discrete) latent variables using the score function estimator \citep{paisley2012viss,mnih2014nvil,ranganath2014black,mnih2016variational}. In language, this approach has been applied to---among others---text summarization \citep{miao2016discrete}, semisupervised semantic parsing \citep{yin2018structvae}, and tree induction in the context of classification \citep{yogatama2016reinforcement}.

  Close to our approach is \citet{yogatama2016reinforcement}, who induce binary tree structures in a classification task by maximizing the expected log likelihood of a tree-structured neural classifier under a distribution described by a shift-reduce parser. Their use of a shif-reduce parser conjures up our use of the RNNG as approximate posterior, and they also obtain gradient estimates using the score function estimator. Although this method is highly consistent over random restarts, it invariably produces trivial left-branching trees \citep{williams2018latent}.

  Similarly close is the work of \cite{yin2018structvae} who use a transition based semantic parser as posterior, and a joint model that factorizes as a prior over semantic trees and a conditional model that generates sequences given those trees. Their transition-based posterior did not wander off like did ours, possibly helped by their use of trainable baselines based on a language model, clipping of the learning signal, or the joint model itself with its different factorization from the RNNG.

  Semisupervised learning of the RNNG has been investigated by \citet{cheng2017rnng}, who like us derive a lower bound on the marginal log likelihood and use the discriminative RNNG as approximate posterior. It appears however that they use pretrained models, keeping the parameters of the approximate posterior fixed while updating only the parameters of the generative models, based on the samples of the approximate posterior.\footnote{\textit{Cf.} this conference talk: https://vimeo.com/234957682\#t=601s.} Although this does circumvent the need for gradient estimation using the score function estimator, it does not constitute variational inference. Instead of optimizing the models in tandem, this method actually fits the \textit{true} posterior to the \textit{approximate} posterior. Doing this, however, does appear to be helpful as this method of finetuning does lower the perplexity on the Penn Treebank \citep{cheng2017rnng}.

  There is a different line of work that circumvents the probabilistic marginalization of a joint model by instead computing deterministic interpolations inside the vector representations themselves, leading to differentiable layers inside the neural network that have a structural bias. An example is the work of \citet{kim2017structured} who use this approach to compute structured attention, in which the attention coefficients are given by the posterior marginals of an edge-factored CRF. Because marginal inference is differentiable, the described interpolation is. This is different from our use of a CRF. Though the structured attention can add a strong inductive bias, it is part of the \textit{parametrization} of the model, while our work focusses on the statistical model itself. That is, our trees are discrete latent variables in a joint probability distribution $p(\x, \y)$, about which we can ask such general probabilistic questions as: what is the posterior probability $p(\y \mid \x)$ of such an tree given an observation? and what is the marginal probability $p(\x)$? The interpolations computed inside the parametrization do not permit these questions as the vectors that they interpolate are not random variables.

  Variational inference for models with \textit{continuous} latent variables suggests an alternative route for dealing with structured latent variables. Arguably, the succes of amortized variational inference with continuous latent variables \citep{kingma2014vae,rezende2014dgm} is due to the \textit{reparametrization trick}, which allows the direct differentiation of the sampling operation by writing it as a deterministic computation on external random noise. This circumvents the use of the score function estimator, and the high variance that it brings along. Extending this reparametrization to discrete latent variables has been explored for integer valued variables \citep{jang2017gumbel,maddison2017concrete}. \citet{corro2018differentiable} show how the idea can be extended to tree structured variables by adding noise to a differentiable dynamic programming to obtain approximate samples that result in gradient estimates with lower variance. This enables semisupervised learning with dependency trees as latent variables, a goal similar to ours, albeit for a different grammatical representation.

  Finally, the concurrent work of \citet{kim2019unsupervised} in particular shares many ideas with our work: the use of a CRF inference model, the parametrization of the CRF following \citet{stern2017minimal}, the exact entropy computation, and even syntactic evaluation on the dataset of \citet{linzen2018targeted}. We consider these similarities to be a remarkable coincidence. Our work differs from theirs however because we do not restrict to binary trees and use the original formulation of the RNNG.
