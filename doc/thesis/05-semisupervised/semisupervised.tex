% \bibliography{../src/bibliography.bib}

In this chapter we show how the RNNG can be trained on unlabeled data. Together with the regular, supervised, objective, this derives a way to perform semisupervised training.
\begin{itemize}
  \item I formulate an unsupervised objective for the RNNG that we can combine with the supervised objective to perfom semisupervised training.
  \item I introduce an approximate posterior in the form of a discriminative parser and derive a variational lower bound on the unsupervised objective.
  \item I show how to obtain gradients for this lowerbound by rewrinting the gradient into a form that is called the score function estimator \citep{williams1992reinforce,fu2006gradient}.
\end{itemize}

\paragraph{Notation} Let $\mathcal{D}_L = \{ (\x_i, \y_i)\}_{i=1}^N$ be a labeled dataset of sentences $\x$ with gold trees $\y$, and let $\mathcal{D}_U = \{ \x_i \}_{i=1}^M$ be an unlabeled dataset consisting of just sentences $\x$.

\section{Objective}
We define the following general semi-supervised objective
\begin{align*}
  \mathcal{L}(\theta, \lambda) \defeq \mathcal{L}_{S}(\theta) + \mathcal{L}_{U}(\theta, \lambda).
\end{align*}
The supervised objective $\mathcal{L}_{S}$ is optimized over a labeled dataset $\mathcal{D}_L = \{ (\x_i, \y_i)\}_{i=1}^N$ and $\mathcal{L}_{U}$ is the unsupervised objective optimized over an unlabeled dataset $\mathcal{D}_U = \{ \x_i \}_{i=1}^M$. We introduce $\alpha \in \mathbb{R}_{\geq 0}$ as an arbitrary scalar controlling the contribution of the unsupervised objective.

\paragraph{Supervised objective}
We define the supervised objective $\mathcal{L}_{S}(\theta)$ as
\begin{align*}
  \mathcal{L}_{S}(\theta)
    &\defeq \sum_{(\x, \y) \in \mathcal{D}_L} \log \ptheta (\x, \y) \\
\end{align*}
This objective is optimized as usual using stochastic gradient estimates:
\begin{align*}
  \nabla_{\theta} \mathcal{L}_{S}(\theta)
    &\approx \frac{N}{K} \sum_{(\x, \y) \in \mathcal{B}} \nabla_{\theta} \log \ptheta (\x, \y), \\
\end{align*}
where $\mathcal{B} \subseteq \mathcal{D}_U$ is a mini-batch of size $K$ sampled uniformly from the dataset. We rely on automatic differentiation to compute $\nabla_{\theta} \log \ptheta(\x, \y)$ \citep{baydin2018automatic}.

\paragraph{Unsupervised objective}
We define the unsupervised objective $\mathcal{L}_{U}(\theta, \lambda)$ as
\begin{align*}
  \mathcal{L}_{U}(\theta, \lambda)
    &\defeq \sum_{\x \in \mathcal{D}_U} \log p (\x) \\
    &= \sum_{\x \in \mathcal{D}_U} \log \sum_{ \y \in \mathcal{Y}(\x)} \ptheta(\x, \y)
\end{align*}
This is a language modelling objective, in which we treat $\y$ as latent. We have noted the a consequence the lack of independence assumptions of the RNNG is that the sum over trees $\y$ is not tractable. To optimize this objective we must thus fall back on approximate methods.

\section{Variational approximation} We optimize the unsupervised objective using variational inference \citep{blei2016vi}. We introduce a posterior $\qlambda(\y | \x)$ parametrised by $\lambda$ and use Jensen's inequality to derive a variational lower bound on the objective $\mathcal{L}_U(\theta, \lambda)$. First we bound the likelihood of one $\x$ in $\mathcal{D}_U$
\begin{align*}
  \log p (\x)
    &= \log \sum_{\y  \in \mathcal{Y}(\x)} \qlambda(\y |\x) \frac{\ptheta(\x,\y )}{\qlambda(\y | \x)} \\
    &= \log \expect_{q} \bigg[\frac{\ptheta(\x,\y )}{ \qlambda(\y | \x)} \bigg] \\
    &\geq \expect_{q} \bigg[\log \frac{\ptheta(\x,\y )}{\qlambda(\y | \x)} \bigg] \\
    &= \expect_{q} \Big[\log \ptheta(\x,\y )  - \log \qlambda(\y | \x) \Big]
  \label{eq:lowerbound}
\end{align*}
and write
\begin{align}
  \elbo(\theta, \lambda)
   &\defeq \sum_{\x \in \mathcal{D}_U} \expect_{q} \Big[\log \ptheta(\x,\y )  - \log \qlambda(\y | \x) \Big] \nonumber \\
   &\leq \sum_{\x \in \mathcal{D}_U} \log p (\x) \nonumber \\
   &= \mathcal{L}_{U}(\theta, \lambda)
\end{align}
as a lower bound on our true objective $\mathcal{L}_{U}$. This is a particular instance of the evidence lower bound (ELBO) \citep{blei2016vi}. The quantity can be rewritten to reveal an entropy term $H(q)$:
\begin{align*}
  \expect_{q} \Big[\log \ptheta(\x,\y )  - \log  \qlambda(\y | \x) \Big]
    &=\expect_{q} \Big[\log \ptheta(\x,\y ) \Big]  - \expect_{q} \Big[\log \qlambda(\y | \x) \Big] \\
    &= \expect_{q} \Big[\log \ptheta(\x,\y ) \Big]  + H(q),
  \label{eq:expectation}
\end{align*}
% which gives this objective an intuitive interpretation. On the one hand, the objective aims to

\paragraph{Posterior}
The posterior $q$ can be any kind of models, with the only condition that for all $ \x $ and $ \y \in \yieldx$,
\begin{equation*}
  p(\x, \y) > 0 \Rightarrow q( \y| \x ) > 0.
\end{equation*}
This condition is fulfilled by any discriminatively trained parser with the same support as the joint RNNG $p$. We have two obvious choices at hand: the discriminatively trained RNNG, and the CRF parser that we introduced in chapter \ref{04-crf}.

An interesting advantage of the CRF parser is that we \textit{can} compute the entropy H$(q)$ exactly. This contrasts with the discriminative RNNG, where H$(q)$ can only be approximated. To make this explicit we introduct separate ELBO objectives:
\begin{align}
  \elbo_{\textsc{rnng}}(\theta, \lambda) &\defeq \sum_{\x \in \dataset_U}\expect_{q} \Big[\log \ptheta(\x,\y )  - \log  \qlambda(\y | \x) \Big] \\
  \elbo_{\textsc{crf}}(\theta, \lambda) &\defeq \sum_{\x \in \dataset_U} \expect_{q} \Big[\log \ptheta(\x,\y ) \Big]  + H(q).
\end{align}


\section{Optimization}
Just like the supervised objective $\mathcal{L}_U$ we optimize the lower bound $\elbo$ by gradient optimization, which means that we need to compute the gradients $\nabla_{\theta} \elbo(\theta, \lambda)$ and $\nabla_{\lambda} \elbo(\theta, \lambda)$.

\paragraph{Gradients of joint parameters} The first gradient is easy and permits a straightforward Monte-Carlo estimate:
\begin{align*}
  \nabla_{\theta} \elbo(\theta, \lambda)
    &= \nabla_{\theta} \expect_{q} \Big[\log \ptheta(\x, \y)  - \log \qlambda(\y | \x) \Big] \\
    &= \expect_{q} \Big[ \nabla_{\theta} \log \ptheta(\x, \y) \Big] \\
    &\approx \frac{1}{K}\sum_{i=1}^K  \nabla_{\theta} \log \ptheta(\x, \y_{i})
\end{align*}
where $y_i \sim \qlambda( \cdot | \x)$ for $i=1,\dots,K$ are samples from the approximate posterior. We can move the gradient inside the expectation because $q$ does not depend on $\theta$, and note that $\nabla_{\theta} \log \qlambda(\y | \x) = 0$.

\paragraph{Gradients of posterior parameters}
The second gradient is not so straightforward and requires us to rewrite the objective into a form that is called the \textit{score function estimator} \citep{fu2006gradient}. Firstly we define a \textit{learning signal}
\begin{equation}
  L(\x, \y) \defeq \log \ptheta(\x, \y) - \log \qlambda(\y | \x),
\end{equation}
and use the identity in equation \ref{eq:score-function-estimator} that we derive in the appendix
\begin{align*}
  \nabla_{\lambda} \elbo(\theta, \lambda)
    &= \nabla_{\lambda} \expect_{q} \Big[ L(\x, \y) \Big] \\
    &= \expect_{q} \Big[ L(\x, \y) \nabla_{\lambda} \log \qlambda(\y | \x) \Big].
\end{align*}
In this rewritten form the gradient is in the form of an expectation, and that does permit a straightforward MC estimate:
\begin{align}
    \expect_{q} \Big[ L(\x, \y) \nabla_{\lambda} \log \qlambda(\y | \x) \Big]
        &\approx \frac{1}{K}\sum_{i=1}^K  L(\x, \y_i)\nabla_{\lambda} \log \qlambda(\x | \y_i)
\end{align}
where again $y_i \sim \qlambda(\cdot | \x)$ for $i=1,\dots,K$ are independently sampled from the approximate posterior. This estimator has been derived in slightly different forms in \citet{williams1992reinforce,paisley2012viss,mnih2014nvil,ranganath2014black,miao2016discrete} and is also known as the \textsc{reinforce} estimator \citep{williams1992reinforce}.


\section{Variance reduction}
We introduce the two baselines:
\begin{itemize}
  \item Feedforward baseline \citep{miao2016discrete}.
  \item Argmax baseline from \citet{rennie2017argmax} which is exact in the CRF, and approximate in the RNNG.
  \item The CRF has no variance in estimating the entropy.
\end{itemize}


\section{Experiments}
\begin{itemize}
  \item Experiments with the two baselines and the two posteriors.
  \item Compare to a simple baseline: supervised learing on mixed gold-silver trees (partially predicted).
  \item Analyze the variance reduction provided by the different baselines.
\end{itemize}


\section{Problems with CRF proposal}
We map a treebank tree to \textit{a single} normal form derivation, but many normal form derivations map to a single treebank tree. This means that if we let $f: \mathcal{D}(x) \to \yieldx$ be the transformation that collapses the dummy nodes, then $f$ is such that $f(d) = y$, but the preimage of $f$ is a set of derivations $f^{-1}( y ) = D \subseteq \mathcal{D}(x)$ with $\lvert D \rvert > 1$. However, to produce normal form trees from treebank trees, we defined a function $g: \yieldx \to \mathcal{D}(x)$ that always produces one particular normal form tree.

Let us illustrate this with an example. The following two normal form trees
\begin{figure}[h]
  \begin{subfigure}[b]{0.5\textwidth}
    \center
    \begin{tikzpicture}[scale=.6]
      \input{../figures/normal-form/cnf-valid.tex}
    \end{tikzpicture}
	\end{subfigure}
	\begin{subfigure}[b]{0.5\textwidth}
    \center
    \begin{tikzpicture}[scale=.6]
      \input{../figures/normal-form/cnf-invalid.tex}
    \end{tikzpicture}
	\end{subfigure}
\end{figure}
both collapse to the same tree when dummy labels $\varnothing$ are collapsed:
\begin{center}
  (S (NP The other hungry cat) (VP meows) .)
\end{center}
% \begin{figure}[h]
%   \center
%   \begin{subfigure}[b]{0.5\textwidth}
%     \begin{tikzpicture}[scale=.6]
%       \input{../figures/normal-form/un-cnf.tex}
%     \end{tikzpicture}
%   \end{subfigure}
% \end{figure}
But going the other way, applying the normal form transformation to the above tree, we obtain the left tree, and never the right tree. This causes the problems noted above: there is derivational ambiguity, and the distribution over derivations is a different distribution from the one over the trees that they collapse to. We propose two solutions to this:
\begin{enumerate}
  \item We can dispense with the dummy label altogether, and instead alter our CRF so that it can deal directly with $m$-ary trees, for any order $m$.\footnote{The way we deal with unary chains remains unchanched} This means that we add all hyperedges with more than 2 children. The general formulation of the inside and outside recursions remain unaltered, but the specific form becomes less efficient because we now need to deal with a sum over all possible partitions of the span under a node.
  \item We keep the dummy label, but prune the hyperforest to only contain derivations that are reachable by the normal form transformation. This means that we assign probability zero to all trees that do not correspond to the cannonical normal form that we commited to by choosing the transformation. Because this transformation is very regular, we can easily incorporate this pruning in the inside and outside algorithms. Again, there is a loss in efficiency due to the loss of regularity: the forest does remain binary, but the sum over labels and splitpoint are no longer independent, and thus we cannot absorb the sums.
\end{enumerate}

We now describe either solution, starting with the solution of making the CRF deal directly with $m$-ary trees.

\subsection{Unrestricted parse forest}
  One solution, as noted, is to make the parse-forest deal with $m$-ary trees directly. This requires that for each noterminal node, we add incoming edges that contain more than two children in the tail. This needs to be done for each permissible number of children (determined by the width of the span), and for all possible partitions of that size of the span. More formally, let $(A,i,j)$ be such a node with $i + 1 < j$, so that it can expand to other noterminal nodes. Then let $i = k_0 < k_1 < k_2 < \cdots < k_m = j$ be a partition of the discrete interval $[i, j]$ into $m$ subspans. Letting $B_1, \dots, B_m$ be labels for those subspans we can form an edge that connects the set
  \begin{align*}
    \Big\{ (B_1, k_0, k_1), (B_2, k_1, k_2), \dots, (B_m, k_{n-1}, k_m) \Big\}
  \end{align*}
  at the tail to the node $(A, i, j)$ at the head. Adding \textit{all} such nodes to the parse forest will allow us to deal with $n$-ary trees directly.

  The generality of the semiring formulations of the inside and outside algorithms, allows us to seamlesly apply them to this new parse forest. If we let $v = (A, i, j)$ such that $I(v) \neq \varnothing$ we can derive the inside value $\alpha(v)$ as
  \begin{align*}
    \alpha(v)
      &= \displaystyle\bigoplus_{e \in I(v)} \omega(e) \otimes \displaystyle\bigotimes_{u \in T(e)} \alpha(u)  \\
      &= \displaystyle\bigoplus_{n = 2}^{j - i} \displaystyle\bigoplus_{k_0 < k_1 < \cdots < k_m} \displaystyle\bigoplus_{B_1 \in \Lambda} \cdots \displaystyle\bigoplus_{B_m \in \Lambda} \omega(A, i, j) \otimes \displaystyle\bigotimes_{l=1}^n \alpha(B_l, k_{l-1}, k_l) \\
      &= \omega(A, i, j) \otimes \displaystyle\bigoplus_{n = 2}^{j - i} \displaystyle\bigoplus_{k_0 < k_1 < \cdots < k_m} \displaystyle\bigotimes_{l=1}^n \displaystyle\bigoplus_{B \in \Lambda} \alpha(B, k_{l-1}, k_l) \\
      &= \omega(A, i, j) \otimes \displaystyle\bigoplus_{n = 2}^{j - i} \displaystyle\bigoplus_{k_0 < k_1 < \cdots < k_m} \displaystyle\bigotimes_{l=1}^n \sigma(k_{l-1}, k_l), \\
  \end{align*}
  where logic that allows us write
  \begin{align*}
    \displaystyle\bigoplus_{B_1 \in \Lambda} \cdots \displaystyle\bigoplus_{B_m \in \Lambda} \displaystyle\bigotimes_{l=1}^n \alpha(B_l, k_{l-1}, k_l) = \displaystyle\bigotimes_{l=1}^n \displaystyle\bigoplus_{B \in \Lambda} \alpha(B, k_{l-1}, k_l)
  \end{align*}
  is the same as in the binary case, but generalized to $m$ terms.

  For the outside value $\beta(v)$ we can follow a similar derivation. Let
  \begin{align*}
    0 \leq k_0 < \cdots < k_a = i < j = k_{a+1} < \cdots < k_m \leq n
  \end{align*}
  be a partition of the discrete interval $[k_0, k_m]$ into $m$ pieces, one of which is $[i, j]$, namely the interval $[k_a, k_{a+1}]$. This partition represents one way in wich the interval $[i, j]$ can be completed with $m-1$ other intervals into an interval $[k_0, k_m]$. With this in place, we can derive the outside recursion as
  \begin{align*}
    \beta(v)
      &= \displaystyle\bigoplus_{e \in O(v)} \omega(w) \otimes \beta(H(e)) \otimes \displaystyle\bigotimes_{ \substack{ w \in T(e) \\ w \neq u } } \alpha(w) \\
      &= \displaystyle\bigoplus_{m = 2}^{n - j + i} \displaystyle\bigoplus_{k_0=0}^{i-1} \displaystyle\bigoplus_{k_m = j}^{n} \displaystyle\bigoplus_{k_0 < \cdots < k_a < k_{a+1} < \cdots < k_m} \displaystyle\bigoplus_{B \in \Lambda}  \\
        &\qquad\qquad \omega(A, i, j) \otimes \beta(B, k_0, k_m) \otimes \displaystyle\bigotimes_{ \substack{1 \leq l \leq n \\ l \neq a } } \displaystyle\bigotimes_{C \in \Lambda} \alpha(C, k_{l-1}, k_l) \\
      &= \displaystyle\bigoplus_{m = 2}^{n - j + i} \displaystyle\bigoplus_{k_0=0}^{i-1} \displaystyle\bigoplus_{k_m = j}^{n} \displaystyle\bigoplus_{k_0 < \cdots < k_a < k_{a+1} < \cdots < k_m}  \\
        &\qquad\qquad \displaystyle\bigoplus_{B \in \Lambda} \omega(B, i, j) \otimes \beta(B, k_0, k_m) \otimes \displaystyle\bigotimes_{ \substack{1 \leq l \leq n \\ l \neq a } } \displaystyle\bigotimes_{C \in \Lambda} \alpha(C, k_{l-1}, k_l)  \\
      &= \displaystyle\bigoplus_{m = 2}^{n - j + i} \displaystyle\bigoplus_{k_0=0}^{i-1} \displaystyle\bigoplus_{k_m = j}^{n} \displaystyle\bigoplus_{k_0 < \cdots < k_a < k_{a+1} < \cdots < k_m} \sigma'(k_0, k_m) \otimes \displaystyle\bigotimes_{ \substack{1 \leq l \leq n \\ l \neq a } } \sigma(k_{l-1}, k_l).
  \end{align*}


\subsection{Pruned parse forest}
  Another solution is to prune the parse forest: to keep only those trees that can be obtained by the normal form transformation. Going back to the example at the beginning of the example, this solution asks us to assign probability zero to the right tree, the tree that we cannot obtain by the normal form transformation. Since the transformation that we apply is very regular, we can easily characterize the set of trees that are impossible to derive. In fact, it boils done to one characteristic: a node can have the label $\varnothing$ only when \textit{either} that node spans a single word \textit{or} that node is the right child of its parent node.\footnote{We binarize \textit{rightwards}, thus introducting the dummy labels in that position, and can only ever introduce the dummy label in the left child position when it spans a single word.} That means we should remove from the parse forest all edges of the form illustrated in figure \ref{fig:illegal-edges}.
  \begin{figure}[h]
    \center
    \begin{tikzpicture}[scale=.6]
      \input{../figures/edges/illegal-edges.tex}
    \end{tikzpicture}
    \caption{Edges in the parse forest that produce the derivational ambiguity, where $k > i+1$ and $A, B \in \Lambda$.}
    \label{fig:illegal-edges}
  \end{figure}

  With this characterization in hand, we can alter the recursions, starting with the inside algorithm.
  \begin{align*}
    \alpha(A, i, j)
      &= \displaystyle\bigoplus_{B \in \Lambda} \displaystyle\bigoplus_{C \in \Lambda} \omega(A, i, j) \otimes \alpha(B,i,i+1) \otimes \alpha(C,i+1,j) \\
        &\qquad\oplus \displaystyle\bigoplus_{B \in \Lambda \setminus \{ \varnothing \} } \displaystyle\bigoplus_{C \in \Lambda} \displaystyle\bigoplus_{k=i+2}^{j-1} \omega(A, i, j) \otimes \alpha(B,i,k) \otimes \alpha(C,k,j) \\
      &= \omega(A, i, j) \otimes \displaystyle\bigoplus_{B \in \Lambda} \displaystyle\bigoplus_{C \in \Lambda} \alpha(B,i,i+1) \otimes \alpha(C,i+1,j)  \\
        &\qquad\oplus \omega(A, i, j) \otimes \displaystyle\bigoplus_{B \in \Lambda \setminus \{ \varnothing \} } \displaystyle\bigoplus_{C \in \Lambda} \displaystyle\bigoplus_{k=i+2}^{j-1} \alpha(B,i,k) \otimes \alpha(C,k,j)  \\
      &= \Bigg[ \omega(A, i, j) \otimes \sigma(i,i+1) \otimes \sigma(i+1,j) \Bigg] \oplus \Bigg[ \omega(A, i, j) \otimes \displaystyle\bigoplus_{k=i+2}^{j-1} \tilde{\sigma}(i,k) \sigma(k,j) \Bigg] 
  \end{align*}
  where
  \begin{align*}
    \tilde{\sigma}(i,k) =  \displaystyle\bigoplus_{B \in \Lambda \setminus \{ \varnothing \} } \alpha(A, i, k).
  \end{align*}


\section{Related work}
\begin{itemize}
  \item Discrete latent variables in neural models \citep{miao2016discrete,yin2018structvae}.
  \item Semisupervised training for the RNNG \citep{cheng2017rnng}.
  \item Argmax baseline \cite{rennie2017argmax}.
  \item Training with the policy gradient of a risk-objective as a surrogate for training with a dynamic oracle \citep{klein2018reinforce}.
\end{itemize}
