The generative RNNG defines a joint distribution over trees and sentences. In chapter \ref{03-rnng} we showed how this distribution can be estimated from labeled data. In this chapter we show how estimation can be extended to unlabeled data. We optimize a tractable lower bound on the marginal log likelihood using amortized variational inference with an approximate posterior---a procedure reminiscent of the importance sampling inference, and both the discriminative RNNG as the CRF can be used. To compute gradients we use samples from the posterior, and the CRF particularly excels in this role: the entropy term in the lower bound can be computed exactly, and the global normalization of the distribution makes the model a well behaved sampler. The lower bound allows us to formulate a semisupervised training objective. This in principle allows us to add any amount of unlabeled data to the already existing labeled data, thus extending the training to both more data and different domains. A further question is whether the RNNG can be estimated from unlabeled data alone. This is particularly challenging because the RNNG makes no independence assumptions and as such it is questionable whether a model so expressive possesses enough inductive bias to induce any consistent structure without supervision.

[...]

In this chapter:
\begin{itemize}
  \item We describe how the RNNG can be used to learn from unlabeled data using amortized variational inference with an approximate posterior.
  \item We show how to obtain gradients for of the lowerbound by using the score function estimator, and show how to reduce the variance of this estimator using baselines.
  \item We describe how the discriminative RNNG and the CRF parser introduced in chapter 4 both can be used as approximate posterior, but emphasize how the globally normalized CRF in excels this role.
  \item We describe experiments with semisupervised and unsupervised training---with both the CRF and RNNG as posteriors, for both labeled an unlabeled trees, with and without supervised pretraining---and obtain no definitive results, but analyze our preliminary findings.
  \item We conclude that with the appropriate changes to the CRF posterior, the unsupervised and unlabeled learning of the RNNG could prove succesful, indicated by the very recent success of \citet{kim2019unsupervised} with this approach for binary trees.
\end{itemize}

\section{Unsupervised learning}
  We first describe how the joint distribution of the RNNG can be estimated from unlabeled data by maximizing the marginal likelihood, making the trees a latent variable. We derive the lower bound and describe how the choice of approximate posterior affects the optimization of it.

  \subsection{Variational approximation}
    The joint log likelihood $\log p_{\theta}(\x, \y)$ of the generative RNNG defines a marginal likelihood
    \begin{align*}
      \log \ptheta(\x) = \log \sum_{\y \in \yieldx} \ptheta(\x, \y).
    \end{align*}
    Optimizing this with respect to $\theta$ directly is intractable due to the sum over trees and so we must resort to an approximate method. We use variational inference \citep{jordan1999vi,blei2016vi} and introduce a posterior $\qlambda(\y | \x)$ parametrised by $\lambda$ and use Jensen's inequality to derive a variational lower bound on the marginal likelihood:
    \begin{align}
      \label{eq:lowerbound}
      \log p (\x)
        &= \log \sum_{\y  \in \yieldx} \qlambda(\y |\x) \frac{\ptheta(\x,\y )}{\qlambda(\y | \x)} \nonumber  \\
        &= \log \expect_{\qlambda} \bigg[ \frac{\ptheta(\x,\y )}{ \qlambda(\y | \x)} \bigg] \nonumber  \\
        &\geq \expect_{\qlambda} \bigg[ \log \frac{\ptheta(\x,\y )}{\qlambda(\y | \x)} \bigg].  \nonumber \\
        &= \expect_{\qlambda} [\log \ptheta(\x,\y )  - \log \qlambda(\y | \x) ].
    \end{align}
    This is called the evidence lower bound (ELBO) \citep{blei2016vi}, and we define it as a function of parameters $\theta$ and $\lambda$ given a single datapoint $\x$ as
    \begin{align}
      \elbo(\theta, \lambda; \x) = \expect_{\qlambda} \bigg[ \log \frac{\ptheta(\x,\y )}{\qlambda(\y | \x)} \bigg].
    \end{align}

    The objective is optimized with respect to both the generative and inference parameters. We can rewrite it in two ways, each providing different perspective on this optimization procedure. On the one hand we can write the as \citep{blei2016vi}
    \begin{align*}
      \elbo(\theta, \lambda; \x) &= \log \ptheta(\x) - \kl(\ptheta(\y \mid \x) \rvert\lvert \qlambda(\y \mid \x)).
    \end{align*}
    This reveals that maximizing the ELBO is equivalent to minimizing the KL divergenge between the approximate posterior $\qlambda(\y \mid \x)$ and the true posterior $\ptheta(\y \mid \x)$, while maximizing the marginal log likelihood $\log \ptheta(\x)$. Because the true posterior $\ptheta(\y \mid \x)$ cannot be computed efficiently---for the same reason that we cannot compute the marginalization---this formulation has no purposes for us other than theoretical insight. The alternative formulation will have practical purpose:
    \begin{align}
      \elbo(\theta, \lambda; \x)
        &= \expect_{\qlambda} [ \log \ptheta( \x, \y ) ] - \expect_{\qlambda} [ \log \qlambda(\y | \x) ]  \nonumber \\
        &= \expect_{\qlambda} [ \log \ptheta( \x, \y ) ] + \entropy_{\qlambda}(Y | X = x).
    \end{align}
    The first term is the expectation of the joint model under the the posterior distribution, and the second is the entropy of that distribution. This reveals that the objective is twofold: the posterior is encouraged to put its mass on those trees that have likelihood under the joint model, while the entropy regularizes $\qlambda$ from overly concentrating probability mass. The first part of the objective is now in the form of an expectation which we can approximate using samples from the our approximate posterior. Whether the entropy can be computed depends on the choice of posterior, and can otherwise be estimated with samples as well.

  \subsection{Approximate posterior}
    As approximate posterior we can use both the RNNG and CRF: both models satisfy the condition that their support is a subset of the support of the true posterior $p(\y \mid \x)$, which is required for the ELBO optimization \citep{kucukelbir2017automatic}. The support of the RNNGs match up. The support of the CRF is a strict subset, because although it can handle common unary chains, it cannot handle the arbitrary chains that are in the support of the generative RNNG. Because this is a marginal phenomenon the space outside the reach of the CRF will be low density anyhow.

\section{Training}
  We use the ELBO to formulate a semisupervised and an unsupervised optimization objective. Let $\dataset_L = \{ (\x_i, \y^{(k)})\}_{i=1}^N$ be the familiar dataset from the supervised training, and let $\dataset_U = \{ \x_i \}_{i=1}^M$ be an unlabeled dataset consisting merely of sentences $\x$. The unsupervised objective is then to maximize
  \begin{align}
    \elbo(\theta, \lambda) = \sum_{ x \in \dataset_U } \elbo(\theta, \lambda; \x),
  \end{align}
  while the semisupervised objective combines the supervised and the unsupervised objective into one as
  \begin{align*}
    \objective(\theta, \lambda) = \objective_{S}(\theta) + \elbo(\theta, \lambda),
  \end{align*}
  where $\objective_{S}(\theta) = \sum_{ (\x, \y) \in \dataset } \log \ptheta(\x, \y)$ is the supervised objective.

  The objectives are optimized with gradient based optimization, which means that we need to compute the gradients $\nabla_{\theta} \elbo(\theta, \lambda)$ and $\nabla_{\lambda} \elbo(\theta, \lambda)$. These gradients are estimated with samples from the posterior, and the form of those estimates will depend on the posterior model used. With the CRF as posterior, we write ELBO as
  \begin{align}
    \elbo_{\text{CRF}}(\theta, \lambda; \x)
      &= \expect_{\qlambda} [ \log \ptheta( \x, \y ) ] + \entropy_{\qlambda}(Y | X = x),
  \end{align}
  to emphasize that we can compute the entropy exactly and that only the first part of the sum needs to be estimated. With the RNNG as posterior we write the ELBO as
  \begin{align}
    \elbo_{\text{RNNG}}(\theta, \lambda; \x)
      &= \expect_{\qlambda} [ \log \ptheta( \x, \y ) - \log \qlambda(\y | \x) ],
  \end{align}
  emphasizing that the entire quantity needs to be estimated. We stress however, that this is a purely practical matter for the objectives are identical from a theoretical standpoint.

  \subsection{Gradients of generative model}
    Computing the gradient with respect to $\theta$ is easy and permits a straightforward Monte-Carlo estimate:
    \begin{align}
      \nabla_{\theta} \elbo_{\text{CRF}}(\theta, \lambda; \x)
        &= \nabla_{\theta} \expect_{\qlambda} [ \log \ptheta(\x, \y) ]  + \nabla_{\theta} \entropy_{\qlambda}( Y | X = x)  \nonumber \\
        &= \expect_{\qlambda} [ \nabla_{\theta} \log \ptheta(\x, \y) ]  \nonumber \\
        &\approx \frac{1}{K}\sum_{k=1}^K  \nabla_{\theta} \log \ptheta(\x, \y^{(k)})
    \end{align}
    where $\y^{(k)}$ are independent samples from the approximate posterior $\qlambda( \cdot | \x)$. We can move the gradient inside the expectation because $q$ does not depend on $\theta$ and for the same reason the gradient of the entropy is zero. In the case of the RNNG posterior we end up with the same estimator:
    \begin{align}
      \nabla_{\theta} \elbo_{\text{RNNG}}(\theta, \lambda; \x)
        &= \nabla_{\theta} \expect_{\qlambda} [ \log \ptheta(\x, \y) ]  - \log \qlambda(\y | \x) ]  \nonumber \\
        &= \expect_{\qlambda} [ \nabla_{\theta} \log \ptheta(\x, \y) ]  \nonumber \\
        &\approx \frac{1}{K}\sum_{k=1}^K  \nabla_{\theta} \log \ptheta(\x, \y^{(k)})
    \end{align}

  \subsection{Gradients of inference model}
    Computing the gradient with respect to $\lambda$ is less straightforward. The difference will be in the expectation: where in the previous derivations we could freely exchange gradients and expectations now we cannot. This requires us to rewrite the gradient as the \textit{score function estimator} \citep{fu2006gradient}.  We will first derive the estimator for the RNNG ELBO and define the \textit{learning signal} as everything that is inside the expectation
    \begin{equation}
      L(\x, \y) = \log \ptheta(\x, \y) - \log \qlambda(\y | \x).
    \end{equation}
    We then use equality \ref{eq:score-function-estimator} to derive
    \begin{align}
      \nabla_{\lambda} \elbo_{\text{RNNG}}(\theta, \lambda)
        &= \nabla_{\lambda} \expect_{\qlambda} [ L(\x, \y) ]  \nonumber \\
        &= \expect_{\qlambda} [ L(\x, \y) \nabla_{\lambda} \log \qlambda(\y | \x) ]
    \end{align}
    In this rewritten form the gradient is an expectation, which does permit a straightforward MC estimate:
    \begin{align}
      \expect_{\qlambda} [ L(\x, \y) \nabla_{\lambda} \log \qlambda(\y | \x) ]
        &\approx \frac{1}{K}\sum_{k=1}^K  L(\x, \y^{(k)})\nabla_{\lambda} \log \qlambda(\x | \y^{(k)})
    \end{align}
    where again $y^{(k)}$ are independent samples from $\qlambda(\cdot | \x)$.

    For the CRF posterior we need the score function estimator only for the part inside the expectation---the gradient of the entropy can be computed exactly. We thus define
    \begin{equation}
      L(\x, \y) = \log \ptheta(\x, \y),
    \end{equation}
    and derive
    \begin{align}
      \nabla_{\lambda} \elbo_{\text{CRF}}(\theta, \lambda)
        &= \nabla_{\lambda} \expect_{\qlambda} [ L(\x, \y) ] +  \nabla_{\lambda} \entropy_{\qlambda}( Y | X = x) ] \nonumber  \\
        &= \expect_{\qlambda} [ L(\x, \y) \nabla_{\lambda} \log \qlambda(\y | \x) ] + \nabla_{\lambda} \entropy_{\qlambda}( Y | X = x).
    \end{align}
    The computation of $\entropy_{\qlambda}( Y | X = x)$ is fully differentiable ($\cf$ \ref{eq:crf-entropy}), and so we can rely on automatic differentiation to compute $\nabla_{\lambda} \entropy_{\qlambda}( Y | X = x)$.

    Estimators of this form have been derived by \citet{williams1992reinforce,paisley2012viss,mnih2014nvil,ranganath2014black,mnih2016variational} and \citet{miao2016discrete}, and is also known as the \textit{reinforce estimator} \citep{williams1992reinforce}.

  \subsection{Variance reduction}
    The score function estimator is unbiased but is known to have high variance---often too much to be useful in practice  \citep{paisley2012viss}. To reduce variance we use a data dependent baseline $b(\x)$ and redefine the estimator as
    \begin{align}
      \frac{1}{K}\sum_{k=1}^K  (L(\x, \y^{(k)}) - b(\x))\nabla_{\lambda} \log \qlambda(\x | \y^{(k)}).
    \end{align}
    The more the value $b(\x)$ correlates with with $L(\x, \y^{(k)})$ the greater the reduction in variance, but the baseline cannot depend on the samples $\y^{(k)}$ ($\cf$ appendix \ref{A4-vi}). We use a clever baseline introduced by \citet{rennie2017argmax} which is based on the argmax decoding of the posterior, that \texit{almost} depends on the samples without being a control variate. Letting $\hat{\y} = \argmax_y \qlambda(\y \mid \x)$ we define
    \begin{align}
      b(x) = L(\x, \hat{\y}).
    \end{align}
    The reasoning is elegant: the samples will tend to look like $\hat{\y}$ when the mass of the $\qlambda$ concentrates, in turn making $L(\x, \hat{\y})$ close to $L(\x, \y^{(k)})$. And while this is a data dependent baseline it involves no additional parameters, in contrast with baselines based on feedforward networks \citep{mnih2014nvil, miao2016discrete} or RNN language models \citep{yin2018structvae}. For the RNNG, we compute use the gready approximation $\y^*$ but with the CRF we can obtain $\hat{\y}$ exactly.

\section{Experiments}
  The above training objectives give us a range of options to investigate, and we explore them all. We have two posteriors at our disposal, labeled and unlabeled data, and the option to work with unlabeled trees. Yet unfortunately, none of the experiments we describe will lead to any defnitive results. But while we show how some approaches do not work for reasons practical and theoretical, other directions look promising. One stands out: to use the CRF posterior as posterior, on unlabeled trees, in the semisupervised and even unsupervised learning setting. In fact concurrent work by \citet{kim2019unsupervised}---recently published and remarkably similar to ours---shows that, with the proper optimization strategies, such an approach can be made to work with unlabeled, binary trees. Our approach differs from theirs because we dot not restrict our CRF to binary trees, and use the original formulation of the RNNG and not a simpler binary TreeLSTM. Unfortunately---as we will describe---the derivational ambiguity in the CRF comes back with a vengeance, and our attempts at this approach strand. For now.

  We now describe the experiments that we performed, and the preliminary results that we obtained. We use the same model architectures as before, and details about optimization and data are in appendix \ref{A2-implementation}. We finish on a positive and describe the future work that will make it likely for our CRF approach to work.

  \subsection{RNNG posterior}
    We experimented with the discriminate RNNG posterior in the semisupervised setting, using the One Billion Word benchmark to obtain unlabeled data. Altough we use pretrained models we do not obtain succes. We consistenly find that the model deteriorates to pathological trees, typically producing endless unary chains of the same symbol. This automatically stops the training by causing problems with numerical stability in the action distributions. We decide against further exploration, and focuss on the CRF posterior instead.

  \subsection{CRF posterior}
    Initial experiments with a pretrained CRF on the semisupervised objective---analog to the RNNG experiment---show us the practical limitations of the CRF posterior: this combination is strikingly slow. We fit not even one epoch in 48 hours, and decide against further investigation.

    On unlabeled trees the CRF is a lot faster.\footnote{The unlabeled CRF takes around 25 minutes per epoch on the PTB takes around, vesus 4 hours for the labeled CRF with the full 100+ labelset.} Using the CRF for unlabeled parsing is straightforward. We use two labels: the dummy label and a label $X$ that all constituents start with. We experiment with the CRF for unlabeled trees and observe promising stability that we did not observe with the RNNG, even without any pretraining.

    We run into the problem of the derivational ambiguity, however, which halts this investigation. We have noted the problem in chapter \ref{04-crf}, but here the problem is even more pronounced because there is just one label besides the dummy label. In this case the problem boils down to the computation of the entropy: with the current setup of the parser forest, we compute the entropy over \textit{derivations}, not the entropy over \textit{trees}. Our optimization exploits this error in a brilliant way: the CRF posterior collapses to a single trivial tree---with all the leaves directly under a single root node---while it learns a perfectly uniform distribution over the \textit{many} derrivations that collapse to that trivial tree. This way the entropy over derivations is high, while we sample the same (collapsed) tree.

    The way forward from here is clear---we have already discussed the core solution in chapter \ref{04-crf}. Altering the inference algorithms will remove the derivational ambiguity and allows us to compute the correct entropy term. Furthermore, we can learn from the optimization details of \citet{kim2019unsupervised}, who found that their approach required considerable finetuning to avoid the posterior to collapse to trivial trees.\footnote{Binary right branching trees in their case.} This required separate optimizers for the generative and inference models, annealing the posterior entropy, and freezing the posterior after two epochs \citep{kim2019unsupervised}. This could turn out to be necessary for our approach as well.

\section{Related work}
  Our approach is informed by work in neural variational inference with discrete latent variables---in general \citep{paisley2012viss,mnih2014nvil,ranganath2014black,mnih2016variational} and in natural language processing in particular \citep{miao2016discrete,yin2018structvae}. The concurrent work \citet{kim2019unsupervised} shares many ideas with our work: the use of a CRF inference model, the parametrization of the CRF following \citet{stern2017minimal}, the exact entropy computation, and even syntactic evaluation on the dataset of \citet{linzen2018targeted}. We consider these similarities to be a remarkable coincidence.

  % Training with the policy gradient of a risk-objective as a surrogate for training with a dynamic oracle \citep{klein2018reinforce}.
