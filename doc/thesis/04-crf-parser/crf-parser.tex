% \bibliography{../src/bibliography}

In this chapter I introduce an alternative parser to act as proposal model in the approximate marginalization. The parser is a neural Conditional Random Field (CRF) parser that parser combines exact dynamic programming of chart-based parsing with the rich nonlinear featurization of neural networks.  The parser is an adaptation of the chart-based parser introduced in \citet{Stern+2017:Minimal}, where it is trained with a margin-based objective.

\begin{itemize}
  \item The chart-based approach allows efficient exact inference. The neural network is used exclusively to learn good representation from which to predict local scores; the global structured interactions are
\end{itemize}

\section{Model}
In this section I describe the probabilistic model of the parser.
\begin{enumerate}
  \item Introduce probabilistic model, reference CRF background in apendix
  \item Desribe how this is an adaptation from \citet{Stern+2017:Minimal} to probabilistic training.
\end{enumerate}'

\subsection{Motivation}
\begin{itemize}
  \item Key point to make: this model regards a constituency tree as a collection of \textit{labeled spans} over a sentence. Earlier models, both log-linear and neural, regard a constituency tree as a collection of \textit{anchored rules} over a sentence \citep{Finkel+2008,Klein+2015}.
  \item A model over spanned rules puts more expressiveness in the state space of the dynamic program, since rules The model in \cite{Stern+2017:Minimal} instead puts the expressiveness in the input space by using rich neural feature representations and a very unconstrained output space.
  \item Earlier approaches went evern further by enriching enriched meanse to lexicalize the rules \cite{Collins2003}; break the grammar's independence assumptions by annotating the rule with parent and sibling labels \citep{Klein+2003}.
  \item
\end{itemize}

\subsection{Features}
In this section I describe how the scores are computed from the bidirectional LSTM features.
\begin{itemize}
  \item Give formal expression for `LSTM minus features` with Feedforward scoring function
\end{itemize}

\section{Inference}
Due to the parametrization, the model allows efficient inference. In this section we describe efficient solutions to three related problems:
\begin{itemize}
  \item Find the best parse $\mathbf{y}^{*} = \arg \max_{\mathbf{y}} p(\mathbf{y} | \mathbf{x})$
  \item Compute the normalizer $Z(\mathbf{x}) = \sum_{ \mathbf{y} } \prod_{ a=1 }^{ A } \Psi( \mathbf{x}, \mathbf{y}_a )$, where $F = \{ \Psi_a \}_{a=1}^{A}$ is the set of factors in the graph.
  \item Compute the entropy conditioned on $\mathbf{x}$, $H(\mathbf{y} | \mathbf{x})$.
\end{itemize}
All three problems can be solved with a different instance of the same two algorithm: the inside algorithm and the outside algorithm.

\subsection{Inside recursion}
NOTE: This is a complete draft section, just for me to get the correct derivation for the implementation. In the actual section I will follow the semiring formulation, and connect it with the belief-propagation/message-passing/sum-product algorithm.

In this derivation we follow Michael Collins notes on the Inside-Outside Algorithm.\footnote{\url{http://www.cs.columbia.edu/~mcollins/io.pdf}}

Let a sentence be $x_1,\dots,x_n$, where each $x_i$
is a word. We are given a CFG $(N, \Sigma, R, S)$ in Chomsky normal
form. Let $\psi$ be a function that maps any rule production $r \in R$ of the form $\langle A \to B \;C, i, k, j \rangle$ or $\langle A, i, i+1 \rangle$ to a value $\psi(r) \geq 0$. Let a tree $T$ be a set of such rules $r$ with the only constraint that these rules make up a tree.

Following the minimal span parser we define $\psi$ as
\begin{align}
\label{eq:rule-score}
    \log\psi(A \to B \;C, i, k, j) &\triangleq s_{label}(i, j, A) + s_{span}(i, j) \\
    &\text{and} \\
    \log\psi(A, i, i+1) &\triangleq s_{label}(i, i+1, A) + s_{span}(i, i+1),
\end{align}
and thus the potential of a tree as
\begin{align}
\label{eq:tree-score}
    \log\Psi(T) &= \sum_{r \in T} \log\psi(r) \\
        &= \sum_{\langle A, i, j \rangle \in T} s_{label}(i, j, A) + s_{span}(i, j), \\
\end{align}
Note that the potential function as defined in \ref{eq:rule-score} disregards most of the information in a binary rule. In particular we see that $B$, $C$ and $k$, the labels and split-point of the children, are discarded.

Now note that equation \label{eq:tree-score} corresponds exactly to the second formula in section 3 of the minimal span-based parser paper
\begin{align}
s_{tree}(T) = \sum_{(\ell(i,j))\in T}[s_{label}(i, j, \ell) + s_{span}(i, j)] .
\end{align}
which is how I derived that \ref{eq:rule-score} is the correct formula for the rule score.

We obtain our CRF objective when we normalize this score globally
\begin{align}
\label{eq:crf-objective}
    P(T) &= \frac{\prod_{r \in T} \psi(r)}{\sum_{T' \in \mathcal{T}} \prod_{r' \in T'} \psi(r')} \\
\end{align}
or equivalently
\begin{align}
\label{eq:logcrf-objective}
    \log P(T) &= \sum_{r \in T} \log \psi(r) - \log \sum_{T \in \mathcal{T}} \prod_{r \in T} \psi(r) \\
\end{align}

From the aforementioned notes we get the following general result for the inside value $\alpha$. For all $A \in N$, for all $0 \leq i < n$
\begin{align}
    \label{eq:collins-inside}
    \alpha(A,i,i+1) = \psi(A, i, i+1)
\end{align}
and for all $(i, j)$ such that $1 \leq i < j \leq n$:
\begin{align}
    \label{eq:collins-inside}
    \alpha(A,i,j) = \sum_{A \to B C} \sum_{k=i+1}^{j-1} \psi(A \to B \;C, i, k, j) \cdot \alpha(B,i,k) \cdot \alpha(C,k,j)
\end{align}
% \begin{align}
% \label{eq:rule-score}
%     \log\psi(A \to B \;C, i, k, j) \triangleq s_{label}(i, j, A) + s_{span}(i, k) + s_{span}(k, j),
% \end{align}
Note that we are considering a CFG in which the rule set is complete, i.e.
\begin{align}
    \langle A \to B \;C \rangle \in R \text{ for each } (A, B, C) \in N^3,
\end{align}
and recall that the labels $B$ and $C$ do not appear in the scoring functions in \ref{eq:rule-score}. These facts will allow us to simplify the expression in formula \ref{eq:collins-inside} as
\begin{subequations}
\begin{align}
    \alpha(A,i,j) &= \sum_{B \in N} \sum_{C \in N} \sum_{k=i+1}^{j-1} \tilde{s}_{label}(i, j, A) \cdot \tilde{s}_{span}(i, j) \alpha(B,i,k) \cdot \alpha(C,k,j) \\
        &= \tilde{s}_{label}(i, j, A) \cdot \tilde{s}_{span}(i, j) \sum_{k=i+1}^{j-1} \sum_{B \in N} \alpha(B,i,k) \cdot \sum_{C \in N} \alpha(C,k,j) \\
        &= \tilde{s}_{label}(i, j, A) \cdot \tilde{s}_{span}(i, j) \sum_{k=i+1}^{j-1} S(i,k) \cdot S(k,j) \label{eq:final-inside}
\end{align}
\end{subequations}
where we've introduced a number of notational abbreviations
\begin{align}
    \tilde{s}_{label}(i, j, A) &= \exp( s_{label}(i, j, A) ) \\
    \tilde{s}_{span}(i, j) &= \exp( s_{span}(i, j) ) \\
    S(i,j) &= \sum_{A \in N} \alpha(A,i,j)
\end{align}
Note that this is the exact same formula as \ref{eq:inside-semiring}.

From equation \ref{eq:final-inside} we can deduce that we in fact do even need to store the values $\alpha(i, j, A)$ but that it suffices to only store the marginalized values $S(i, j)$. In this case, the recursion simplifies even further:
\begin{subequations}
\begin{align}
    S(i, j) &= \sum_{A \in N} \alpha(A,i,j) \\
        &=  \sum_{A \in N} \tilde{s}_{label}(i, j, A) \cdot \tilde{s}_{span}(i, j) \sum_{k=i+1}^{j-1} S(i,k) \cdot S(k,j) \\
        &= \Bigg[ \sum_{A \in N} \tilde{s}_{label}(i, j, A) \cdot \tilde{s}_{span}(i, j) \Bigg] \Bigg[\sum_{k=i+1}^{j-1} S(i,k) \cdot  S(k,j) \Bigg]
\end{align}
\end{subequations}
where we put explicit brackets to emphasize that independence of the subproblems of labeling and splitting. We can now recognize this as the `inside' equivalent of the expression from the paper\footnote{I believe there is actually an error in this equation: it should read  $s_{label}(i, j, \ell) + s_{span}(i, j)$ instead of just $s_{label}(i, j, \ell)$. This is implied by the score for a single node, which is given by equation \ref{eq:rule-score}, taken directly from the paper.}
\begin{align}
    s_{best}(i, j) = \max_{\ell} [s_{label}(i, j, \ell)] + \max_{k}[ s_{split}(i, k, j)].
\end{align}
The recursions are the same; the semirings are different. The viterbi recursion given above is in the \textsc{ViterbiSemiring}, which uses the $\max$ operator as $\oplus$; the inside recursion given in \ref{eq:final-inside} has standard addition (+) instead.


\subsection{Outside recursion}
\begin{align*}
    \beta(A, i, j) &= \sum_{B \to C A \in R} \sum_{k=1}^{i-1} \psi(B \to C A, k, i-1, j) \cdot \alpha(C, k, i-1) \cdot \beta(B, k, j) \\
            &\qquad + \sum_{B \to A C \in R} \sum_{k=j+1}^{n} \psi(B \to A, C, i, j, k) \cdot \alpha(C, j+1, k) \cdot \beta(B, i, k) \\
        &= \sum_{B \in N} \sum_{C \in N} \sum_{k=1}^{i-1} \psi(B, k, j) \cdot \alpha(C, k, i-1) \cdot \beta(B, k, j) \\
            &\qquad + \sum_{B \in N} \sum_{C \in N} \sum_{k=j+1}^{n} \psi(B, i, k) \cdot \alpha(C, j+1, k) \cdot \beta(B, i, k) \\
        &=  \sum_{k=1}^{i-1}  \Bigg[ \sum_{B \in N} \psi(B, k, j)  \cdot \beta(B, k, j) \Bigg] \cdot \Bigg[ \sum_{C \in N} \alpha(C, k, i-1) \Bigg] \\
            &\qquad + \sum_{k=j+1}^{n}  \Bigg[ \sum_{B \in N}  \psi(B, i, k) \cdot \beta(B, i, k) \Bigg] \cdot  \Bigg[ \sum_{C \in N} \alpha(C, j+1, k) \Bigg] \\
        &=  \sum_{k=1}^{i-1}  S'(k, j) \cdot S(k, i-1) + \sum_{k=j+1}^{n} S'(i, k) \cdot  S(j+1, k) \\
\end{align*}
where
\begin{align*}
    S(i, j) &= \sum_{A \in N} \alpha(A, i, j) \\
    S'(i, j) &= \sum_{A \in N} \psi(A, i, j) \beta(A, i, j)
\end{align*}


\section{Related work}
\begin{enumerate}
  \item Conditional Random Fields \citep{Sutton+2012:CRF}
  \item Conditional Random Field parsing with linear \citep{Finkel+2008} and nonlinear features \citep{Klein+2015:neural-crf}
\end{enumerate}



% \section{Semiring formulation}
% So yeah, the highlights are: an edge connects three nodes, a parent and two \textsc{children}, each node is a labelled \textsc{span}; you need to identify the scoring function for an edge, let’s call it $w(e)$,  in this case we have
% \begin{align}
%     w(e) = f(\textsc{head}(e)) \bigotimes_{c \in \textsc{children}(e)} g(\textsc{span}(c))
% \end{align}
% where $f$ and $g$ are parametric functions; then you can compute the Inside recursion for a node v
% \begin{align}
%     I(v) = \bigoplus_{e \in BS(v)} w(e) \otimes \bigotimes_{c \in \textsc{children}(e)} I(c)
% \end{align}
% where I’m using $BS(v)$ to denote the set of edges incoming to $v$; note that BS here basically enumerates the different ways to segment the string under $(i,j)$ into two adjacent parts and the different labels of each child \textsc{span} (let’s call these $a$ and $b$, each an element in the labelset $L$), thus we can write
% \begin{align}
% I(v=[i,j,l]) = \bigoplus_{ \substack{ e=[i,j,l,k,a,b]: \\ a \in L, \\ b \in L, \\ k \in \{i+1,...,j-1\} } } w(e) \otimes I([i,k,a]) \otimes I([k+1,j,b])
% \end{align}
% Now the key is to realise that $w(e)$ factorises and therefore we can rewrite this as
% \begin{align}
%     I(v=[i,j,l]) = f(i,j,l) &\otimes \bigoplus_{k=i+1}^{j-1} g(i,k) \otimes g(k+1,j) \\
%         &\otimes \bigoplus_{a \in L}  I([i,k,a]) \\
%         &\otimes \bigoplus_{b\in L} I([k+1,j,b])
% \end{align}
% and this finally motivates having an inside table for the \textsc{span}s (with labels summed out), let’s call that
% \begin{align}
%     S(i,j) = \bigoplus_{l \in L} I(i,j,l)
% \end{align}
% and then we have the result
% \begin{align}
% \label{eq:inside-semiring}
%     I(i,j,l) = f(i,j,l)\otimes \bigoplus_{k=i+1}^{j-1} g(i,k)\otimes g(k+1,j) \otimes S(i,k) \otimes S(k+1,j).
% \end{align}
