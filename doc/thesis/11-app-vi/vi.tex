\bibliography{../src/bibliography.bib}

In this appendix we give an account of variational inference in general of amortized variational inference in particular. We focus on amortized inference with discrete latent variables, and in particular when the variables are structured. We then derive the score function gradient, as used in chapter \ref{05-semisupervised}, and we describe techniques to reduce the variance of this estimator.


\section{Variational Inference}
We will write some generic things about variational inference.
\begin{itemize}
  \item Variational inference for exponential families, with conjugate priors \citep{Jordan+1999:VI,Jordan+2008:GM,Blei+2016:VI}.
  \item With amortized inference, using neural networks, for non-conjugate models \citep{Kingma+2014:VAE,Rezende+14:DGM} and in particular the reparametrization trick that makes these models efficiently trainable.
  \item Reparametrization for discrete latent variables \citep{maddison2017concrete,jang2017gumbel}.
  \item The generalization of this reparametrization trick in automatic differentiation variational inference \citep{kucukelbir2017automatic}.
  \item Black box variational inference, which uses the same combination of score function gradient with variance reduction that we resort to \citep{Ranganath+2014:BBVI}.
\end{itemize}


\section{Score function estimator}
In this section we provide a detailed derivation of the score function estimator:
\begin{align}
  \label{eq:score-function-estimator}
  \nabla_{\lambda} \expect_{q} \Big[ L(\x, \y) \Big] = \expect_{q} \Big[ L(\x, \y) \nabla_{\lambda} \log \qlambda(\y | \x) \Big]
\end{align}
where
\begin{equation*}
  L(\x, \y) \triangleq \log \ptheta(\x, \y) - \log \qlambda(\y | \x)
\end{equation*}

\begin{align*}
  \nabla_{\lambda} \expect_{q} \Big[ L(\x, \y) \Big] &=
  \nabla_{\lambda} \expect_{q} \Big[ \log \ptheta(\x, \y) - \log \qlambda(\y | \x) \Big] \\
    &= \nabla_{\lambda} \sum_{\y \in \yieldx} \Big\{ \qlambda(\y | \x) \log \ptheta(\x, \y) - \qlambda(\y | \x)\log \qlambda(\y | \x) \Big\} \\
    &= \sum_{\y \in \yieldx} \Big\{ \nabla_{\lambda} \qlambda(\y | \x) \log \ptheta(\x, \y)  - \nabla_{\lambda} \qlambda(\y | \x)\log \qlambda(\y | \x) \\
    &\qquad\qquad -  \qlambda(\y | \x)\nabla_{\lambda}\log \qlambda(\y | \x) \Big\} \\
    &= \sum_{\y \in \yieldx} \Big\{ \nabla_{\lambda} \qlambda(\y | \x) \log \ptheta(\x, \y) - \nabla_{\lambda} \qlambda(\y | \x)\log \qlambda(\y | \x) \Big\} \\
    &= \sum_{\y \in \yieldx} \Big\{ L(\x, \y)\nabla_{\lambda} \qlambda(\y | \x) \Big\} \\
    &= \sum_{\y \in \yieldx} \Big\{ L(\x, \y) \qlambda(\y | \x) \nabla_{\lambda} \log \qlambda(\y | \x) \Big\}   \\
    &= \expect_{q} \Big[ L(\x, \y) \nabla_{\lambda} \log \qlambda(\y | \x) \Big].
\end{align*}
In this derivation we used the identity
\begin{align*}
    \nabla_{\lambda} \qlambda(\y | \x) &= \qlambda(\y | \x)\nabla_{\lambda}\log \qlambda(\y | \x),
\end{align*}
which follows from the derivative
\begin{align*}
    \nabla_{\lambda}\log \qlambda(\y | \x) &= \nabla_{\lambda} \qlambda(\y | \x)\qlambda(\y | \x)^{-1}.
\end{align*}
We
\begin{align*}
    \sum_{\y \in \yieldx} \qlambda(\y | \x)\nabla_{\lambda}\log \qlambda(\y | \x)
        &= \sum_{\y \in \yieldx}  \qlambda(\y | \x) \frac{\nabla_{\lambda} \qlambda(\y | \x)}{\qlambda(\y | \x)}  \\
        &= \sum_{\y \in \yieldx} \nabla_{\lambda} \qlambda(\y | \x) \\
        &= \nabla_{\lambda} \sum_{\y \in \yieldx} \qlambda(\y | \x)\\
        &= \nabla_{\lambda} 1 \\
        &= 0. \\
\end{align*}


\section{Optimization}
We use automatic differentiation \citep{Baydin+2017:AD} to obtain all our gradients. In order to obtain the gradients in formula \ref{eq:score-function-estimator} using this method we rewrite it in the form of a \textit{surrogate objective} \citep{Schulman+2015:surrogate}:
\begin{align}
  \label{eq:surrogate}
  \objective_{ \textsc{surr} }(\theta, \lambda) &= \frac{1}{K}\sum_{i=1}^K \log \qlambda(\x | \y_i) \blockgrad( L(\x, \y_i) ).
\end{align}
The function \blockgrad detaches a node from its upstream computation graph. This turns it effectively into a scalar. More precisely, let $f$ be function (computed by a node in the computation graph) with parameters $\theta$ and input $\x$, then
\begin{align*}
    \blockgrad(f_{\theta}( \x )) &\triangleq f( \x ),
\end{align*}
such that
\begin{align*}
    \nabla_{\theta}\blockgrad(f_{\theta}( \x )) = \nabla_{\theta} f( \x ) = 0.
\end{align*}
Automatic differentiation of equation \ref{eq:surrogate} with respect to $\lambda$ will give us the exact expression we are looking for
\begin{align*}
    \nabla_{\lambda} \objective_{ \textsc{surr} }(\theta, \lambda) &= \frac{1}{K}\sum_{i=1}^K L(\x, \y) \nabla_{\lambda} \log\qlambda( \x | \y_i ),
\end{align*}
hence the adjective \textit{surrogate}.

\section{Variance reduction}
We have derived an estimator for the gradient of the posterior parameters in the unsupervised objective. This estimator is unbiased, but is known to have high variance, often too much to be useful \citep{Paisley+2012:VISS}. Two effective methods to counter this are control variates and baselines \citep{Ross:2006:SIM}.

\paragraph{Variance of estimator} First, let's analyze the variance of our estimator. Note that our expectation is of the general form
\begin{align*}
    \mu \triangleq \expect \Big[ f(X) \Big]
\end{align*}
and that we estimate this quantity by generating $n$ independent samples $X_1,\dots,X_n \sim P(X)$ and computing
\begin{align*}
    \hat{\mu} \triangleq \frac{1}{n} \sum_{i=1}^n f(X_i).
\end{align*}
This is an unbiased estimator for $\mu$ with error
\begin{align*}
    \text{MSE} = \expect \Big[(\mu - \hat{\mu})^2 \Big] &= \var \Big[ \hat{\mu} \Big] = \frac{\var \Big[ \hat{\mu} \Big]}{n},
\end{align*}
which means that the error is of the order
\begin{align*}
    \mu - \hat{\mu} \sim \sqrt{\frac{\var \Big[ \hat{\mu} \Big]}{n}}
\end{align*}
and reducing it linearly requires a quadratic number of samples.

In our particular case, the function $f$ is
\begin{align*}
    f_{X=x}(Y) &\triangleq l(X,Y) \nabla_{\lambda} \log q_{\lambda}(Y|X=x)
\end{align*}
where we have made explicit that $y$ is the random variable, and $x$ is given.

\paragraph{Control variates} Consider a function $g(X)$ with known expectation
\begin{align*}
    \mu_{g} \triangleq \expect \Big[ g(X) \Big]
\end{align*}
Then we can define a new function $\hat{f}$ such that
\begin{align*}
    \hat{f}(X) \triangleq f(X) - g(X) + \mu_{g}.
\end{align*}
This function is also an estimator for $\mu$, since
\begin{align*}
    \expect \Big[ \hat{f}(X) \Big] &= \expect \Big[ f(X) \Big] - \mu_{g} + \mu_{g} \\
        &= \expect \Big[ f(X) \Big],
\end{align*}
and a computation shows that the variance of the new function is
\begin{align*}
    \var \Big[ \hat{f}(X) \Big] &= \expect \Big[ (f(X) - g(X) + \mu_{g}) - \mu)^2 \Big] \\
    &= \expect \Big[ (f(X) - g(X) + \mu_{g})^2 \Big] - 2\expect \Big[ (f(X) - g(X) + \mu_{g})\mu \Big] + \expect \Big[ \mu^2 \Big] \\
    &= \expect \Big[ (f(X) - g(X) + \mu_{g})^2 \Big] - 2\expect \Big[ (f(X) - g(X) + \mu_{g})\Big]\mu + \mu^2 \\
    &= \expect \Big[ (f(X) - g(X) + \mu_{g})^2 \Big] - 2\mu^2  + \mu^2 \\
    &= \expect \Big[ f(X)^2 + g(X)^2 + \mu_{g}^2 - 2f(X)g(X) + 2f(X)\mu_{g} - 2g(X)\mu_{g} \Big] - \mu^2\\
    % &= \expect \Big[ f(X)^2 + g(X)^2 + \expect \Big[ g(x) \Big] ^2 - 2f(X)g(X) + 2f(X)\expect \Big[ g(x) \Big] - 2g(X)\expect \Big[ g(x) \Big] \Big] - \expect \Big[ f(X) \Big]^2\\
    &= \expect \Big[ f(X)^2 \Big] - \expect \Big[ f(X) \Big]^2 \\
    &\quad\qquad - 2(\expect \Big[ f(X)g(X) \Big] - \expect \Big[ f(X) \Big]\expect \Big[ g(X) \Big]) \\
    &\quad\qquad + \expect \Big[ g(X)^2 \Big] - \expect \Big[ g(X) \Big]^2 \\
    &= \var \Big[ f(X) \Big] - 2  \cov \Big[ f(X), g(X) \Big] + \var \Big[ g(X) \Big]
\end{align*}
This means we can get a reduction in variance whenever
\begin{align*}
    \cov \Big[ f(X), g(X) \Big] > \frac{1}{2} \var \Big[ g(X) \Big].
\end{align*}
The function $g$ is called a \textit{control variate}---it allows us to control the variance of $f$.

From the equality above we can see that this will be the case whenever $f(X)$ and $g(X)$ are strongly correlated. Our choice of control variate will be made with the that in mind. Furthermore, $\expect \Big[ g(X) \Big]$ must be known. What is an optimal control variate? Typically a control variate of the form $ag$ is chosen with fixed, and $a$ is optimized to maximize the correlation. This brings us to the generic formulation of a control variate:
\begin{align*}
    \hat{f}(X) \triangleq f(X) - a(g(X) - \expect \Big[ g(X) \Big])
\end{align*}
with variance
\begin{align*}
    \var \Big[ \hat{f}(X) \Big] &= \var \Big[ f(X) \Big] - 2a  \cov \Big[ f(X), g(X) \Big] + a^2 \var \Big[ g(X) \Big] \\
\end{align*}
We take a derivative of this with respect to $a$
\begin{align*}
    \frac{\partial}{\partial a}\var \Big[ \hat{f}(X) \Big] &= - 2  \cov [ f(X), g(X) \Big] + 2a \var \Big[ g(X) \Big]
\end{align*}
Setting this to zero and solving for $a$ we obtain the optimal choice for $a$
\begin{align}
\label{eq:cv-scale}
    a &= \frac{ \cov \Big[ f(X), g(X) \Big]}{ \var \Big[ g(X) \Big]}.
\end{align}

Plugging in this solution into the expression for $\var \Big[ \hat{f}(X) \Big]$ and dividing by $\var \Big[ f(X) \Big]$ we get
\begin{align}
\label{eq:var-red}
    \frac{\var \Big[ \hat{f}(X) \Big]}{\var \Big[ f(X) \Big]} &= 1 - \frac{\cov[ f(X), g(X) \Big]}{ \var \Big[ f(X) \Big]  \var \Big[ g(X) \Big]} \\
        &= 1 - \corr^2 \Big[ f(X), g(X) \Big],
\end{align}
which shows that given this choice of $a$ the reduction in variance is directly determined by the correlation between $f(X)$ and $g(X)$.

Bringing this all together, we let our new estimator be
\begin{align*}
    \expect \Big[ f(X) \Big] &= \expect \Big[ \hat{f}(X) \Big] \approx \frac{1}{n} \sum_{i=1}^n [ f(X_i) - ag(X_i) \Big] - \mu_{g}
\end{align*}

\paragraph{Example} \citep{Ross:2006:SIM} Suppose we want to use simulation to determine
\begin{align*}
    \expect \Big[f(X)\Big] &= \expect \Big[e^X\Big] = \int_0^1 e^x dx = e - 1
\end{align*}
with $X \sim \mathcal{U}(0,1)$. A natural control variate to use in this case is the random variable $X$ itself: $g(X) \triangleq X$. We thus define the new estimator
\begin{align*}
    \hat{f}(X) &= f(X) - g(X) + \expect \Big[ g(X) \Big] \\
        &= e^X - X + \frac{1}{2}.
\end{align*}

To compute the decrease in variance with this new estimator, we first note that
\begin{align*}
    \cov(e^X, X) &= \expect \Big[ Xe^X \Big] - \expect \Big[ X \Big]\expect \Big[ e^X \Big] \\
        &= \int_0^1 xe^x dx - \frac{e-1}{2} \\
        &= 1 - \frac{e-1}{2} \approx 0.14086 \\
    \var \Big[ e^X \Big] &= \expect \Big[ e^{2X} \Big] - (\expect \Big[ e^X \Big])^2 \\
        &= \int_0^1 e^{2x} dx - (1 - e^x)^2 \\
        &= \frac{e^2 - 1}{2}  - (1 - e^x)^2 \approx 0.2420 \\
    \var \Big[ X \Big] &= \expect \Big[ X^2 \Big] - (\expect \Big[ X \Big])^2 \\
        &= \int_0^1 x^2 dx - \frac{1}{4} \\
        &= \frac{1}{3} - \frac{1}{4} = \frac{1}{12}.
\end{align*}
When we choose $a$ as in formula \ref{eq:cv-scale} we can use formula \ref{eq:var-red} to compute that
\begin{align*}
    \frac{ \var \Big[ \hat{f}(X) \Big] }{ \var \Big[ f(X) \Big]} &= 1 - \frac{(0.14086)^2}{\frac{0.2420}{12}} \\
        &\approx 0.0161.
\end{align*}
This is a reduction of 98.4 percent! A simulation illustrates what this looks like in practice with \dots samples:
% \begin{figure}
%   \center
%   \includegraphics[width=0.7\textwidth\Big]{control-variate.pdf}
% \end{figure}
