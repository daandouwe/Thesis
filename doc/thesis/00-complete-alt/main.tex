\documentclass{uvamath}
\usepackage{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{bbm}
\usepackage{bm}
%\usepackage{mathpazo}
\usepackage{natbib}
\usepackage{todonotes}
\usepackage{mathtools}
\usepackage{appendix}
\usepackage{url}
\usepackage{nameref}
\usepackage[colorlinks=true, allcolors = blue]{hyperref}
%Indexing
%\usepackage{makeidx}
%\makeindex


\usepackage{palatino}
%\usepackage{txfonts}
%\usepackage[T1]{fontenc}

% \usepackage{heuristica}
% \usepackage[heuristica,vvarbb,bigdelims]{newtxmath}
% \usepackage[T1]{fontenc}
% \renewcommand*\oldstylenums[1]{\textosf{#1}}

%Number sets
\newcommand*{\reals}{\mathbb{R}}
\newcommand*{\nats}{\mathbb{N}}
\newcommand*{\rats}{\mathbb{Q}}
\newcommand*{\borel}[1]{\mathcal{B}(#1)}
%Caligraphic letters
\newcommand*{\calE}{\mathcal{E}}
\newcommand*{\calF}{\mathcal{F}}
\newcommand*{\calG}{\mathcal{G}}
\newcommand*{\calH}{\mathcal{H}}
\newcommand*{\calL}{\mathcal{L}}
\newcommand*{\calN}{\mathcal{N}}
\newcommand*{\calP}{\mathcal{P}}
\newcommand*{\calQ}{\mathcal{Q}}
\newcommand*{\calX}{\mathcal{X}}
\newcommand*{\calY}{\mathcal{Y}}
\newcommand*{\calZ}{\mathcal{Z}}
%bbm letters
\newcommand*{\bbP}{\mathbb{P}}
\newcommand*{\bbQ}{\mathbb{Q}}
\newcommand*{\bbE}{\mathbb{E}}
%common operators
\newcommand*{\prob}[2][]{\mathbb{P}_{#1}\left\{#2\right\}}
\newcommand*{\expv}[1]{\mathbb{P}\left[#1\right]}
\newcommand*{\indicator}[1]{\mathbbm{1}\left\{#1\right\}}
%Delimiters
\newcommand*{\bracks}[1]{\left\{#1\right\}}
\newcommand*{\sqbrack}[1]{\left[#1\right]}
\newcommand*{\paren}[1]{\left(#1\right)}
\newcommand*{\abs}[1]{\left|#1\right|}
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\evat}[2]{\left.#1\right|_{#2}}
\newcommand*{\mart}[2]{\expv{#1 | \calF_{#2}}}
%Modes of convergence
\newcommand*{\toinP}{\overset{\bbP}{\longrightarrow}}
\newcommand*{\toas}{\overset{\text{a.s.}}{\longrightarrow}}
\newcommand*{\toinD}{\overset{\text{wk.}}{\longrightarrow}}
\newcommand*{\toinL}{\overset{\mathcal{L}^1}{\longrightarrow}}
%roman letters
\newcommand*{\rme}{\mathrm{e}}
\newcommand*{\rmd}{\mathrm{d}}
\newcommand*{\KL}{\mathrm{KL}}
\newcommand*{\IC}{\mathrm{IC}}
%Operators
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\var}{Var}
%Theorems

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\theoremstyle{definition}
\newtheorem{counterexample}[theorem]{Counterexample}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{definition}
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{condition}{Condition}



% Order Relations
\newcommand*{\esi}[1]{\leq_{#1}}



\title{Fast Rate Conditions in Statistical Learning}

\author[muriel.perezortiz@student.uva.nl, 11391758]{Muriel Felipe Pérez
  Ortiz}
\documentTitle{Master Thesis}

\supervisorsTitle{Prof. dr. P.D. Grünwald
  \newline  Prof. dr. J.H. van Zanten} %This is the list of supervisors for the title page, seperate with \newline

\supervisors{Prof. dr. P.D. Grünwald,  Prof. dr. J.H. van Zanten}

\secondexaminer{Prof. dr. B.J.K.  Kleijn}

\date{September 20, 2018}

\secondinstitute{Centrum voor Wiskunde en Informatica}
\secondlogo{\includegraphics[height=1cm]{cwi-logo2}}
\secondadress{Centrum voor Wiskunde en Informatica \\
Science Park 123, 1098 XG Amsterdam \\
\url{http://www.cwi.nl}} %Adress of second institute, for the second page



% #######
% # The Document
% ######

\begin{document}
\maketitle

\begin{abstract}
  We study conditions under which the order of convergence of
  algorithms in statistical learning can be improved from
  $O_\bbP(n^{-1/2})$ to $O_{\bbP}(n^{-1})$ (up to logarithmic factors)
  in the number of data points. If excess losses are bounded, it is
  known that two conditions, called Bernstein's and strong central
  condition, are equivalent and lead to fast rates both for Empirical
  Risk Minimization and for randomized algorithms.  If the excess
  losses are unbounded, they are no longer equivalent and are known to
  lead to faster rates either under additional assumptions or for
  specific randomized algorithms. We investigate their relation in the
  unbounded case and show weak, realistic assumptions under which they
  become equivalent. Furthermore, in this regime we show tighter
  bounds than those presented in the literature.
\end{abstract}


{\hypersetup{linkcolor=black}
% or \hypersetup{linkcolor=black}, if the colorlinks=true option of hyperref is used
\tableofcontents
}

\chapter{Introduction \label{sect:introduction}}
%\addcontentsline{toc}{chapter}{Introduction}

In machine learning, statistics and pattern recogition the goal is
often to make optimal decisions based on data. This work is about
conditions under which the quality of the conclusions of such
procedures increases \textit{fast} as the amount of data increases.

In order to illustrate the kind of problem that concerns us, let us
first consider the standard pattern recognition problem of
\textit{supervised classification} \citep{gyorfi_probabilistic_1996}.
In this situation, we want to be able to classify objects in one of
two classes, say positive $(+)$ or negative $(-)$. Suppose that we can
observe $n$ objects and that for each of them we record a set of $m$
features which we believe are informative of the class to which they
belong.  Thus, suppose that we number the objects from $1$ to $n$ and
that for the $i$-th one we encode its features in an ordered vector
$X_i = (x_i^1,\dots,x_i^m)$, which can consist of categorical, numeric
or other type of entries. The vector $X_i$ is an element of the set
$\calX$ that contains all the possible values that the features might
take. Suppose that we can also observe the class to which each one
belongs, so that for the $i$-th one we know that it belongs to class
$Y_i$, which is either equal to $+$ or $-$.  Thus, the task of finding
a classifier based on our observations can be seen as that of finding
a correspondence $g_n$ that outputs a class ($+$ or $-$) given the
features of an object, that is, $g_n:\calX\to\calY :=\{+,-\}$. An
example of this problem is that of predicting if the land use of some
area will change or not in a given period of time. In this particular
case $+$ would correspond to land use change occurring, while $-$
would correspond to the opposite. Thus, data
$(X_1,Y_1),\dots, (X_n,Y_n)$ would consist of records of variables
that are believed to be predictive of land use change and whether it
occurred or not in $n$ areas. A classifier would take the values of
the predictive variables for an area where it is unknown if land use
will change or not and output our best guess based on data.

Naturally there are many ways in which such a classifier might be
found, based on the observations $(X_1,Y_1),\dots,(X_n,Y_n)$. We
consider a special model for analyzing these situations, that of
statistical learning, which has proven to be very useful
\citep{vapnik_overview_1999}. This model includes three ingredients.
First, a probabilistic model for how data is generated. Second, a
criterion or algorithm for choosing a classifier based on data. Third,
a restricted set of classifiers from which to choose. We assume that
each data point $(X_i,Y_i)$ is generated independently of the others
and follows a probability distribution $\bbP$ (which is typically
unknown) on all possible pairs of features and classes
$\calX\times\calY$. This means that $\prob{x,y}$ is the probability of
observing an object with the features $x$ (out of all possible
features $\calX$) that belongs to the class $y$ (an element of
$\calY = \{+,-\}$). With this probabilistic model in mind, we can
already set as a goal to minimize the probability of error, that is,
we set ourselves to find a classifier $g^*$ that minimizes
$\prob{g(X)\neq Y}$. Often the relationship between the features that
we observe and the class to which objects belong is not one to one,
that is, it might occur that two objects that have the same features
belong to different classes. This implies that in general it can occur
that the probability of error is never zero, that is
$\prob{g(X)\neq Y}>0$ for each classifier $g$. Additionally, the
distribution $\bbP$ is often unknown and one must do the best one can
to find a classifier using the data available. Thus, perhaps the most
intuitive idea is that of choosing the classifier that does best
according to the data available, that is, a classifier $g^*_n$ that
minimizes the estimated probability of error $\prob[n]{g(X)\neq Y}$
given by
\begin{equation*}
  \prob[n]{g(X)\neq Y} = \frac{\text{Number of data points where }
    g(X_i)\neq Y_i}{n}.
\end{equation*}
Unfortunately, this idea, called \textit{Empirical Risk Minimization}
(ERM), might fail as one can always choose a classifier $\tilde{g}$
that does not make any mistakes on the data. For instance, we can
choose it to satisfy $\tilde{g}(X_i)=Y_i$ for $i=1,\dots,n$ and
$\tilde{g}(x) = +$ in any other case.  This classifier achieves zero
error in the data, but intuitively, it is not what we want. Thus, the
set of classifiers that we chose $g$ from should not contain all
possible classifiers and we adopt the point of view that one chooses
from a set of classifiers $\calG$ that has been fixed in advance and
is not too big (in a sense discussed below).

Given this setup, we are ready to illustrate the first paragraph of
this introduction and the scope of this work. By \textit{decision} we
mean the act of choosing one of the classifiers from $\calG$ to make
predictions. The speed at which our best guess $g_n^*$ (that minimizes
the error in the sample) becomes optimal is the rate at which its
probability of error converges to the lowest possible in $\calG$ as
more data is gathered, that is, the rate at which
\begin{equation*}
  \prob{g^*_n(X)\neq Y} \toinP \inf_{g\in\calG}\prob{g(X)\neq Y}
\end{equation*}
as $n\to\infty$ (see Section \ref{sect:erm_consistency} for a precise
definition of $\toinP$). We will call rates of order $n^{-1/2}$ in the
number of data points \textit{slow}, because they can be attained
under very weak conditions on $\bbP$ and $\calG$. On the other hand,
we refer to rates of order $n^{-1}$ as \textit{fast}, because even
faster rates are usually not achievable. Rates in-between $n^{-1/2}$
and $n^{-1}$ are achievable under \textit{reasonable} conditions that
hold in many but by no means all situations. A \textit{fast rate}
roughly means that in order to to increase the relative performance by
a factor of 100, one needs to gather $\sim 10000$ times more data
points when rates are slow, while one needs $\sim 100$ times more data
points when the rates are fast. This means that finding conditions
under which rates are fast is important for practical purposes. This
work is about such conditions.

Much has been said about classification in particular
\citep[see][]{gyorfi_probabilistic_1996, boucheron_theory_2005}, but
in this work we will focus on the more general abstract learning
problem. Consider a set of hypotheses $\calH$ and suppose that we
observed data $Z_1,\dots,Z_n$, which is assumed to be identically and
independently distributed according to a distribution $\bbP$ on a
space $\calZ$. For instance, in the case of classification $\calH$ can
be identified with a set of classifiers $\calG$ and $\calZ$ with the
set of all possible pairs of features and classes
$\calX\times\calY$. Thus, given a loss function
$\ell:\calH\times\calZ\to\reals$ that quantifies how bad choosing $h$
is when we observe $Z$, we judge a decision $h$ as good if its
expected loss over the distribution of the data is small. We denote
the expected loss of $h\in\calH$ by $L(h)$ and write the expected
value as
\begin{equation*}
  L(h) = \bbP[\ell(h,Z)] = \int \ell(h,Z) \rmd \bbP.
\end{equation*}
In the classification case, one has that
$\ell(g,x,y) = \indicator{g(x)\neq y}$, so that for the classifier $g$
the expected loss $L(g)$ is nothing more than the probability of error
$\prob{g(X)\neq X}$. Thus, minimizing the expected loss is equivalent
to minimizing the probability of error in the classification
case. Since it is typically not known what the optimal expected loss
might be, we are happy to consider the performance relative to the
best possible achievable performance on the class $\calH$, which is
measured by the expected excess loss $E(h)$ given by
\begin{equation*}
  E(h) = L(h) - \inf_{h\in\calH}L(h).
\end{equation*}
The study of the expected excess loss, also called excess risk, has
been central to the theory of statistical learning
\citep{vapnik_statistical_1998}. We will focus on the case in which
there is a $h^*\in\calH$ for which the infimum on the right is
achieved. Consequently, we can define the excess loss
$\varepsilon(h,Z)$ as
$$ \varepsilon(h,Z) = \ell(h,Z) - \ell(h^*,Z)$$ and note that $E(h) =
\bbP[\varepsilon(h,Z)]$.
This work deals with conditions on the excess losses and their
distributions in the case that they might be unbounded but are still
constrained to have exponentially small left tails. This means that we
concern ourselves with situations in which the probability that the
hypotheses in the class perform better than the optimal is
exponentially small. Obviously, this makes the task of empirically
identifying elements $h\in\calH$ with small expected excess loss
easier.

As we noted before, there are many valid ways in which one can choose
a hypothesis $h\in\calH$ based on the available data
$Z_1,\dots,Z_n$. Here we will consider two. The first method is ERM,
which consists of choosing a hypothesis $h^*_n$ that minimizes the
\textit{empirical risk}, that is
\begin{equation*}
  h^*_n \in \argmin_{h\in\calH}\frac{1}{n}\sum_{i=1}^n\ell(h,Z_i)
\end{equation*}
from a restricted class of hypotheses $\calH$. This is the hypothesis
that looks best according to our data. Second, we consider the case of
\textit{randomized prediction}. In this case, predictions are made
according to a random hypothesis $\hat{h}_n$ sampled from a data
dependent distribution $\Pi_n$ on $\calH$. Usually one establishes a
\textit{prior} distribution $\Pi_0$ and bases the distribution $\Pi_n$
both on the data and $\Pi_0$ in analogy to Bayesian procedures. More
generally, the distribution $\Pi_n$ is built strategically in such a
way that hypotheses with lower empirical risk are sampled with higher
probability. This approach also includes ERM and other deterministic
predictors as a special case, as they can be viewed as sampling one
hypothesis with probability one for prediction.

As we will recall in Chapter \ref{sect:basic_theory}, deviation bounds
for both ERM and randomized prediction have been proven. Under mild
conditions it has been shown that
\begin{equation}
E(h^*_n) = O_{\bbP}(n^{-1/2}\mathrm{Comp}^{1/2}(n))
\end{equation}
\citep[see][]{vapnik_statistical_1998} where $\mathrm{Comp}(n)$ is a
specific measure of the complexity of the hypotheses class $\calH$ for
which $\mathrm{Comp}(n) \leq \log|\calH|$ if $\calH$ has finite size
(see also Appendix \ref{app:basic_rates} for the exact meaning of the
$O_\bbP$ notation). For the randomized case, through the so-called
PAC-Bayesian inequalities, it has been established that
\begin{equation*}
  \Pi_n[E(h)] \leq \Pi_n[E_n(h)] + O_{\bbP}(n^{-1/2}\mathrm{Comp}(n)),
\end{equation*}
where $\Pi_n$ is any suitable distribution on $\calH$ and and $E_n$ is
the empirical excess loss
\begin{equation*}
  E_n(h) = \frac{1}{n}\sum_{i=1}^n\ell(h,Z_i)
\end{equation*}
\citep[see][]{mcallester_pac-bayesian_1998}. In this work we deal with
conditions under which fast rates can be obtained, that is, situations
in which $O_{\bbP}(n^{-1}\mathrm{Comp}(n))$ can replace its
counterparts from the previous equations. We next introduce the
conditions with which we will deal.

Consider first the case in which excess losses are bounded, that is,
when $$\sup_{h,z}\varepsilon(h, z)<\infty.$$ This case has been
extensively studied. It it has been shown that if $\calH$ has finite
size
\begin{equation*}
  \sup_{h\in\calH}\frac{\bbP[\varepsilon^2(h,Z)]}{\bbP[\varepsilon(h,Z)]}
  \leq B,
\end{equation*}
where $\bbP[\varepsilon^2(h,Z)]$ is the second moment of the excess
loss, faster rates can be achieved. This condition is the strongest
version of what has been called \textit{Bernstein's condition} (see
Condition \ref{cond:bernstein} on page \pageref{cond:bernstein} ) in
association with the use of Bernstein's inequality to obtain faster
concentration (see Section \ref{sect:bernstein}).
\citet{bartlett_empirical_2006} proved that also in the case that the
class $\calH$ is infinite and the excess losses are uniformly bounded,
Bernstein's condition implies faster
concentration. \citet{van_erven_fast_2015} found that it is equivalent
to the cumulant generating function of $-\varepsilon(h,Z)$ being non
positive for all $h$ at some point, that is,
\begin{equation*}
  \sup_{h\in\calH}\log\bbP[\rme^{-\eta\varepsilon(h,Z)}]\leq 0
\end{equation*}
for some $\eta>0$, where $\bbP$ is the data generating
distribution. This condition is the strong version of what the authors
called \textit{Central Condition} (see Condition \ref{cond:central} on
page \pageref{cond:central}). They noted that it had been used in the
past, often implicitly, in order to obtain fast rates. For instance,
it can be related to having a well specified model in the case of
density estimation (see Example \ref{ex:mle_central}). Consequently,
when viewed as a statistical learning problem, the central condition
also allows for fast rates (see Example \ref{ex:density_estimation})
for density estimation. Furthermore, \citet{grunwald_fast_2016} showed
that from the strong central condition condition fast rates could also
be established using a PAC-Bayesian-style analysis, where the study of
cumulant generating functions is essential.

Nevertheless, the landscape changes when the excess losses might be
unbounded, which is the case in many practical situations. In the
unbounded case, \citet{audibert_fast_2009} showed a nonstandard
randomized learning algorithm that achieved fast rates if Bernstein's
condition is satisfied. To the best of our knowledge, it is still not
known if satisfying Bernstein's condition also leads to fast rates for
ERM in the unbounded case. One might hope that this question can be
answered using the relation of Bernstein's condition to the strong
central condition. This is not the case because on the one hand
\citet{grunwald_fast_2016} showed that the strong central condition
was not enough to obtain fast rates and on the other, as we will show
in Chapter \ref{sect:relations}, the strong central condition and
Bernstein's condition are not equivalent in the unbounded case even
under additional conditions. Despite this, \citet{grunwald_fast_2016}
showed that under an additional condition on the tails of the excess
losses and the strong central condition, fast rates could be obtained
for randomized algorithms and for ERM. The condition, called the
\textit{witness condition}, is the existence of some $u>0$ such that
\begin{equation*}
  0<\sup_{h\in\calH}
  \frac{\bbP[\varepsilon(h,Z)\indicator{\varepsilon(h,Z) > u}]}{\bbP[\varepsilon(h,Z)]} \leq 1,
\end{equation*}
that is, it is a mild condition on the relative weight of the right
tail of the excess losses. It is easy to see that it is implied by
Bernstein's condition (see Section
\ref{sect:bernstein}). \citet{grunwald_fast_2016} related this
condition to conditions for the convergence of likelihood ratios that
had been previously studied by \citet{wong_probability_1995}, who use
lower truncations of log likelihood ratios in their analysis.  Recall
that $\varepsilon(h,Z) > u$ implies for the losses that
$\ell(h,Z) > u + \ell(h^*,Z)$, thus we are dealing with restrictions
on how much worse $h$ can be compared to the optimal $h^*$ on average.

Apart from the conditions that we previously mentioned, little is
known about conditions on the excess losses that lead to fast
rates. In this work we make a first step in understanding the relation
between these conditions, and the uses and limitations of the tools at
hand. The main contributions of this work are contained in Chapter
\ref{sect:relations} and are the following:
\begin{enumerate}
\item As we mentioned before, we establish by way of counterexamples
  (Counterexamples \ref{ctrex:bernstein_but_not_central} and
  \ref{ctrex:central_but_not_bernstein}) that even under stringent
  tail restrictions, it is not true that Bernstein's condition and the
  combination of the central condition and the witness condition are
  equivalent. Since the combination of the strong central condition
  and the witness condition lead to fast rates (see Section
  \ref{sect:fast_risk_bound}), this means that the question of whether
  Bernstein's condition leads to fast rates for ERM in the unbounded
  case remains open as Bernstein's condition does not imply them.
\item We establish that if there exists $r>1$ such hat
  \begin{equation*}
    \sup_{h\in\calH}\frac{(\bbP[|\varepsilon(h)|^{2r}])^{1/r}}{\bbP[\varepsilon^2(h)]}
    < \infty,
  \end{equation*}
  (see Condition \ref{cond:new} on page \pageref{cond:new})
  Bernstein's condition and the strong central condition are
  equivalent as long as the cumulant generating function of
  $-\varepsilon(h)$ satisfies
  \begin{equation}\label{eq:finite_cumulant}
    \sup_{h\in\calH}\log\bbP[\rme^{-\eta\varepsilon(h)}]<\infty
  \end{equation}
  for some $\eta>0$ (Theorem \ref{thm:bernstein_central_equivalence}).
  The requirement in \eqref{eq:finite_cumulant} means that the left
  tail probabilities of the excess loss can be uniformly bounded by an
  exponential function, a natural relaxation of the bounded excess
  loss condition. The condition on the moments of random variables had
  been considered before, for instance by \citet[Lemma
  6.1]{mendelson_learning_2014}. It is satisfied in the case that for
  each $h$ the excess loss $\varepsilon(h,Z)$ has a Gaussian,
  Laplacian or uniform distribution and many other distributions with
  well-behaved tails also satisfy it (see Example
  \ref{ex:new_cond_example}). This means that this condition can be
  used in place of the witness condition (because Bernstein's
  condition implies it) in addition to the central condition to
  achieve fast rates. In this case, we proved a tighter excess risk
  bound (Corollary \ref{cor:tighter_excess_bound}) as our next
  contribution.
\item We notice that in the case that the previous condition and the
  central condition hold, a condition (see Condition
  \ref{cond:vapnik} on page \pageref{cond:vapnik}) also considered by
  \citet{vapnik_statistical_1998} holds. This condition, which is also
  on the relative size of a moment of $\varepsilon(h,Z)$, is the
  existence of some $r>1$ such that
  \begin{equation*}
    \sup_{h\in\calH}\frac{(\bbP[|\varepsilon(h,Z)|^{2r}])^{1/2r}}{\bbP[\varepsilon(h,Z)]} <
    \infty.
  \end{equation*}
  \citet{vapnik_statistical_1998} argued that this condition
  characterizes light tails and proved that it implied slow rates for
  ERM in the case that it was imposed on the loss function
  $\ell(h,Z)$. We prove (Theorem \ref{thm:tighter_bound}) that this
  condition and \eqref{eq:finite_cumulant} are enough to obtain
  sharper bounds than those obtained by \citet{grunwald_fast_2016}. We
  also prove that if \eqref{eq:finite_cumulant} holds,
  \citeauthor{vapnik_statistical_1998}'s condition implies the strong
  central (Lemma \ref{lem:vapnik_then_central}) and Bernstein's
  condition (Lemma \ref{lem:vapnik_then_bernstein}), and thus also the
  witness condition. Since this condition holds in many practical
  cases, the resulting bounds are relevant.
\item We prove that if \eqref{eq:finite_cumulant} holds for some
  $\eta>0$, then either the witness condition alone or an alternative
  second moment condition implies that slow rates can be achieved
  (Theorem \ref{theo:cfg_bernstein_bound}).
\end{enumerate}

With this in mind, we now describe how this thesis is organized.
\begin{description}
\item[Chapter \ref{sect:basic_theory}.] In Section
  \ref{sect:statistical_learning} we describe formally the basic
  theory of statistical learning and the set-up for the remainder of
  this thesis. In Section \ref{sect:erm_intro} we introduce empirical
  risk minimization. Since before worrying about rates of convergence
  one needs to establish consistency first, for the sake of
  completeness we recall in Section \ref{sect:erm_consistency} the
  classic theorem on consistency of ERM due to
  \citet{vapnik_necessary_1991}. In Section \ref{sect:excess_losses}
  we make some considerations about excess losses, the main object of
  study. In Section \ref{sect:concentration_inequalities} we show how
  concentration inequalities can be used to obtain rates of
  convergence for ERM for finite classes. In Section
  \ref{sect:pac_intro} we introduce the PAC-Bayesian setting, and
  recall and prove the PAC-Bayesian inequalities that are the backbone
  of PAC-Bayesian bounds in Section \ref{sect:rates_pac_bayesian},
  which are also valid in the case of infinite classes.

\item[Chapter \ref{sect:pac_fast_rates}.] In Section
  \ref{sect:second_pac_bayes} we describe the inequality obtained by
  \citet{zhang_information-theoretic_2006} on which the bounds
  obtained by \citet{grunwald_fast_2016} are based. We do this because
  we later use \citeauthor{zhang_information-theoretic_2006}'s
  inequality to obtain tighter bounds using Vapnik's condition in
  Chapter \ref{sect:relations}. We then describe the bounds on the
  expected excess loss obtained by \citet{grunwald_fast_2016}
  in Section \ref{sect:fast_risk_bound} after having explained how
  Bernstein's, Central and the Witness condition come into play in
  Section \ref{sect:conditions_fast}.

\item[Chapter \ref{sect:relations}] It is in this chapter where the
  main contributions of this work can be found. In Section
  \ref{sect:central_witness_bernstein} we establish counterexamples
  that relate the strong central, the witness and Bernstein's
  condition. In Section \ref{sect:new_vapnik} we establish our new
  condition, recall \citeauthor{vapnik_statistical_1998}'s condition
  and prove tighter excess risk bounds than those of
  \citet{grunwald_fast_2016}. In Section \ref{section:tail_bounds} we
  prove that under the witness condition or an alternative mild
  condition, slow rates can be obtained as long as
  \eqref{eq:finite_cumulant} holds.

\item[Conclusion and Appendices] We finish with concluding remarks in
  Chapter \ref{sect:conclusion}, where we also present some open
  questions that arise as a result of this thesis. Furthermore, we
  present four appendices. In Appendix \ref{app:basic_rates} we
  describe what rates of convergence in probability mean. In Appendix
  \ref{app:cramer_chernoff} we explain the Cramér-Chernoff method for
  proving deviation inequalities and we show why it yields $n^{-1/2}$
  rates. In Appendix \ref{sect:bernstein_inequality} we discuss
  subgamma tails for random variables and present \textit{Bernstein's
    moment condition}.  In Appendix \ref{sect:chaining} we point at
  results on how the results derived in Section
  \ref{sect:excess_losses} can be extended to infinite classes using a
  celebrated and now classical technique called
  \textit{chaining}. Although there are other methods for obtaining
  rates of convergence in the case of infinite classes, chaining is
  known to yield optimal rates.
\end{description}


% This is the plan
% \begin{enumerate}
% \item Talk a bout general theory and conditions for convergence. The
%   greatest classical hits (mostly done by now)
% \item Talk about rates of convergence and when they are fast.
%   \begin{enumerate}
%   \item ERM theory
%     \begin{enumerate}
%     \item Give results for finite classes
%     \item Introduce Bernstein's condition (Mendelson)
%     \item Say that for unbounded case only Audibert did it, and it is
%       still not known if for ERM Bernstein gives fast rates
%     \item Point at chaining, introduce Orlicz norms (van der Vaart and
%       Pollard). Mention how Orlicz norms characterize tail
%       growth. Make emphasis on Van de Geer's Bernstein-Orlicz (will
%       use later) and Bennet-Orlicz.
%     \end{enumerate}
%   \item PAC-Bayesian Stuff
%     \begin{enumerate}
%     \item Give my slightly generalized PAC-Bayesian theorem (for rv's
%       that satisfy Bernstein's inequality) (use van de Geer). Point at
%       $n^{-1/2}$ rates.
%     \item Introduce Grunwaldo's work (Central and Witness) and point
%       at fast rates.
%     \end{enumerate}
%   \end{enumerate}

% \item Talk about what I did.
%   \begin{enumerate}
%   \item Show that Grunwaldo's random variables are sub-gamma in the
%     worst case and thus, my PAC-Bayesian bound applies (worst case).
%   \item Show the counter examples
%   \item Write down de la Vallé Poussin (Orlicz norms were already
%     introduced) and equivalence between central and Bernstein
%   \item Talk about how this relates to suff others have done. (Vapnik
%   1998, p.210)
%  \end{enumerate}
% \item Close nicely.

% \item Appendixes
%   \begin{enumerate}
%   \item Bernstein's inequality
%   \end{enumerate}
% \end{enumerate}


\chapter{Basic Theory\label{sect:basic_theory}}

There is a vast family of problems in statistics, pattern recognition
and machine learning in which one wants to make an optimal decision
based on data. We will take the point of view that data are
independent realizations of a random process. In this chapter we
describe a useful high-level model for such situations, that of
statistical learning theory \citep{vapnik_statistical_1998}, which is
framed in the language of probability theory. We introduce the
notation that we will use throughout the rest of the document. In
Section \ref{sect:statistical_learning} we describe the set up of an
abstract learning problem and give some examples of how this model
applies in concrete statistical applications. We will treat two
situations: Empirical Risk Minimization (ERM from now on), and
randomized prediction. In Section \ref{sect:erm_intro} we explain the
method of ERM and we describe what its consistency means. Since before
wondering about rates of convergence one needs to worry about whether
consistency happens at all, in Section \ref{sect:erm_consistency} we
recall necessary and sufficient conditions for it to happen, which are
classical. In Section \ref{sect:excess_losses} we introduce excess
losses, the main object of analysis. We focus on them because their
study is useful to infer nonasymptotic bounds on the performance of
learning algorithms. In Section \ref{sect:concentration_inequalities}
we introduce excess losses with subgamma and subgaussian tails and
explain how rates of convergence can be obtained for ERM. We focus on
the case in which the hypotheses class is finite and point at how to
generalize this analysis to infinite classes in Appendix
\ref{sect:chaining}. In Section \ref{sect:pac_intro} we introduce the
PAC-Bayesian model for situations in which our prediction might not
depend deterministically on the observed data and we prove the main
PAC-Bayesian inequality, which is central in this type of analysis.

\section{Statistical Learning
  Theory \label{sect:statistical_learning}}

The statistical learning model consists of three parts. First, a
probabilistic model for the generation of the data that we observe.
Second, a set of hypothes (or hypotheses class) from which we want to
choose. Third, a notion of optimality for choosing a hypothesis. We
assume that data take values in a set $\calZ$ and we endow it with a
probability space structure. Thus, let $(\calZ, \calF,\bbP)$ be a
probability space. We model data $Z_1, \dots, Z_n$ as independent and
identically distributed random variables taking values on $\calZ$ with
distribution $\bbP$. For any random variable $Z$ we write as $\bbP Z$
or $\bbP[Z]$ its expectation with respect to $\bbP$. We follow the
same notation when taking expectations with respect to other
distributions.

We adopt the point of view that set of hypotheses $\calH$ from which
we want to choose has been fixed in advance, that is, it does not
depend on the number of data points $n$. Since we will also consider
situations in which our predictions do not depend deterministically on
the data that is observed, we endow $\calH$ with a sigma-algebra
$\calG$. In this situation, we also need to endow $\calZ\times \calH$
with a measurable structure, for which we choose the product
sigma-algebra. We consider $\reals$ as a measurable space with the
Borel sigma-algebra generated by its usual topology.

We encode our notion of optimality in a measurable function
$\ell: \calH \times \calZ\to \reals$. We interpret
$\ell:(h,Z)\mapsto\ell(h, Z)$ as the loss associated to using $h$ to
make predictions when observing the data point $Z$ and refer to it as
the loss function. When it is clear from context, we will drop the
$Z$-dependence of $\ell$ and just write $\ell(h)$ instead of
$\ell(h,Z)$, and $\ell_i(h)$ instead of $\ell(h,Z_i)$. This notation
also stresses the fact that we might also view $\ell$ as a random
function (or a stochastic process) taking values in some subset of
$\reals^\calH$ and $\ell_{1},\dots,\ell_n$ as a random sample of such
functions. For a fixed $h\in\calH$ let the expected loss be
\begin{equation*}
  L(h) = \bbP[\ell(h)],
\end{equation*}
also called the risk of $h$.

With this in mind, we will concern ourselves with an \textit{abstract
  learning problem}, which is that of finding elements $h\in\calH$ as
close to the infimum over $\calH$ of the expected excess loss as
possible based on data. This notion is encoded in the expected excess
loss, also called excess risk functional. We denote it by $E(h)$ and
define it as
\begin{equation*}
  E(h) = L(h) - \inf_{h\in \calH}L(h).
\end{equation*}


\section{Empirical Risk Minimization (ERM) \label{sect:erm_intro}}

Empirical Risk Minimization (ERM) is the idea of choosing the
hypotheses that looks best on the available data. First, we define the
empirical risk, which is nothing more than the average of the loss on
the sample. As we pointed out already in the introduction, ERM is not
always consistent and problems may arise if, roughly speaking, the
hypothesis class at hand is too big. We will cite a condition that is
necessary and sufficient for the consistency of ERM in the next
section.

For a random sample $Z_1,\dots,Z_n$ consider the empirical measure
$\bbP_n$ given by
\begin{equation*}
  \bbP_n = \frac{1}{n}\sum_{i=1}^{n}\delta_{Z_i} ,
\end{equation*}
where $\delta_z$ is the probability measure that concentrates all of
its mass on the point $z$.  Consequently, $\bbP_n$ is a (random)
measure that puts mass $1/n$ at each of the (random) points
$Z_1, \dots, Z_n$. Call empirical risk, or empirical loss $L_n(h)$ the
empirical mean of the loss function, that is,
\begin{equation*}
  L_n(h) = \bbP_n [\ell(h)] = \frac{1}{n}\sum_{i=1}^n\ell_i(h).
\end{equation*}

Empirical Risk Minimization (ERM) is the idea of picking from $\calH$
an element $h^*_n$ that minimizes the empirical risk, that is,
\begin{equation*}
  h^*_n \in \argmin_{h\in \calH} L_n(h).
\end{equation*}
Thus, the ERM is the best fitting hypothesis according to the data
that is available. Nevertheless, the size of $\calH$ is a
compromise. For instance in the case of classification, if the
hypotheses class $\calH$ includes all possible functions
$\calX\to\calY$, then the empirical loss will always be zero, while
the expected loss can remain positive. Thus, two questions arise. The
first question is about consistency, that is, finding necessary and
sufficient conditions for the convergence of the excess risk to
zero. It turns out that the expected excess risk can be bounded in a
distribution-free manner, and necessary and sufficient conditions for
its convergence are known. While this is remarkable, in practice we
are also interested in non asymptotic results about the speed at which
this convergence occurs, as it might be arbitrarily slow
\citep[see][Chapter 7]{gyorfi_probabilistic_1996}.

In the introduction we already highlighted how the problem of
classification can be cast in these terms. We now give two more
examples, that of regression and that of density estimation. This
means that our analysis also has implications for these problems.

\begin{example}[Least Squares Regression with Random Design]
  Let $\calZ = \calX\times\calY$. Identify $\calH$ with a set $\calF$
  of functions from $\calX$ to $\calY$ and let
  $\{(X_1, Y_1),\dots, (X_n, Y_n)\}$ be an iid sample from
  $\bbP$. Thus, we say that the design is random because the points
  $X_1,\dots,X_n$ not fixed by us. In this case, $\calY = \reals$ and
  for $f\in\calF$, the loss function is given by
  $$\ell(f(X), Y) = \frac{1}{2}(f(X)-Y)^2.$$  One is then interested in
  finding a function $f$ that minimizes
  \begin{equation*}
    L(f) = \bbP[\ell(f(X),Y)]=
    \frac{1}{2}\bbP[f(X) - Y]^2,
  \end{equation*}
  often called the mean quadratic error. It is known that under
  certain regularity conditions the estimator $f^*_n$ that minimizes
  the error in the sample, that is,
  \begin{equation*}
    f^*_n = \argmin_{f\in\calF}\frac{1}{n}\sum_{i=1}^{n}\ell(f(X_i), Y_i)
  \end{equation*}
  is a consistent estimator of the best possible $f$ in the class of
  functions $\calF$.
\end{example}

\begin{example}[Density Estimation]\label{ex:density_estimation}
  Say that given a random sample $Z_1,\dots,Z_n$ from a probability
  distribution $\bbP$, one is interested in estimating its density
  $\bbP$ by choosing one from a set of distributions $\calQ$, called
  \textit{statistical model}. Suppose that there is a dominating
  measure $\nu$ such that each element $\bbQ\in\calQ$ has a density
  $q$ with respect to $\nu$.  Identify $\calH$ with the set $\calQ$ of
  probability distributions over $\calZ$. If we choose for
  $\bbQ\in\calQ$ the loss function $\ell(\bbQ, z) = -\log q(z)$, then
  the expected loss
  \begin{equation*}
    L(\bbQ) = -\expv{ \log q(Z)}
  \end{equation*}
  is the negative likelihood. Thus, minimizing $L$ in this case amounts to
  maximizing the likelihood.

  It is known that under certain regularity conditions, the maximum
  likelihood estimator $\bbQ^*_n$ with density $q^*_n$
  \begin{equation*}
    q^*_n = \argmax_{\bbQ\in\calQ}\frac{1}{n}\sum_{i=1}^n\log q(Z_i)
  \end{equation*}
  is unique and is a consistent estimator of $\bbP$ in the case that
  $\bbP \in \calQ$, that is, in the case that the model is well
  specified.
\end{example}

In the next section we a classical result on the necessary and
sufficient conditions for the consistency of ERM which is due to
\citet{vapnik_necessary_1991} and was later compiled by
\citet{vapnik_statistical_1998}. The result gives necessary and
sufficient conditions for the consistency of ERM and provides a link
to the study of uniform deviations of empirical averages from their
meas.

\section{Consistency of Empirical Risk
  Minimization \label{sect:erm_consistency}}

We say that ERM is consistent if both its risk and its empirical
counterpart become as small as possible as $n\to\infty$, that is, if
\begin{equation*}
  L(h^*_n) \toinP \inf_{h\in\calH}L(h)
\end{equation*}
and
\begin{equation*}
  L_n(h^*_n) \toinP \inf_{h\in\calH}L(h).
\end{equation*}
where $\toinP$ denotes convergence in probability (see Appendix
\ref{app:basic_rates}). Nevertheless, \citet{vapnik_nature_2000}
argued that this concept of consistency allows for trivial cases in
which it fails to capture the intrinsic capacity of the hypotheses
class $\calH$ at hand. Their argument goes as follows. If it has been
established that ERM is not consistent for a specific learning
problem, an easy trick can turn it into a consistent one. It is enough
to add a hypothesis and to redefine the loss function so that the
newly added hypothesis minorizes the loss function. This means that
consistency in this sense would not necessarily depend on the
intrinsic \textit{capacity} of the hypotheses class at hand, but it
may depend on the existence of a hypothesis that minorizes the loss
function, that is, the existence of some $h^*$ such that
$\ell(h^*)\leq \ell(h)$ almost surely for all $h\in\calH$. Thus,
procedures should be consistent even if hypotheses with small losses
are removed. Consequently, they proposed a stronger concept, that of
\textit{nontrivial consistency}, In order to define it, we need to
introduce the set of hypotheses $\calH_c\subseteq\calH$ with loss
bigger than the threshold $c>0$, that is,
\begin{equation*}
  \calH_c = \{h\in\calH : L(h)\geq c\}.
\end{equation*}

\begin{definition}[Nontrivial Consistency]
  We say that ERM is \textit{nontrivially consistent} if for each
  nonempty $\calH_c$ (defined above) it holds that
  \begin{equation*}
    \inf_{h\in\calH_c}L_n(h) \toinP \inf_{h\in\calH_c}L(h).
  \end{equation*}
\end{definition}

Nontrivial consistency implies consistency and
\citet{vapnik_necessary_1991} gave necessary and sufficient conditions
for it to happen. They call this result The Key Theorem, which
reads as follows.

\begin{theorem}[The Key Theorem, \citet{vapnik_necessary_1991}]
\label{thm:key_theorem}
Let $\calH$ be a hypotheses class, let $\ell(h)$ be a loss function
with expected loss $L(h)$ such that
\begin{equation*}
  \sup_{h\in\calH}|L(h)|<\infty
\end{equation*}
Then the two following statements are equivalent
\begin{itemize}
\item The nontrivial consistency of ERM
\item The existence of one-sided uniform convergence over $\calH$ of
  the empirical losses to their expectations, that is,
  \begin{equation*}
    \sup_{h\in\calH}L(h) - L_n(h) \toinP 0
  \end{equation*}
  as $n\to\infty$
\end{itemize}
\end{theorem}

This result has as important consequence that the analysis of the
consistency of ERM has a worst case flavor. On the other hand it
establishes a link with the study of the uniform convergence of
empirical averages $L_n(h)$ to their expectations $L(h)$, to which
much attention has been paid
\citep[see][]{van_der_vaart_weak_1996}. Nevertheless, the theorem does not
provide tools for deriving nonasymptotic results on speeds of
convergence.

The random variable
\begin{equation*}
  \sup_{h\in\calH}L(h) - L_n(h)
\end{equation*}
has been called one-sided empirical process. Note that its
measurability can be a concern as the supremum is taken over the set
$\calH$, which might be uncountable. In order to avoid this issue, we
will assume that the empirical processes in question are
\textit{separable}, that is, that there exists some countable subset
of $\calH$ such that the supremum of the empirical processes over
$\calH$ equals the supremum taken over the countable subset. Note that
this is the case in which $\calH$ is a complete, separable, metric
space (i.e. a Polish space) and $\ell:h\mapsto\ell(h)$ is almost
surely continuous.

% \todo[inline]{Why do I write about two sided UC?}
% Note that the convergence of the two-sided empirical process in
% \eqref{eq:ep_two_sided} implies nontrivial consistency for both the
% maximum and the minimum of the empirical loss. Consequently, the
% conditions that are necessary and sufficient for this convergence to
% happen are also sufficient for the nontrivial consistency of ERM. In
% their classical article, \citet{gine_limit_1984} found such
% conditions, which we recall in the following after introducing some
% concepts.

% In the case that the hypothesis class $\calH$ has finite size
% $|\calH| = N<\infty$ nontrivial consistency can easily be established
% using a simple union bound. Indeed for any $\epsilon>0$
% \begin{align*}
%   \prob{\sup_{h\in\calH}L(h) - L_n(h) > \epsilon}
%   &\leq
%   \sum_{h\in\calH} \prob{L(h) - L_n(h) > \epsilon}\\
%   &\leq N\max_{h\in\calH}\prob{L(h) - L_n(h) > \epsilon}\\
%   &\to 0
% \end{align*}
% where the last line is a consequence of the Law of Large Numbers. Thus
% the possible complications for consistency arise when the classes we
% are dealing with are infinite (as long as the expected value of the
% losses exists).

% Even though we will mainly focus on finite classes in order to prove
% rates of convergence, the following ideas can be used to extend our
% results to infinite classes and we include them for the sake of
% completeness. One useful idea to prove bounds for empirical processes
% over infinite sets is making increasingly better approximations over
% finite sets. For this, covering numbers are used.
% \begin{definition}[Covering Number and Metric Entropy]
%   Let $(S,d)$ be a semimetric\footnote{Recall that a semimetric $d$
%     over a set $S$ is a symmetric function $d:S\times S\to\reals^+$
%     that satisfies the triangle inequality and $d(s,s)=0$ for each
%     $s\in S$. Note that the fact that $d(s_1,s_2)=0$ does not imply
%     that $s_1=s_2$, which is why semimetrics fail in general to be
%     metrics.} space. For $\epsilon>0$ the \textit{covering number}
%   $N(d,S,\epsilon)$ is the minimum ammount of balls of radious
%   $\epsilon$ with centers in $S$ whose union contains $S$. The
%   logarithm of the covering number $\log N(d,S,\epsilon)$ is called
%   \textit{metric entropy}.
% \end{definition}

% The conditions found by \citet[Section 8]{gine_limit_1984} are
% two. The first one is the existence of an integrable random variable
% $F$, called \textit{envelope}, such that
% \begin{equation*}
%   \sup_{h\in\calH}|\ell(h)| \leq F
% \end{equation*}
% almost surely.  The second condition is on the \textit{VC entropy}
% $H_n(\ell, \epsilon)$, defined for a bounded loss function $\ell$ as
% \begin{equation*}
%   H_n(\ell, \epsilon) = \bbP[ \log N(d_n, \mathcal{S}_n(\ell), \epsilon)].
% \end{equation*}
% where $d_n$ is the metric on $\reals^ n$ given by
% \begin{equation*}
%   d_n(x,y) = \max_{1\leq i \leq n }|x_i - y_i|.
% \end{equation*}
% and $\mathcal{S}_n$ is the random set given
% by
% \begin{equation*}
%   \mathcal{S}_n(\ell) = \{(\ell_1(h),\dots,\ell_n(h)): h\in\calH\}.
% \end{equation*}
% By the boundedness of $\ell$, the set $\mathcal{S}_n$ is a subset of a
% hyper cube in $\reals^n$.

% \begin{theorem}[\citet{gine_limit_1984}]
%   \label{theo:two_sided_ep_conditions}
%   Let $\ell$ be a loss function over a hypotheses class $\calH$ with
%   expected loss $L$ such that
%   \begin{equation*}
%     \sup_{h\in\calH}|L(h)|<\infty.
%   \end{equation*}
%   The convergence
%   \begin{equation*}
%     \sup_{h\in\calH} |L(h) - L_n(h)| \to 0
%   \end{equation*}
%   happens if and only if both the following conditions hold:
%   \begin{enumerate}
%   \item There exists an integrable envelope function $F$ for $\ell$
%   \item For any $C$ and $\epsilon>0$
%     \begin{equation*}
%       \lim_{n\to\infty}\frac{H_n(\ell_C, \epsilon)}{n} \to 0
%     \end{equation*}
%     where $\ell_C$ are the trimmed losses given by
%     \begin{equation*}
%       \ell_C(z,h) =
%       \begin{cases*}
%       C & for  $\ell(z,h) > C$  \\
%       \ell(z)  & for $|\ell(z,h)|\leq C$ \\
%       -C  & for $\ell(z,h) < -C$
%     \end{cases*}
%     \end{equation*}
%   \end{enumerate}
% \end{theorem}


% The study of empirical processes has been important not only for
% statistical learning theory, but also for other areas of statistics
% and probability. Some excellent references are
% \citet{pollard_convergence_1984}, \citet{van_der_vaart_weak_1996}, and
% \citet{talagrand_upper_2014}.


\section{Excess Losses \label{sect:excess_losses}}

In the remainder of this thesis, we will focus on the case in which
there exists a unique element $h^*\in\calH$ that minimizes the
expected loss, that is,
\begin{equation*}
  h^* = \argmin_{h\in\calH}L(h).
\end{equation*}
In this case, one can define the excess loss
$\varepsilon: \calH \times \calZ\to \reals$ as
\begin{equation*}
  \varepsilon(h) = \ell(h) - \ell(h^*),
\end{equation*}
so that the expected excess loss $E(h)$ can be written as
\begin{equation*}
  E(h)  = \bbP[\varepsilon(h)] =  \bbP[\ell(h) - \ell(h^*)] = L(h) - L(h^*),
\end{equation*}
which is non negative. Analogously, we define its empirical counterpart
$E_n(h) = \bbP[\varepsilon(h)]$.

The reason for studying excess losses lies in the fact that if for
some estimator $\hat{h}_n$ dependent on the data it happens that
\begin{equation*}
  E(\hat{h}_n) - E_n(\hat{h}_n) \toinP 0,
\end{equation*}
then the estimator is (non-trivially) consistent, as discussed in the
previous section. In the special case of the ERM $h^*_n$, this implies
that
\begin{equation*}
  E(h^*_n) = E_n(h^*_n) + o_{\bbP}(1).
\end{equation*}
But, because of the definition of the excess loss, the empirical
excess loss at the ERM is negative while the expected excess loss
itself is positive. This means that the last equation would imply that
\begin{equation*}
  E(h^*_n) = o_{\bbP}(1).
\end{equation*}
Since
\begin{equation}
  E(h^*_n)-E_n(h^*_n)\leq  \sup_{h\in\calH}E(h)-E_n(h),
\end{equation}
much attention has been paid to the study of the right hand side, that
is, to the study of uniform deviations random variables from their
averages. This has resulted in a fruitful approach.


\section{Rates from Concentration
  Inequalities \label{sect:concentration_inequalities}}

Concentration inequalities quantify the size of the deviations of a
random variable from its mean, in other words, to which extent they
\textit{concentrate} around it. The phenomenon of concentration holds
in many situations \citep[see][]{boucheron_concentration_2013}. We
will focus on excess losses for which a uniform subgaussian or
subgamma bound bound holds. We will aslo focus on finite classes
$\calH$ and explain the methods that can be used to extend these
results to infinite classes in Appendix \ref{sect:chaining}. The
reason for considering the following types of inequality is that it is
exactly inequalities of this type that one obtains through methods
such as the Cramér-Chernoff method (see Appendix
\ref{app:cramer_chernoff}).

Uniformly subgaussian excess losses include those that are bounded
(via Hoefding's inequality) and uniformly subgamma excess losses
include those that satisfy Bernstein's moment condition for fixed
constants (see Appendix \ref{sect:bernstein_inequality}). Said excess
losses satisfy the deviation inequality
\begin{equation}\label{eq:pac_considered}
  \prob{n^{1/2}(E(h) - E_n(h))\geq t}\leq \rme^{-\phi(t/\tau)}
\end{equation}
for all $h\in\calH$, some $\tau>0$, and $\phi(t) = \phi_2(t):= t^2$ in
the subgaussian case. In the subgamma case
$$\phi(t)= \phi_{\text{Bern}}^L(t) = \paren{\frac{\sqrt{1 +
      2Lt/\sqrt{n}} - 1}{L/\sqrt{n}}}^2$$
with some $L>0$. These two last inequalities can be written
equivalently in the more enlightening form
\begin{equation*}
  \prob{n^{1/2}(E(h) - E_n(h))\geq \tau\paren{\sqrt{t} +
    \frac{L}{2}\frac{t}{\sqrt{n}}}}\leq \rme^{-t},
\end{equation*}
for the subgamma case by inverting $\phi_{\text{Bern}}^L$ and as
\begin{equation*}
  \prob{n^{1/2}(E(h) - E_n(h))\geq \tau\sqrt{t}} \leq \rme^{-t},
\end{equation*}
in the subgaussian case. This means that for small values of $t$,
subgamma random variables behave essentially as subgaussian, while
they have exponential tails for larger values of $t$. Note also that
subgaussian excess losses are also subgamma.

In order to illustrate how rates of convergence are obtained from
concentration inequalities, let us first consider finite classes. In
Appendix \ref{sect:chaining} we point at how these results can be
extended to infinite classes.

Let $\calH$ be a finite hypotheses class of size $|\calH| = N<\infty$.
If the excess losses satisfy a concentration inequality such as those
discussed earlier, with a simple union bound one can obtain that
\begin{equation*}
  \prob{n^{1/2}\sup_{h\in\calH}E(h) - E_n(h) \geq t}
  \leq\rme^{-\phi(t/\tau)}
\end{equation*}
which means that with probability higher than $1-\delta$ it holds
simultaneously for all $h\in\calH$ that
\begin{equation*}
  E(h) \leq  E_n(h) + \frac{\tau}{n^{1/2}}\phi^{-1}\paren{\log\frac{N}{\delta}}.
\end{equation*}
Note that at an empirical risk minimizer $h^*_n$, the empirical excess
risk $E_n(h^*_n)$ is negative while the excess risk $E(h^*_n)$ itself
is positive. This implies that for the ERM rule $h^*_n$ with
probability higher than $1-\delta$
\begin{equation*}
  E(h^*_n) \leq  \frac{\tau}{n^{1/2}}\phi^{-1}\paren{\log\frac{N}{\delta}}.
\end{equation*}
In the case that excess losses are subgamma,
\begin{equation*}
  E(h^*_n) \leq  \tau\paren{\sqrt{\frac{\log\frac{N}{\delta}}{n}} + \frac{L}{2}\frac{\log\frac{N}{\delta}}{n}}
\end{equation*}
which implies rates of order $n^{-1/2}$ for excess losses. The
ubiquity of this type of rate is inherent to Cramer-Chernoff Method
(see Appendix \ref{app:cramer_chernoff}).


\begin{example}[Simple Normal Means Model]\label{ex:normal_means}
  Consider the task of estimating the mean of a sample $Z_1,\dots,Z_n$
  which is known to be generated from a normal distribution with
  variance 1 and unknown mean $\mu^*$. Identify
  $\calH = \{\calN(\mu, 1) : \mu\in\reals\}$ and use the maximum
  likelihood estimator $\mu^*_n$
  \begin{equation*}
    \mu^*_n
    = \argmax_{\mu\in\reals}\bbP_n[\ell(\mu,Z_i)]
    = \argmin_{\mu\in\reals}\frac{1}{2}\bbP_n[\mu-Z_i]^2.
  \end{equation*}
  Differentiation leads to
  \begin{equation*}
    \mu^*_n = \frac{1}{n}\sum_{i=1}^nZ_i,
  \end{equation*}
  which is normally distributed with variance $1/n$ and mean
  $\mu^*$.
  Thus, applying the Cramér-Chernoff method to both tails leads to
  \begin{equation*}
    \prob{\abs{\mu^*_n - \mu^*} \geq \epsilon} \leq 2\rme^{-n\epsilon^2 / 2},
  \end{equation*}
  that is, $\mu^*_n$ tends to $\mu^*$ at a $n^{-1/2}$ rate. Since the
  expected excess loss can be written in this case as
  \begin{equation*}
    E(\mu^*_n) = L(\mu^*_n) - L(\mu^*) = \frac{1}{2}(\mu^*_n - \mu^*)^2,
  \end{equation*}
  the previous bound translates into
  \begin{equation*}
    \prob{ L(h^*_n) - L(h^*) \geq \epsilon} \leq 2\rme^{-n\epsilon},
  \end{equation*}
  Thus the expected excess loss converges at a $n^{-1}$ rate to
  zero. Note that even though $\mu^*_n\toinP\mu^*$ at a $n^{-1/2}$
  rate, the loss converges at a faster rate to zero.
\end{example}


\section{The PAC-Bayesian Model\label{sect:pac_intro}}

There are situations in which it is either desirable or natural for a
learning algorithm to provide as an output a probability distribution
over the class of hypotheses. Predictions are thus made according to a
randomized hypothesis drawn from the output distribution so that the
predictions no longer depend deterministically on the data. An example
of such procedures are Gibbs learning algorithms, which make
predictions according to a randomized hypothesis drawn from a data
dependent distribution constructed in a pseudo-Bayesian fashion (see
Remark \ref{rem:information_risk_minimization}). In addition to Gibbs
algorithms, PAC-Bayesian bounds have been proven to lead to faster
rates of convergence in situations in which little is known for ERM
\citep{audibert_fast_2009}. Randomized algorithms and their
PAC-Bayesian analysis have proven to be necessary to obtain nontrivial
rates in situations in which data are not assumed to be iid and might
even be adversarial \citep{cesa-bianchi_prediction_2006}.

One model for such situation is the so-called \textit{PAC-Bayesian}
model, where PAC stands for \textit{Probably Approximately
  Correct}. In this model, it is assumed that there exists an initial
or \textit{prior} distribution $\Pi_0$ over the hypotheses class
$\calH$. Given the data $Z_1,\dots,Z_n$ an algorithm outputs a
\textit{posterior} distribution $\Pi_n$, in analogy to the Bayesian
terminology. Note that these distributions need not be related to each
other through Bayes formula (see Remark
\ref{rem:information_risk_minimization}) and can even be concentrated
on a single element, in which case we recover the deterministic case
(and consequently ERM). In this setting we are interested in studying
the $\Pi_n$-expected excess loss
\begin{equation*}
  \Pi_n[E(h)] = \Pi_n\bbP[\varepsilon(h)]
\end{equation*}
We stress the fact that the distribution $\Pi_n$ is allowed to depend
on the sample $Z_1,\dots,Z_n$, turning $\Pi_n[E(h)]$ into a random
variable.

We now recall the fundamental inequalities that have been used in
PAC-Bayesian analysis.

\subsection{PAC-Bayesian
  Inequalities \label{sect:rates_pac_bayesian}}

The first PAC-Bayesian bounds have been attributed to
\citet{mcallester_pac-bayesian_1998} and overviews were given by
\citet{mcallester_pac-bayesian_2013}, \citet{audibert_theorie_2004}
and \citet{van_erven_pac-bayes_2014}. We will prove a PAC-Bayesian
result in Theorem \ref{thm:pac_bayesian_bound}.  PAC-Bayesian theorems
can be thought of as refined union bounds and are based in the careful
study of the cumulant generating function of random
variables. Remarkably, they hold in great generality, including for
instance infinite hypotheses classes $\calH$. The main tool is
Donsker-Varadhan's variational formula, which we now recall
\citep[see][Corollary 4.14]{boucheron_concentration_2013}.

\begin{theorem}[Donsker-Varadhan Variational Formula]
  \label{thm:dv_formula}
  Let X be a real valued integrable random variable. Then for every
  $\eta\in\reals$ and every distribution $\bbP$
  \begin{equation}
    \log\bbP\rme^{\eta X} = \sup_{\bbQ}\eta\bbQ X - \KL(\bbQ, \bbP)
  \end{equation}
  where
  \begin{equation}\label{eq:def_kl}
    \KL(\bbQ, \bbP) = \bbQ\sqbrack{\log\frac{\rmd\bbQ}{\rmd\bbP}}
  \end{equation}
  is the Kullback-Leibler divergence and the supremum is taken with
  respect to all probability measures $\bbQ$ that are absolutely
  continuous\footnote{Remember that $\bbQ$ is absolutely continuous
    with respect to $\bbP$ if and only if for any measurable set $S$,
    the fact that $\bbP(S) = 0$ implies that $\bbQ(S)=0$.} with
  respect to $\bbP$.  Furthermore, the supremum is achieved at
  $\bbQ = \bbP_\eta$, where $\bbP_\eta$ is the probability measure
  with density
  \begin{equation*}
    \frac{\rmd \bbP_\eta}{\rmd \bbP} = \frac{\rme^{\eta
        X}}{\bbP[ \rme^{\eta X}]}
  \end{equation*}
\end{theorem}

As a consequence, we have that for each distribution $\Pi_n$ and
$\Pi_0$ (with $\Pi_n$ absolutely continuous with respect to $\Pi_0$)
it holds that
\begin{equation}\label{eq:kl_inequality}
  \Pi_n X \leq \inf_{\eta > 0} \frac{1}{\eta}(\log \Pi_0[\rme^{\eta X}] + \KL(\Pi_n, \Pi_0)).
\end{equation}
This means that if we choose $X$ carefully to be a suitable function
of $E(h) - E_n(h)$, roughly speaking, the problem of bounding
$\Pi_n[E(h) - E_n(h)]$ can be translated into the problem of bounding
a cumulant generating function defined by
\begin{equation}\label{eq:def_cumulant}
  \psi_X(\eta) = \log\bbP[\rme^{\eta X}],
\end{equation}
a well-known provider of deviation inequalities (see Appendix
\ref{app:cramer_chernoff}). Since we focus on excess losses that
satisfy inequalities of the form \eqref{eq:pac_considered}, a natural
choice for $X$ is
\begin{equation}\label{eq:x_h_definition}
  X_{h} = \phi\paren{\frac{n^{1/2}(E(h) - E_n(h))_+}{\tau}}.
\end{equation}
With this in mind, the following is the anticipated PAC-Bayesian
inequality and its proof.

\begin{theorem}\label{thm:pac_bayesian_bound}
  Let $\varepsilon$ an excess loss function over the hypothesis class
  $\calH$ that satisfies a concentration inequality of the form
  \begin{equation*}
    \prob{n^{1/2}(E(h) - E_n(h))\geq t}\leq \rme^{-\phi(t/\tau)}
  \end{equation*}
  for $\phi = \phi_2$ or $\phi = \phi_{\text{Bern}^L}$.
  Then it holds that with $\bbP$-probability higher than $1-\delta$
  \begin{equation*}
    \Pi_nE(h) \leq \Pi_nE_n(h) +
    \frac{\tau}{n^{1/2}}\phi^{-1}\paren{2\log\frac{2}{\delta} +
          2\KL(\Pi_n,\Pi_0)}
  \end{equation*}
\end{theorem}
\begin{remark}
  In the subgaussian case we recover the result originally obtained by
  \citet{mcallester_pac-bayesian_1998}. Our proof is a reworking of
  the exposition of \citet[Section 6]{boucheron_theory_2005}. Their
  proof is given in the context of classification. We note that the
  only property that is used in their proof is the subgaussianity of
  excess losses, which is a consequence of the boundedness of the
  excess loss and Hoefding's inequality.
\end{remark}
\begin{proof}
  Take $X_h$ as in \eqref{eq:x_h_definition}. We will use
  \eqref{eq:kl_inequality}, so we need to bound the quantity
  \begin{equation*}
    \log\Pi_0[\rme^{\eta X_h}].
  \end{equation*}
  Using Markov's inequality and Fubinni's theorem we obtain that for
  any $\epsilon>0$ it holds that
  \begin{equation*}
    \prob{\Pi_0[\rme^{\eta X_h}] \geq \epsilon} \leq
    \frac{\Pi_0\bbP[\rme^{\eta X_h}]}{\epsilon}.
  \end{equation*}
  Note that
  \begin{align*}
    \bbP\rme^{\eta X_h}
    &= 1 + \int_{0}^{\infty}\prob{\rme^{\eta X_h} \geq t}\rmd t\\
    &= 1 + \int_{0}^{\infty}\prob{ X_h \geq t/\eta}\rme^{t}\rmd t\\
    &= 1 + \int_{0}^{\infty}\prob{ n^{1/p}(E(h)-E_n(h)) \geq \tau\phi^{-1}(t /
      \eta)}\rme^t \rmd t\\
    &\leq 1 + \int_0^\infty\rme^{-t(1/\eta - 1)}\rmd t\\
    &= 1 + \frac{\eta}{1-\eta}
  \end{align*}
so that picking
\begin{equation*}
  \eta = \frac{1}{2}
\end{equation*}
and
\begin{equation*}
  \delta = \frac{2}{\epsilon}
\end{equation*}
one obtains that with probability higher than $1-\delta$
\begin{equation*}
  \Pi_0[\rme^{\eta X_h}] \leq \frac{2}{\delta}.
\end{equation*}
Using Jensen's inequality, \eqref{eq:kl_inequality} and the inequality
that we just obtained one obtains that with probability higher than
$1-\delta$
\begin{equation*}
  \phi\paren{\Pi_n\sqbrack{\frac{n^{1/2}(E(h) - E_n(h))_{+}}{\tau}}} \leq 2\paren{\log\frac{2}{\delta} + \KL(\Pi_n, \Pi_0)}.
\end{equation*}
Inversion, rearrangement and the fact that $x\leq x_+$ lead to the
result.
\end{proof}
\begin{remark}
  Note that this bound holds for general hypotheses classes and for
  arbitrary distributions $\Pi_n$. Thus, in the case that
  $n^{1/2}(E(h)-E_{n}(h))$ has a subgaussian right tail with scale
  parameter $\tau$, then the bound reads
  \begin{equation*}
    \Pi_nE(h) \leq \Pi_nE_n(h) +
    \tau\sqrt{\frac{2\log\frac{2}{\delta} +
      2\KL(\Pi_n,\Pi_0)}{n}}
  \end{equation*}
  while in the case that $n^{1/2}(E(h)-E_{n}(h))$ is sub gamma then
  \begin{equation*}
    \Pi_nE(h) \leq \Pi_nE_n(h) +
    \tau\paren{\sqrt{\frac{2\log\frac{2}{\delta} +
        2\KL(\Pi_n,\Pi_0)}{n}} + L\frac{\log\frac{2}{\delta} +
      \KL(\Pi_n,\Pi_0)}{n}},
  \end{equation*}
  that is, a $n^{-1/2}$ rate in both cases.
\end{remark}

\begin{remark}
  This inequality can be applied also for the analysis of ERM in the
  case that $\calH$ is finite. Indeed, suppose that $\Pi_0$ is the
  uniform distribution on $\calH$, then if $\Pi_n$ is the distribution
  concentrated at the ERM $h^*_n$, Theorem
  \ref{thm:pac_bayesian_bound} gives that
  \begin{equation*}
    E(h^*_n) \leq
    \frac{\tau}{n^{1/2}}\phi^{-1}\paren{2\log\frac{2|\calH|}{\delta}}
  \end{equation*}
  in the case that the excess losses are either subgaussian ($\phi =
  \phi_2$) or subgamma ($\phi = \phi_{\text{Bern}}^L$).
\end{remark}

As we have seen, this PAC-Bayesian inequality makes use of the tail
behavior of the excess losses. In the first section of the next
chapter we will use a different approach to obtaining such
inequalities.


% \section{Characterization of Tail Behavior}

% One of the main tool for obtaining non asymptotic results are
% concentration inequalities, which describe the tail behavior of the
% excess losses. Much has been said about concentration inequalities,
% and the phenomenon of concentration of measure has been studied in
% many situations \citep[See][]{boucheron_concentration_2013}. The
% behavior of the tails of random variables has been characterized in
% different ways for different purposes. For instance, in PAC-Bayesian
% inequalities the cumulant generating function plays a central role,
% while in extending deviation inequalities to infinite families,
% \textit{Orlicz norms} are important.

% Let us first ilustrate this relationship using what is Probably the
% best known concentration inequality, Hoefdings's inequality, to
% introduce sub-gaussian random variables. In this setting, Hoefding's
% inequality states that if an excess loss $\varepsilon(h)$ is
% bounded and takes values in the interval $[-M,M]$, then the tails of
% excess losses satisfy
% \begin{equation*}
%   \prob{n^{1/2}(E(h) - E_n(h))_+\geq t}\leq \rme^{-\frac{t^2}{2v}}.
% \end{equation*}
% and
% \begin{equation*}
%   \prob{n^{1/2}(E(h) - E_n(h))_-\geq t}\leq \rme^{-\frac{t^2}{2v}}.
% \end{equation*}
% with $v = M^2$. In the case that one of this inequalities hold, it is
% said that $n^{1/2}(E(h) - E_n(h))$ has a \textit{sub-gaussian} (right
% or left) tail, as their tail probabily is dominated by that of a
% gaussian random variable.

% Sub-Gaussian random variables can be characterized in at least two
% more ways: using their cumulant generating function, which is central
% to PAC-Bayesian analysis, and using a norm, called \textit{Orlicz
%   norm}, which is central in \textit{Chaining}. First, usig
% Cramér-Chernoff's technique (See Appendix XXX), it is possible to show
% that a random variable $E(h) - \varepsilon(h)$ has subgaussian tails
% if and only if its cumulant generating function
% $\psi_{E(h) - \varepsilon(h)}$, defined by
% \begin{equation*}
%   \psi_{E(h) - \varepsilon(h)}(\eta) = \log\bbP[\rme^{E(h) - \varepsilon(h)}]
% \end{equation*}
% is dominated by that of a centered Gaussian random variable with some
% variance $v$, that is, if
% \begin{equation*}
%   \psi_{E(h) - \varepsilon(h)}(\eta) \leq \frac{1}{2}v\eta^2.
% \end{equation*}
% Since we assume that samples $Z_1,\dots,Z_n$ are iid, the preceding
% implies that
% \begin{equation*}
%   \psi_{n^{1/2}(E(h) - E_n(h))}(\eta) \leq \frac{1}{2}v\eta^2
% \end{equation*}
% Second, sub-gaussianity can be characterized by the existence of some
% $C>0$ such that
% \begin{equation*}
%   \bbP[\rme^{C(E(h) - \varepsilon(h))^2} - 1] \leq 1
% \end{equation*}
% which can be rewritten in the somewhat complicated way
% \begin{equation*}
%   \norm{E(h) - \varepsilon(h)}_{\Phi_{2}} < \infty
% \end{equation*}
% where $\Phi_{2}(x) = \rme^{x^2}-1$ and $\norm{\cdot}_{\Phi}$ is the
% \textit{Orlicz Norm} defined for a general positive nondecreasing
% convex $\Phi$ with $\Phi(0) = 0$ by
% \begin{equation*}
%   \norm{X}_{\Phi} = \inf\bracks{C:\bbP[\Phi(C|X|)]\leq 1}.
% \end{equation*}
% Note that this also means that
% \begin{equation*}
%   \norm{n^{1/2}(E(h) - E_n(h))}_{\Phi_{2}} < \infty.
% \end{equation*}

% These characterizations are not exclussive to the sub-gaussian
% case. In the case that the excess loss satisfies Bernstein's moment
% condition, that is, in the case that there exist positive $v$ and $c$
% such that
% \begin{equation*}
%   \bbP[\varepsilon^2(h)]\leq v
% \end{equation*}
% and
% \begin{equation*}
%   \bbP[\varepsilon^m(h)_{-}]\leq \frac{1}{2}m!vc^{m-2}
% \end{equation*}
% for $m>2$, then
% \begin{equation*}
%   \prob{E(h) - \varepsilon(h)\geq \sqrt{2vt} + ct}\leq \rme^{-t}
% \end{equation*}
% which, using Cramér-Chernoff's method implies that this happens if and
% only if
% \begin{equation*}
%   \psi_{E(h)-\varepsilon(h)}(\eta)\leq \frac{1}{2}\frac{v\eta^2}{1-c\eta},
% \end{equation*}
% for $\eta\in[0,1/c)$. This interpolates between the subgaussian case
% for small $\eta$ and the subexponential calse for large $\eta$. Note
% that this implies that
% \begin{equation*}
%   \prob{n^{1/2}(E(h) - E_n(h))\geq \sqrt{2vt} + \frac{c}{\sqrt{n}}t}\leq \rme^{-t}.
% \end{equation*}
% Likewise, for
% $$\Phi_{\text{Bern}}^{L}(x) = \rme^{\paren{\frac{\sqrt{1 + 2Lt} -
%       1}{L}}^2}-1$$ with $L = c/\sqrt{2vn}$ and $\tau = \sqrt{2v}$ one
% has that
% \begin{equation*}
%   \norm{n^{1/2}(E(h) - E_n(h)}_{\Phi_{\text{Bern}}^{L}}<\infty.
% \end{equation*}

% Finally, for these reasons, we focus on random variables that satisfy
% inequalities of the form


% The conditions that we study will imply that inequalities of these
% types hold. For instance, under an additional condition, the witness
% condition implies that $n^{1/p}(E(h) - c'E_n(h))$ is subgamme, while
% the central condition implies that

% For these reasons we will consider in what follows random variables
% for which inequalities of the form
% \begin{equation*}\label{eq:generic_concentration_inequality}
%   \prob{n^{1/p}(E(h) - E_n(h))\geq t}\leq \rme^{-\phi(t/\tau)}
% \end{equation*}
% for some $\phi$ which is increasing, convex, positive for which
% $\phi(0)=0$.


\chapter{Toward Fast Rates \label{sect:pac_fast_rates}}

In this section we present a second risk bound derived by
\citet{grunwald_fast_2016} based on an inequality previously obtained
by \citet{zhang_information-theoretic_2006}, itself a variation of the
PAC-Bayesian inequalities studied in Section
\ref{sect:pac_intro}. These excess risk bounds hold in the case that
two conditions are met and they lead to potentially fast rates. The
first of these two conditions (Condition \ref{cond:witness}) is on the
relative weight of the right tail of the excess losses and is called
\textit{witness condition}. The second condition is called
\textit{(strong) central condition} and is a uniform condition on the
rate of decrease of the cumulant generating function of the negative
excess losses close to the origin.

In Section \ref{sect:second_pac_bayes} we introduce
\citeauthor{zhang_information-theoretic_2006}'s inequality. We explain
how this inequality can be used either as a bound on the expected
loss, from which the results obtained in Section \ref{sect:pac_intro}
can be recovered, and how it was used both by
\citeauthor{zhang_information-theoretic_2006} and by
\citeauthor{grunwald_fast_2016} as a bound on a quantity closely
related to the cumulant generating function of the excess loss, its
\textit{annealed expectation} (which we define below). In Section
\ref{sect:conditions_fast} we introduce the conditions that
\citeauthor{grunwald_fast_2016} used to obtain fast bounds using
\citeauthor{zhang_information-theoretic_2006}'s inequality, and that
will be the main topic of analysis of Chapter \ref{sect:relations}. In
Section \ref{sect:fast_risk_bound} we present
\citeauthor{grunwald_fast_2016}'s bound, which leads to fast
concentration and explain in Example \ref{ex:risk_bound_with_zhang}
how it can be used to obtain fast rates for ERM in the case that
losses are Lipschitz continuous under the conditions introduced in
Section \ref{sect:conditions_fast}.

\section{Second PAC-Bayesian Inequality \label{sect:second_pac_bayes}}

In this section we consider another type of PAC-Bayesian
Inequalities. They have been attributed to
\citet{zhang_e-entropy_2006} and
\citet{zhang_information-theoretic_2006}. These inequalities are are
also based on the use of Donsker-Varadhan's variational formula from
Theorem \ref{thm:dv_formula}. Later, \citet{grunwald_fast_2016} used
these formulas to derive rates of convergence for unbounded loss
functions under conditions on the tails of the excess losses and its
cumulant generating function. Just as it was the case in the previous
section, the inequalities that we explain here relate the cumulant
generating function of $E(h)-E_n(h)$ and the Kulback-Leibler
divergence and they can be used to bound the expected excess loss
directly. But now, these inequalities will be used as a bound on the
\textit{annealed expectation}, which at the same time and under
additional conditions can be used to bound the expected excess
loss. We start by explaining some consequences of
\citeauthor{zhang_information-theoretic_2006}'s inequality, the main
element of this discussion, which we then prove it in Theorem
\ref{thm:zhang_inequality}.

\citeauthor{zhang_information-theoretic_2006}'s inequality implies
that for each $\delta\in(0,1)$ with probability higher than $1-\delta$
the relation
\begin{equation}\label{eq:zhang_implication}
  \Pi_n\sqbrack{ - E_n(h) -
          \frac{1}{\eta }\psi_{ - \varepsilon(h)}(\eta)} -
          \frac{\KL(\Pi_n, \Pi_0)}{\eta n}\leq
        \frac{\log\frac{1}{\delta}}{\eta n}
\end{equation}
holds for every choice of prior $\Pi_0$ and posterior $\Pi_n$, where
$\psi_{-\varepsilon(h)}(\eta)$ is the cumulant generating function of
$-\varepsilon(h)$ as defined in \eqref{eq:def_cumulant} on page
\pageref{eq:def_cumulant}. One could rewrite this inequality as
\begin{equation*}
  \Pi_n\sqbrack{E(h)} \leq  \Pi_n\sqbrack{E_n(h) +\frac{1}{\eta }\psi_{E(h) - \varepsilon(h)}(\eta)} +
  \frac{\KL(\Pi_n, \Pi_0) + \log\frac{1}{\delta}}{\eta n}
\end{equation*}
and use it as a bound on the expected excess risk (see Remark
\ref{rem:zhang_interpretation}). Instead, \citet{grunwald_fast_2016}
(and \citeauthor{zhang_information-theoretic_2006} himself in his
original article) use it as a bound on
\begin{equation}\label{eq:annealed_definition}
  A_\eta(h) =  -\frac{1}{\eta}\psi_{-\varepsilon(h)}(\eta) = -\frac{1}{\eta}\log \bbP[\rme^{-\eta \varepsilon(h)}].
\end{equation}
which is called the \textit{annealed expectation} of the excess loss,
or \textit{free energy} in the statistical mechanics
community. Additionally define the \textit{Information Complexity}
$\IC(\eta, n)$ as
\begin{equation}\label{eq:def_ic}
  \IC(\eta,n) = \Pi_nE_n(h)  + \frac{\KL(\Pi_n, \Pi_0)}{\eta n}.
\end{equation}
which is a combination of the empirical excess loss and the Kullback
Leibler divergence, as defined in \eqref{eq:def_kl}. This rearrangement
leads to the conclusion that
\begin{equation*}
  \Pi_n\sqbrack{A_\eta(h)} \leq \IC(\eta,n)+ \frac{\log\frac{1}{\delta}}{\eta n}.
\end{equation*}
with probability higher than $1-\delta$. \citet{grunwald_fast_2016}
and others have observed that rates of convergence for $\IC(\eta, n)$
can be obtained as $n\to\infty$. Indeed, if concentration inequalities
are available for the excess losses $\varepsilon(h)$, rates for
$\Pi_nE_n(h)$ can be obtained. At the same time, randomized algorithms
can be designed with appropriate choice of prior $\Pi_0$ and
posteriors $\Pi_n$ which lead to a fast decrease of the $\KL$ term. We
show exactly this in the Example \ref{ex:risk_bound_with_zhang}.


Nevertheless, even if it is possible to obtain rates of convergence
for the annealed expectation, this does not imply the same for the
expected excess loss, the main quantity of interest. Recall that by
Jensen's inequality $A_\eta(h)\leq E(h)$, while in light of our
previous observations we are interested in bounds on the opposite
direction, that is, bounds of the form $E(h)\lesssim A_\eta(h)$.
Accordingly, \citet{grunwald_fast_2016} found two additional mild
conditions under which such an inequality can be derived (see Theorem
\ref{thm:annealed_inverse_bound}). Those conditions will be the focus
of Section \ref{sect:conditions_fast}.

With this in mind, we give the Theorem due to \citet[Lemma
2.1]{zhang_information-theoretic_2006}, which implies
\eqref{eq:zhang_implication} at the beginning of this discussion.

\begin{theorem}[Information Exponential Inequality]
  \label{thm:zhang_inequality}
  Let $\varepsilon$ an excess loss function over the hypothesis class
  $\calH$. Then for any prior distribution $\Pi_0$ and any posterior
  $\Pi_n$ it holds that
  \begin{equation*}
    \bbP\sqbrack{\exp\paren{\eta n\Pi_n\sqbrack{ A_\eta(h) -
          \IC(\eta, n) }}} \leq  1.
  \end{equation*}
\end{theorem}
\begin{proof}
  The starting point is again Donsker-Varadhan's variational formula
  as described in Theorem \ref{thm:dv_formula} and its consequence
  from \eqref{eq:kl_inequality}, which we rewrite with $\eta n$
  instead of $\eta$ and rearrange as
  \begin{equation*}
    \eta n\Pi_n[ X] - \KL(\Pi_n, \Pi_0)\leq  \log \Pi_0[\rme^{\eta n X}]  .
  \end{equation*}
  for any $\eta>0$ and any random variable $X$.  Taking exponentials and
  taking $\bbP$-expectations we obtain that
  \begin{equation}\label{eq:dv_formula_rewriting}
    \bbP[\rme^{\eta n \Pi_n X - \KL(\Pi_n, \Pi_0)}] \leq
    \bbP\Pi_0[\rme^{\eta n X}].
  \end{equation}
  In \eqref{eq:dv_formula_rewriting} take
  \begin{align*}
    X_h &= - E_n(h) - \frac{1}{\eta n}\psi_{ - E_n(h)}(\eta n
          )\\
        & =  - E_n(h) - \frac{1}{\eta }\psi_{ -
          \varepsilon(h)}(\eta)\\
        &= A_{\eta}(h) - E_n(h).
  \end{align*}
  This choice and Fubini's theorem makes the right hand side of the
  inequality in \eqref{eq:dv_formula_rewriting} smaller than one and
  thus the result follows.
\end{proof}

The main consequence of Theorem \ref{thm:zhang_inequality} is that it
implies inequalities both in probability and in expectation for
$\Pi_n[A_{\eta}(h) - \IC(\eta, n)]$. Indeed, by Markov's inequality
\begin{align*}
  \prob{\Pi_n\sqbrack{A_\eta(h) - \IC(\eta,n)}\geq t}
  &=
    \prob{\exp\paren{\eta n \Pi_n\sqbrack{A_\eta(h) - \IC(\eta,n)}}\geq \rme^{\eta n
    t}}\\
  &\leq \rme^{-\eta t}.
\end{align*}
This means that with probability higher than $1-\delta$ it holds that
\begin{equation*}
  \Pi_n\sqbrack{A_\eta(h) - \IC(\eta,n)} \leq \frac{\log\frac{1}{\delta}}{\eta n},
\end{equation*}
which is just a rewriting of
\eqref{eq:zhang_implication}. Additionally, by Jensen's inequality
\begin{equation*}
  \bbP\sqbrack{\Pi_n\sqbrack{A_\eta(h) - \IC(\eta,n)}}\leq 0.
\end{equation*}

\begin{remark}\label{rem:zhang_interpretation}
  The inequality obtained in Theorem \ref{thm:zhang_inequality} can
  alternatively be written as
  \begin{equation}\label{eq:zhang_bound}
    \bbP\sqbrack{\exp\paren{\eta n\Pi_n\sqbrack{E(h) - E_n(h) -
          \frac{1}{\eta }\psi_{E(h) - \varepsilon(h)}(\eta) -
          \frac{\KL(\Pi_n, \Pi_0)}{\eta n}}}} \leq  1
  \end{equation}
  By our previous remark, this implies that with probability higher
  than $1-\delta$ the inequality
  \begin{equation*}
    \Pi_n[E(h)]
    \leq
    \Pi_n\sqbrack{E_n(h) +  \frac{1}{\eta }\psi_{E(h) - \varepsilon(h)}(\eta)} -
    \frac{\KL(\Pi_n, \Pi_0) + \log\frac{1}{\delta}}{\eta n}
  \end{equation*}
  holds. This implies two things. First, that a bound on the cumulant
  generating function of the centered excess loss
  $E(h) - \varepsilon(h)$ can be turned into bound on the excess
  loss. Second, that since we have the freedom to choose $\eta$, this
  bound can be optimized. Even though it is not novel, we think it is
  instructive to also show how such bounds can be derived in this
  way. We do this in the following corollary, which is the same result
  as the obtained in Theorem \ref{thm:pac_bayesian_bound} up to
  constants.
\end{remark}

\begin{corollary}\label{cor:risk_subgamma}
  Let $\varepsilon$ such that for
  $\varepsilon(h)-\bbP[\varepsilon(h)]$ a uniform subgamma bound holds
  (see Section \ref{sect:concentration_inequalities}). Then, with
  probability higher than $1-\delta$,
  \begin{equation*}
    \Pi_n\sqbrack{E(h)}
    \leq
    \Pi_n\sqbrack{E_n(h)} + \sqrt{2v\frac{\KL(\Pi_n, \Pi_0) + \log\frac{1}{\delta}}{n}}  +
    c\frac{\KL(\Pi_n, \Pi_0) + \log\frac{1}{\delta}}{ n}.
  \end{equation*}
  for positive constants $v$ and $c$ and for all prior distributions
  $\Pi_0$ and posterior distributions $\Pi_n$.
\end{corollary}
\begin{remark}
  Notice that with $\tau = \sqrt{2v}$ and $L/2 = c/\sqrt{2v}$ we
  obtain the same bound as we would have obtained in Theorem
  \ref{thm:pac_bayesian_bound} up to constants.
\end{remark}
\begin{proof}
  Recall that if an excess loss function has a subgamma right tail,
  then there exists $v$ and $c$ such that (see Appendix
  \ref{sect:bernstein_inequality})
  \begin{equation}\label{eq:bernstein_cgf_remember}
    \psi_{E(h) - \varepsilon(h)}(\eta) \leq \frac{1}{2}\frac{v\eta^2}{1-c\eta}
  \end{equation}
  for each $h\in\calH$ and for $0<c\eta<1$.  Because of Bernstein's
  moment condition, the bound from \eqref{eq:bernstein_cgf_remember}
  holds for every $\eta \in [0,1/c)$. By the previous remark and and
  \eqref{eq:bernstein_cgf_remember} one obtains that with probability
  higher than $1-\delta$
  \begin{equation*}
    \Pi_n\sqbrack{E(h)}
    \leq
    \Pi_n\sqbrack{E_n(h)} + \frac{1}{2}\frac{v\eta}{1-c\eta}  +
    \frac{\KL(\Pi_n, \Pi_0) + \log\frac{1}{\delta}}{\eta n}.
  \end{equation*}
  as long as $0<c\eta<1$. Since this inequality holds for all $\eta$,
  we can take $\eta$ so as to minimize the right hand. Since the
  function $x\mapsto \frac{x}{1-Cx} + \frac{K}{x}$ is minimized at
  $x^* = \frac{\sqrt{K}}{1+C\sqrt{K}}$ and has maximum equal to
  $2\sqrt{K} + CK$ we obtain the result.
\end{proof}

\begin{remark}[Information Risk
  Minimization] \label{rem:information_risk_minimization}
  \citet{zhang_information-theoretic_2006} used his inequality to in
  order to produce randomized algorithms according to what he called
  \textit{Information Risk Minimization}. Note three things. First, if
  we write \eqref{eq:zhang_implication} explicitly in terms of the
  losses $\ell$ and their expected values $L$, we obtain that with
  probability higher than $1-\delta$ and for every choice of prior
  $\Pi_0$ and posterior $\Pi_n$ it holds that
  \begin{equation} \label{eq:information_risk_minimization}
    \Pi_n\sqbrack{-\frac{1}{\eta}\psi_{-\ell(h)}(\eta)}
    \leq \Pi_n\sqbrack{L_n(h)} + \frac{\KL(\Pi_n, \Pi_0) }{\eta n} +
    L(h^*) - L_n(h^*) +
    \frac{\log\frac{1}{\delta}}{\eta n}.
  \end{equation}
  Second, by Jensen's inequality the left hand side of this equation
  is smaller than the expected loss $\Pi_n L(h)$.  Third, the right
  hand consists of two parts: one which does not depend on $\Pi_n$,
  and another one that can be interpreted as a penalized empirical
  risk. Indeed, it is the sum of the empirical risk and the Kullback
  Leibler divergence, a measure of the inefficiency of using the
  distribution $\Pi_0$ on $\calH$ when the true distribution is
  $\Pi_n$ \citep[see][Section 2.3]{cover_elements_2006}. Consequently,
  if one is to minimize the right hand side over the choices of
  $\Pi_n$, from Donsker Varadhan's variational formula it follows that
  the minimum is attained at the distribution $\Pi_n^*$ with density
  with respect to $\Pi_0$ given by
  \begin{equation*} \frac{\rmd\Pi_{n}^*}{\rmd \Pi_0} =
    \frac{\rme^{-\eta \sum_{i=1}^n \ell_i(h)}}{\Pi_0[\rme^{-\eta
        \sum_{i=1}^n \ell_i(h)}]}.
  \end{equation*}
  \textit{Information Risk Minimization} is exactly the procedure of
  choosing the posterior $\Pi^*_n$ in this way. Notice that in the
  case of density estimation (see Example \ref{ex:density_estimation})
  the loss is the negative log-likelihood, which turns the posterior
  distribution $\Pi_n^*$ into the Bayesian posterior by taking
  $\eta = 1$.
\end{remark}




\section{Conditions for Faster Concentration \label{sect:conditions_fast}}

In the previous section we proved bounds on the annealed expectation
$A_\eta(h)$ of the excess loss (see its definition in
\eqref{eq:annealed_definition} on page
\pageref{eq:annealed_definition}) and noticed that in general it is
smaller than the expected excess loss $E(h)$ by Jensen's
inequality. We anticipated that, under suitable conditions,
\citet{grunwald_fast_2016} obtained an estimate of the form
$E(h)\lesssim A_{\eta}(h)$. This section is about those conditions,
and an additional condition, Bernstein's condition, which has been
proven to lead to fast bounds.

We start by describing Bernstein's condition. We show how it leads to
fast rates in the case that excess losses are bounded and hypothesis
classes are finite and cite results that extend this to infinite
classes. After that, we describe two conditions used by
\citet{grunwald_fast_2016} to obtain a bound on the expected excess
loss through this method. These two conditions are the \textit{Strong
  Central Condition} (Condition \ref{cond:central} on page
\pageref{cond:central}), and the \textit{Witness Condition} (Condition
\ref{cond:witness} on page \pageref{cond:witness}). We recall
\citeauthor{grunwald_fast_2016}'s exact result in Theorem
\ref{thm:annealed_inverse_bound} in the next section, where we also
discuss its implications.


\subsection{Bernstein's Condition \label{sect:bernstein}}

This condition leads to advantages when using Bernstein's inequality
(see Appendix \ref{sect:bernstein_inequality}), hence the name. The
condition reads
\begin{condition}\label{cond:bernstein}
  The excess losses $\varepsilon(h) = \ell(h)-\ell(h^*)$ on a
  hypotheses class $\calH$ with expected values
  $E(h) = \bbP[\varepsilon(h)]$ satisfy for all $h\in\calH$
  \begin{equation*}
    \bbP[\varepsilon^2(h)]\leq B (\bbP[\varepsilon(h)])^\beta
  \end{equation*}
  for some $B>0$ and some $0\leq\beta\leq1$.
\end{condition}
\begin{remark}
  We refer to the condition with $\beta = 1$, which is its strongest
  version, simply as Bernstein's condition. This is because we are
  mainly interested in its implications in this case. We will mention
  explicitly when we use other values of $\beta$. In order to avoid
  confusion we refer to it simply as Bernstein's condition while we
  refer to condition in Theorem \ref{thm:bernstein_inequality} on page
  \pageref{thm:bernstein_inequality}, which is also commonly
  associated with Bernstein's name, as Bernstein's \textit{moment}
  condition.
\end{remark}

In order to motivate why Condition \ref{cond:bernstein} leads to
faster rates, let $\calH$ be a finite hypotheses class of size
$|\calH| = N$ and suppose that the excess loss $\varepsilon$ is
uniformly bounded from above by some positive $b<\infty$, that is
$\varepsilon(h) \leq b$ almost surely for all $h\in\calH$.  Then,
Bernstein's inequality and the union bound gives that with probability
higher than $1-\delta$ it holds for each $h\in\calH$ that
\begin{equation*}
  E(h) \leq E_n(h) +
  \sqrt{2\bbP[\varepsilon^2(h)]\frac{\log\frac{N}{\delta}}{n}} +
  \frac{b}{3}\frac{\log\frac{N}{\delta}}{n}.
\end{equation*}
If as in Condition \ref{cond:bernstein} it holds for each $h\in\calH$
\begin{equation*}
  \bbP[\varepsilon^2(h)]\leq B E(h)^\beta
\end{equation*}
for fixed $\beta > 0$ and $B>0$, then also using the fact that at the
empirical risk minimizer $h^*_n$ empirical risk $E_n(h^*_n)$ is
negative one obtains that
\begin{equation*}
  E(h^*_n) \leq
  \sqrt{2BE(h^*_n)^\beta\frac{\log\frac{N}{\delta}}{n}} +
  \frac{b}{3}\frac{\log\frac{N}{\delta}}{n}.
\end{equation*}
Rearranging this one obtains that
\begin{equation*}
  E(h^*_n) \leq \paren{2B\frac{\log\frac{N}{\delta}}{n}}^{\frac{1}{2-\beta}},
\end{equation*}
which implies faster rates than $n^{-1/2}$ in the case that
$\beta>0$. This result was extended to infinite classes by
\citet{bartlett_empirical_2006}. Their argument is more involved and
uses a generalization of Bernstein's inequality due to
\citet{talagrand_sharper_1994}. In the case that losses are unbounded,
to our best knowledge, it is still an open question whether Condition
Bernstein's condition leads to fast rates for ERM. Nevertheless,
\citet{audibert_fast_2009} found a nonstandard randomized algorithm
that achieves fast rates if it holds.

In the special case of classification, Bernstein's condition has been
known to also lead to faster rates. It can be shown that it is implied
by a well studied condition found by \citet{mammen_smooth_1999}, which
also leads to fast rates \citep[see also][Section
5]{boucheron_theory_2005}.


\begin{remark}
  For finite classes Bernstein's condition holds every time that the
  expected loss minimizer $h^* = \argmin_{h} L(h)$ is unique and the
  excess losses are bounded from above. Indeed, in that case for each
  $h\neq h^*$ it holds that for some $m>0$ the expected loss is bigger
  than $m$, that is, $\bbP[\varepsilon(h)]>m$. On the other hand, by
  the boundedness of the excess losses, there is some $b$ such that
  $\bbP[\varepsilon^2(h)]\leq b$. This means that if $h\neq h^*$, then
  $$\bbP[\varepsilon^2(h)]\leq \frac{b^2}{m}\bbP[\varepsilon(h)],$$ that
  is, Bernstein's condition holds for $B = b^2/m$.

  Still, in practical situations, with finite classes one might not
  always expect a fast rate even if the expected loss minimizer $h^*$
  is unique (see Remark \ref{rem:finite_classes}). In those cases
  Bernstein's condition becomes harder to satisfy.
\end{remark}



% Alternatively, if we are in the situation of Remark
% \ref{rem:uniform_over_p},  Bernstein's condition becomes would have to
% hold for each $\bbP\in\calP$.

\subsection{The Central Condition}

The central condition is a uniform condition on the rate of decrease
near the origin of the cumulant generating function of
$-\varepsilon(h)$. We will show that it leads to fast rates for the
empirical risk of ERM and even though we will mainly focus on its
strong version, we cite also its weaker version as proposed by
\citet{grunwald_fast_2016}, which leads to intermediate rates.


\begin{condition}[The Central Condition]\label{cond:central}
  The excess loss $\varepsilon(h)$ on the hypotheses class $\calH$
  satisfies the central condition if there exists a bounded
  non-decreasing function $\eta:[0,\infty)\to[0,\infty)$ and an
  element $h^*\in\calH$ so that for all $\epsilon>0$, for
  $\eta = \eta(\epsilon)$
  \begin{equation}\label{eq:def_central_condition}
    \sup_{h\in \calH}\psi_{-\varepsilon(h)}(\eta)\leq \eta\epsilon.
  \end{equation}
  If it happens that $\eta$ can be chosen not to depend on $\epsilon$,
  then
  \begin{equation*}
    \sup_{h\in \calH}\psi_{-\varepsilon(h)}(\eta)\leq 0.
  \end{equation*}
  for some $\eta>0$ and we say that the \textit{strong central
    condition} holds.
\end{condition}

\begin{remark}
  In both its strong and weaker version after using Jensen's
  inequality, the central condition implies that for any $h$
  \begin{equation*}
    L(h) \geq L(h^*),
  \end{equation*}
  that is, $h^*$ minimizes the expected loss.
\end{remark}


\begin{example}[Maximum Likelihood Estimation and the Central
  Condition]
  \label{ex:mle_central}
  In the case of well specified maximum likelihood estimation the
  strong central condition is satisfied. Indeed, if $\calP$ is a set
  of probability densities and for $p\in\calP$ the loss function
  $\ell(p) = -\log p$ and $\bbP$ has density $p^*$, then the the
  cumulant generating function of the excess loss
  $\varepsilon(p) = \ell(p) - \ell(p^*)$ has cumulant generating
  function $\psi_p(\eta)$ given by
  \begin{equation*}
    \psi_{-\varepsilon(p)}(\eta) = \log \bbP\rme^{\eta(\log(p) - \log(p^*))} = \log
    \expv{\frac{p}{p^*}}^\eta
  \end{equation*}
  which equals zero for $\eta = 1$.
\end{example}

\begin{remark}
  From the central condition it is possible to derive rates at which
  the empirical excess loss tends to zero.  The rate at which
  $\eta(\epsilon)$ decreases as $\epsilon\downarrow 0$ determines its
  rate of convergence. Consider a hypotheses class of size
  $\calH=N$. If the central condition holds we can write for any
  $\epsilon>0$ and for each $h\in\calH$
  \begin{equation*}
    \prob{E_n(h)_-\geq  2\epsilon}\leq \rme^{-n( 2\eta\epsilon  -
      \psi_{-\varepsilon(h)}(\eta))} \leq \rme^{-n\eta\epsilon}
  \end{equation*}
  with $\eta=\eta(\epsilon)$. The union bound implies that
  \begin{equation*}
    \prob{\sup_{h\in\calH}E_n(h)_-\geq  2\epsilon} \leq N\rme^{-n\eta\epsilon}
  \end{equation*}
  Thus, if $\eta(\epsilon) = O(\epsilon^{\beta})$ as
  $\epsilon\downarrow 0$ for some $\beta\in[0,1]$, we can conclude
  that for $n$ big enough with probability higher than $1-\delta$ the
  inequality
  \begin{equation*}
    |E_n(h^*_n)|\leq \paren{\frac{1}{n}\log\frac{N}{\delta}}^{\frac{1}{1+\beta}}
  \end{equation*}
  holds, that is, a $n^{-\frac{1}{1+\beta}}$ rate.  In the case that
  the strong version of the central condition holds, a similar
  analysis and a change of variable leads to the conclusion that the
  identity
  \begin{equation*}
    \prob{nE_n(h)_-\geq t} \leq \rme^{-\eta t/2}
  \end{equation*}
  holds \textit{for all} values of $t$. Corollary
  \ref{thm:special_chaining} can be used to obtain $n^{-1}$ rates of
  convergence for $E_n(h^*_n)$ in the case that the excess loss is
  Lipschitz.
\end{remark}

As we showed in the previous remark, rates for the expected loss of
ERM can be obtained from the central condition. Nevertheless the main
quantity of interest is the expected excess loss. We show in the next
subsection an additional condition that allows us to do just that.

\subsection{The Witness Condition}

The second condition is implied by Condition \ref{cond:bernstein} with
$\beta=1$ and is an uniform condition on the relative weight of the
upper tail of the excess loss. It was named \textit{Witness Condition}
by \citet{grunwald_fast_2016} and it reads:
\begin{condition}[Witness Condition]
  \label{cond:witness}
  The excess loss $\varepsilon$ on the hypotheses class $\calH$
  satisfies the witness condition if for all $h\in \calH$ it is true
  that
  \begin{equation*}
    \sup_{h\in\calH}
    \frac{\bbP[\varepsilon(h)\indicator{\varepsilon(h) > u}]}{\bbP[\varepsilon(h)]} \leq c
  \end{equation*}
  for some $u>0$ and $c\in(0,1]$.
\end{condition}
\begin{remark}
  Condition \ref{cond:witness} holds for bounded excess losses
  trivially and proving that Condition \ref{cond:bernstein} with
  $\beta = 1$ implies Condition \ref{cond:witness} (that Bernstein's
  condition implies the witness condition) is not hard. Indeed, using
  Hölder's and Markov's inequalities one obtains
  \begin{align*}
    \bbP[\varepsilon(h)]\indicator{\varepsilon(h) > u}
    &\leq \bbP[\varepsilon^2(h)]^{1/2}(\prob{\varepsilon(h) > u})^{1/2}\\
    &\leq \frac{1}{u}\bbP[\varepsilon^2(h)]\\
    &\leq \frac{B}{u}\bbP[\varepsilon(h)]
  \end{align*}
  so that it is enough to choose $u>B$.
\end{remark}

\section{Expected Excess Loss Bound \label{sect:fast_risk_bound}}

Under Conditions \ref{cond:witness} and the strong version of
Condition \ref{cond:central}, \citet[Lemma 16]{grunwald_fast_2016}
proved the following, which, chained with the inequality from Theorem
\ref{thm:zhang_inequality}, leads to potentially faster rates of
convergence as we show in Corollary \ref{cor:risk_bound}. We show how
these ideas can be applied to ERM in Example
\ref{ex:risk_bound_with_zhang}.


\begin{theorem}\label{thm:annealed_inverse_bound}
  Let $\bar{\eta}$ such the strong central condition (see Condition
  \ref{cond:central}) holds. Let $u>0$ and $c\in (0,1)$ such that the
  witness condition (Condition \ref{cond:witness}) holds. Then
  \begin{equation*}
    E(h) \leq \mu(\eta) A_\eta(h)
  \end{equation*}
  with
  \begin{equation}\label{eq:grunwald_mu}
    \mu(\eta) = \frac{1}{c}\frac{\eta u + 1}{1 - \eta/\bar{\eta}}
  \end{equation}
  for $\eta\in (0,\bar{\eta})$.
\end{theorem}

\begin{remark}
  Note that the two assumptions of this theorem are very different in
  nature. On the one hand the strong central condition implies the
  existence of all moments of $\varepsilon(h)_-$ while the witness
  condition can hold even if the second moment of $\varepsilon(h)_-$
  does not exist.
\end{remark}
It is possible to chain both the preceding theorem and Theorem
\ref{thm:zhang_inequality} to obtain an expected excess risk bound:
\begin{corollary}\label{cor:risk_bound}
  Under the same conditions as those of of Theorem
  \ref{thm:annealed_inverse_bound} it holds that with probability
  higher than $1-\delta$
\begin{equation*}
  \Pi_n[ E(h)] \leq
  \mu(\eta)\paren{\IC(\eta,n) +
    \frac{1}{\eta n}\log\frac{1}{\delta}}
\end{equation*}
where $\mu$ is as defined in \eqref{eq:grunwald_mu} and $\Pi_n$ that
is absolutely continuous with respect to $\Pi_0$.
\end{corollary}

\begin{remark}\label{rem:risk_bound_bounded_excess}
  In the case that the excess losses are bounded the witness condition
  is trivially satisfied. This means that the strong central condition
  is sufficient for obtaining fast rates. Indeed,
  \citet{van_erven_fast_2015} proved that Bernstein's and the strong
  central condition are equivalent for bounded losses.
\end{remark}

\begin{example}[A Bound for ERM Using Corollary \ref{cor:risk_bound}]
  \label{ex:risk_bound_with_zhang}
  Consider a loss function $\ell$ on a hypotheses class $\calH$
  equipped with a semi-norm $d$, a sigma algebra and a finite measure
  $\mu$. Suppose that $\ell$ is Lipschitz continuous with Lipschitz
  constant $L$, that is,
  \begin{equation*}
    |\ell(h_1) - \ell(h_2)|\leq Ld(h_1, h_2)
  \end{equation*}
  almost surely for each $h_1,h_2\in\calH$. Suppose that for each
  $\epsilon>0$ the covering number $N = N( \calH, d, \epsilon)$ (see
  Appendix \ref{sect:chaining} for the definition of covering number)
  is finite and satisfies
  \begin{equation*}
    \log N( \calH, d, \epsilon) = O(\log\epsilon^{-1})
  \end{equation*}
  as $\epsilon\downarrow 0$. Lastly, Suppose that the conditions of
  Theorem \ref{thm:annealed_inverse_bound} (and consequently those of
  Corollary \ref{cor:risk_bound}) are satisfied.

  We obtain rates for ERM by constructing an algorithm by specifying a
  prior probability $\Pi_0$ and a posterior probability $\Pi_n$. In
  some cases, the same bound without a $\log n$ factor can be obtained
  using \textit{chaining}. This technique is considered in Appendix
  \ref{sect:chaining} but we do not consider its use for obtaining fast
  rates explicitly. Choose $\Pi_0$ to be uniform over $\calH$ so that
  $\rmd\Pi_0 =\mu(\calH)^{-1}\rmd\mu$.  Choose $\Pi_n$ in to be the
  uniform distribution on a ball of radius $\epsilon$ around the ERM
  $h^*_n$. Notice that $\Pi_n$ makes predictions in the set of
  elements that are at distance smaller than $\epsilon$ to
  $\calH$. Call the $\epsilon$-ball around $h^*_{n}$ as
  $B_\epsilon(h^*_{n})$, then
  $$\rmd\Pi_n =
  \indicator{B_\epsilon(h^*_{n})}\frac{d\mu}{\mu(B_\epsilon(h^*_{n}))}.$$


  Consider a random sample $Z_1,\dots,Z_n$. Because $\ell$ is
  Lipschitz
  \begin{equation*}
    L(h^*_n) \leq \Pi_n[L(h)] + L\epsilon,
  \end{equation*}
  so that
  \begin{equation*}
    E(h^*_n) \leq \Pi_n[E(h)] + L\epsilon
  \end{equation*}
  and we can use Corollary \ref{cor:risk_bound}. In order to do so, we
  need to bound the Kullback Leibler divergence $\KL(\Pi_n,\Pi_0)$ and
  the empirical risk $\Pi_n[E_n(h)]$. First, note that
  \begin{equation*}
    \frac{\rmd \Pi_n}{\rmd \Pi_0} = \frac{\mu(\calH)}{\mu(B_\epsilon(\hat{h}^*_{n}))}\indicator{B_\epsilon(\hat{h}^*_{n})}
  \end{equation*}
  which implies that the Kulback Leibler divergence satisfies
  \begin{equation*}
    \KL(\Pi_n, \Pi_0) =
    \log\frac{\mu(\calH)}{\mu(B_\epsilon(\hat{h}^*_{n}))}\leq \log N(\calH, d, \epsilon).
  \end{equation*}
  On the other hand the empirical excess risk $\Pi_n[E_n(h)]$ also
  satisfies
  \begin{equation*}
    \Pi_nE_n(h) \leq  E_n(h^*_n) + L\epsilon \leq L\epsilon
  \end{equation*}
  because at the empirical risk minimizer $E_n(h^*_n)\leq 0$. This
  means that, using Corollary \ref{cor:risk_bound}, the excess risk
  satisfies with probability higher than $1-\delta$
  \begin{equation*}
    E(h^*_n)\leq  \mu(\eta)\paren{L\epsilon + \frac{\log N(\calH,
        d, \epsilon) +  \log\frac{1}{\delta}}{\eta n}}
  \end{equation*}
  for each $\epsilon>0$, each $\eta$ sufficiently small and $\mu$ as
  defined in \eqref{eq:grunwald_mu}. Thus, in the last equation one
  can choose $\epsilon$ to depend on $n$ as $\epsilon = 1/n$ and the
  assumption on the metric entropy leads to the conclusion that
  \begin{equation*}
    E(h^*_n) = O_{\bbP}(n^{-1}\log(n)).
  \end{equation*}
\end{example}

\chapter{Relation Among Conditions \label{sect:relations}}

This chapter contains the main contributions of this work. We start by
highlighting the panorama so far, and how our contribution fits into
it. We first describe what has been established so far in the case of
unbounded and unbounded excess losses. Initially, whenever we refer to
the possibility of obtaining a fast (or slow) rate, we refer to the
case when $\calH$ is finite, for ease of exposition. We will point at
an extension of the situation of finite classes and comment on
infinite classes on Remark \ref{rem:finite_classes}.

\begin{description}
\item[Bounded Excess Losses] The case that the excess losses are
  bounded, that is, $$\sup_{h\in\calH}|\varepsilon(h)|<\infty$$ almost
  surely has been thoroughly studied and the following has been
  established.
  \begin{enumerate}
  \item Bernstein's condition (Condition \ref{cond:bernstein} on page
    \pageref{cond:bernstein}) leads to fast rates of convergence for
    ERM (see Section \ref{sect:bernstein}).
  \item Bernstein's condition is equivalent to the strong central
    condition (Condition \ref{cond:central} on page
    \pageref{cond:central}) \citep{van_erven_fast_2015} and it leads
    to fast rates (see Remark \ref{rem:risk_bound_bounded_excess}
    after Corollary \ref{cor:risk_bound})
  \item Neither the strong central condition nor Bernstein's condition
    hold automatically.
  \item Slow rates hold without any additional condition and are
    provable via Bernstein's or Hoeffding's inequality.
  \end{enumerate}
\item[Unbounded Excess Losses] Little is known about sufficient
  conditions for fast rates in the case that excess losses might be
  unbounded but $$\sup_{h\in\calH}\bbP[\varepsilon(h)]<\infty.$$
  \begin{enumerate}
  \item \citet{audibert_fast_2009} showed a nonstandard randomized
    algorithm that achieves fast rates if Bernstein's condition
    holds. The same result is, to our best knowledge, unknown for ERM.
  \item If both the strong central condition (Condition
    \ref{cond:central} on page \pageref{cond:central}) and the witness
    condition (Condition \ref{cond:witness} on page
    \pageref{cond:witness}) hold simultaneously, fast rates are
    possible in the PAC-Bayesian setting (see Corollary
    \ref{cor:risk_bound}) and for ERM.
  \end{enumerate}
\item[Our Contribution] We make a number of contributions in the
  unbounded case, that is, when excess losses might be unbounded
  but $$\sup_{h\in\calH}\bbP[\varepsilon(h)]<\infty$$
  \begin{enumerate}
  \item We prove by way of counterexample that Bernstein's condition
    does not imply the strong central condition even if excess losses
    are forced to have exponential tails (Counterexample
    \ref{ctrex:bernstein_but_not_central}).
  \item We show an excess loss function for which both the strong
    central condition and the witness condition hold but Bernstein's
    condition does not (Counterexample
    \ref{ctrex:central_but_not_bernstein}).
  \end{enumerate}
  We considered the situation in which
  \begin{equation}\label{eq:bounded_cumulant_2}
    \sup_{h\in\calH}\psi_{-\varepsilon(h)}(\eta)<\infty
  \end{equation}
  for some $\eta>0$. This is the situation in which the probability of
  any fixed hypothesis being better than the best hypothesis is
  exponentially small, a natural weakening of the bounded case.  The
  following results are proven under this assumption.
  \begin{enumerate}
    \setcounter{enumi}{2}
  \item We find a new condition (Condition \ref{cond:new} on page
    \pageref{cond:new}) on the moments of the excess loss under
    which Bernstein's condition and the strong central condition
    become equivalent, just as in the bounded case. In
    consequence, this new condition can replace the witness
    condition in Corollary \ref{cor:risk_bound} in order to obtain
    fast rates.
  \item We note that if both Bernstein's condition and the new
    condition from the last item (Condition \ref{cond:new}) hold, then
    a condition (Condition \ref{cond:vapnik} on page
    \pageref{cond:vapnik}), under which
    \citet{vapnik_statistical_1998} proved expected loss bounds for
    ERM, holds.
  \item Using Vapnik's condition (Condition \ref{cond:vapnik}) we
    obtain fast rates by proving a tighter analogue (Theorem
    \ref{thm:tighter_bound}) of Corollary \ref{cor:risk_bound},
    improving the result of \citet{grunwald_fast_2016}, although under
    a stronger assumption.
  \item We prove that the witness condition or the right tail of the
    excess loss having bounded second moment is sufficient to obtain
    slow rates (see Subsection \ref{section:tail_bounds}).
  \end{enumerate}
\end{description}

\begin{remark}\label{rem:finite_classes}
  The case in which $\calH$ is finite also includes the situation in
  which one might allow the size of $\calH$ to depend on the number of
  data points. More formally, we have at hand a sequence of hypotheses
  classes $\calH_1,\calH_2,\dots$ and, for each sample of size $n$ we
  learn $\hat{h}_n$ from the $n$-th hypotheses class $\calH_n$, that
  is, $\hat{h}_n\in\calH_n$. If the conditions that we discuss here
  hold on $\calH = \cup_n\calH_n$ and the size of each $\calH_i$
  remains bounded by some constant $N$, then the analysis and the
  rates that we present also apply predictions made according to
  $\hat{h}_n\in\calH_n$.

  All results mentioned for fast (or slow) rates can be extended up to
  $\log n$ factors for ERM in the case where $\calH$ is infinite and
  has logarithmic metric entropy (see Definition \ref{def:metric_entropy})
  with respect to a suitable semimetric and the losses satisfy a
  Lipschitz condition (see Example
  \ref{rem:risk_bound_bounded_excess}).
\end{remark}

\section{Strong Central, Witness and Bernstein's Conditions \label{sect:central_witness_bernstein}}

We have seen that in cases when both the witness condition (Condition
\ref{cond:witness} on page \pageref{cond:witness}) and the central
condition (Condition \ref{cond:central} on page
\pageref{cond:central}) hold fast rates are possible.  As shown by
\citet{audibert_fast_2009} and \citet{bartlett_empirical_2006}, the
same occurs when Bernstein's condition (Condition \ref{cond:bernstein}
on page \pageref{cond:bernstein}) holds.  Thus the question of their
equivalence arises. In the case that the losses are totally bounded,
that is, in the case that $\bbP$-almost surely for some $M<\infty$
\begin{equation*}
  \sup_{h\in\calH}|\varepsilon(h)| < M,
\end{equation*} \citet{van_erven_fast_2015} showed that Bernstein's and
the central condition are equivalent.

In the case in which excess losses might be unbounded, these two
conditions cannot be equivalent without further restrictions. First,
Bernstein's condition cannot imply the strong central condition as the
latter implies the existence of all moments of
$\varepsilon(h)_- = \max\{-\varepsilon(h), 0\}$ and Bernstein's
condition is only a relationship between the first and the second
moments of $\varepsilon(h)$. It is then enough to consider an excess
loss for which $\var \varepsilon(h)< \infty$ but for which the third
moment of its left tail is infinite, that is,
$\bbP[(-\varepsilon(h)_-)^3]$ is infinite. In the next counterexample
we show that there even exists an excess loss for which
$\varepsilon(h)$ has finite support for each $h\in\calH$ for which
Bernstein's condition holds but the strong central condition does not.


\begin{counterexample}\label{ctrex:bernstein_but_not_central}
  There exists an excess loss $\varepsilon(h)$ such that for each
  $h\in\calH$ it has exponential and Bernstein's condition holds (and
  thus $\varepsilon(h)$ has bounded variance and mean) but the strong
  central condition does not. Indeed, let $\calH = \nats$ and let the
  excess loss $\varepsilon$ be such that
  \begin{equation*}
    \varepsilon(i)  =
    \begin{cases}
      -\log^2 i &\mbox{with probability } \frac{1}{i} \\
      1 &\mbox{with probability } 1-\frac{1}{i}.
    \end{cases}
  \end{equation*}
  Its mean is
  \begin{equation*}
    \bbP[\varepsilon(i)] = 1 - \frac{1 + \log^2 i}{i}
  \end{equation*}
  and its second moment
  \begin{equation*}
    \bbP[\varepsilon^2(i)] = 1 + \frac{\log^4 i - 1}{i}.
  \end{equation*}
  Note two things. First, that $\varepsilon(1) = 0$ and for any other
  $i\in\calH$, $\bbP[\varepsilon(i)]>0$ so that in this case
  $h^* = 0$.  Second, Bernstein's condition holds for $B = 11$ but the
  strong central condition does not hold. In order to see why, rewrite
  the definition of $\varepsilon(i)$ as
  \begin{equation*}
    \varepsilon(i)  =
    \begin{cases}
      -a_i &\mbox{with probability } \rme^{-\eta_i a_i} \\
      1 &\mbox{with probability } 1-\rme^{-\eta_i a_i}
    \end{cases}
  \end{equation*}
  with $\eta_i = 1/\log i$ and $a_i = \log^2 i$. Then, for any
  $\eta>0$
  \begin{equation*}
    \bbP[\rme^{-\eta\varepsilon(i)}]> \rme^{(\eta-\eta_i)a_i}.
  \end{equation*}
  Thus, we can chose $i$ big enough so that $\eta-\eta_i > 0$ because
  $\eta_i\to 0$ as $i\to\infty$. Consequently
  \begin{equation*}
    \bbP[\rme^{-\eta \varepsilon(i)}] > 1
  \end{equation*}
  for big enough $i$, which means that the strong central condition
  does not hold.
\end{counterexample}


On the other hand Bernstein's condition implies that both the mean and
the variance of the excess losses are bounded as shown in the
following lemma. Since there are excess loss functions without an
uniformly bounded variance or mean that satisfy the strong central
condition, it cannot imply Bernstein's condition.

\begin{lemma}\label{lem:bernstein_bounded_var_mean}
  Let $\varepsilon$ and excess loss function on a hypotheses class
  $\calH$. If $\varepsilon$ satisfies Bernstein's condition (Condition
  \ref{cond:bernstein}) then both its mean and is variance are
  uniformly bounded over $\calH$.
\end{lemma}
\begin{proof}
  This is the case for the variance since
  \begin{equation*}
    \bbP[\varepsilon(h)^2] = \var[\varepsilon(h)]  + (\bbP[\varepsilon(h)])^2 \leq B(\bbP[\varepsilon(h)])^\beta
  \end{equation*}
  it holds that
  \begin{equation}\label{eq:bernstein_variance}
    \var [\varepsilon(h)] \leq B(\bbP[\varepsilon(h)])^\beta -  (\bbP[\varepsilon(h)])^2.
  \end{equation}
  Since $\bbP[\varepsilon(h)] \geq 0$ and the function
  $x\mapsto Bx^\beta - x^2$ is bounded by its value at
  $x^* = \paren{\frac{B\beta}{2}}^\frac{1}{2-\beta}$, we can conclude
  that
  \begin{equation*}
    \var[\varepsilon(h)] \leq B\paren{\frac{B\beta}{2}}^\frac{\beta}{2-\beta} - \paren{\frac{B\beta}{2}}^\frac{2}{2-\beta}.
  \end{equation*}
  which is a uniform bound over all $h$. In turn
  \eqref{eq:bernstein_variance} implies that
  \begin{equation*}
    B(\bbP[\varepsilon(h)])^\beta -  (\bbP[\varepsilon(h)])^2 \geq 0
  \end{equation*}
  which means that
  \begin{equation*}
    \bbP[\varepsilon(h)] \leq B^\frac{1}{2-\beta},
  \end{equation*}
  again a uniform bound over $\calH$.
\end{proof}

Lemma \ref{lem:bernstein_bounded_var_mean} indicates that there are
cases in which the strong central condition holds but Bernstein's
conditions does not, because the latter implies uniformly bounded
means and variances of the excess losses. In the next counterexample
we exhibit an excess loss for which its means and variances are
uniformly bounded and still the strong central and witness condition
hold, but Bernstein's does not. In order to show this we reasoned as
follows. We first showed that the combination of witness and central
condition implies Bernstein's condition for a family of trimmed excess
losses.  We then looked for an excess loss whose trimmed version
satisfied Bernstein's condition but the untrimmed did not. Even though
the counterexample does not use the following lemma, we show it to
present the need to create an excess loss that behaves very
differently from their truncated versions.

\begin{lemma}
  Let $\varepsilon$ be an excess loss function over a hypotheses class
  $\calH$ that satisfies both the witness condition (Condition
  \ref{cond:witness}) for some $u>0$ and $c\in(0,1)$, and the strong
  central condition (Condition \ref{cond:central}). Then the trimmed
  excess loss
  $\varepsilon'(h) = \varepsilon(h)\indicator{\varepsilon(h)\leq u}$
  satisfies Bernstein's condition.
\end{lemma}
\begin{proof}
  Let $\eta$ such that for all $h\in\calH$ it happens that
  $\psi_h^-(\eta)\leq 0$. This means that
  $\bbP[\rme^{-\eta\varepsilon(h)}]\leq 1$. Fix $h\in\calH$. By Taylor's
  theorem there exists $\eta^*\in(0,\eta)$ depending such that
  \begin{equation*}
    \bbP[\rme^{-\eta\varepsilon(h)}] = 1 - \eta\bbP[\varepsilon(h)] +
    \frac{\eta^ 2}{2}\bbP[\rme^{-\eta^*\varepsilon(h)}\varepsilon^2(h)],
  \end{equation*}
  which in addition to the central condition means that
  \begin{equation*}
    \frac{\eta}{2}\bbP[\rme^{-\eta^*\varepsilon(h)}\varepsilon^2(h)]
    \leq \bbP[\varepsilon(h)]
  \end{equation*}
  By the witness condition the expected value of $\varepsilon(h)$ can
  be bounded by that of its trimmed version $\varepsilon'(h)$ so that
  \begin{equation*}
    \bbP[\rme^{-\eta^*\varepsilon(h)}\varepsilon^2(h)] \leq C\bbP[\varepsilon'(h)]
  \end{equation*}
  for some constant $C$ independent of $h$. Thus it is enough to prove
  that $\bbP\varepsilon'^ 2(h)$ can be bounded by the term in the left
  hand of the previous inequality. Write
  \begin{align*}
    \bbP[\rme^{-\eta^*\varepsilon(h)}\varepsilon^2(h) ]
    &=
      \bbP[\rme^{-\eta^*\varepsilon'(h)}\varepsilon'^2(h)] +
      \bbP[\rme^{-\eta^*\varepsilon(h)}\varepsilon^2(h)\indicator{\varepsilon(h)]\geq
      u}\\
    &\geq \bbP[\rme^{-\eta_h\varepsilon'(h)}\varepsilon'^2(h)]\\
    &\geq \rme^{-\eta u}\bbP[\varepsilon'^2(h)].
  \end{align*}
  Conclude that
  \begin{equation*}
    \bbP[\varepsilon'^2(h)] \leq B \bbP[\varepsilon'(h)]
  \end{equation*}
  for some constant $B$ independent of $h$.
\end{proof}

\begin{counterexample}\label{ctrex:central_but_not_bernstein}
  There exists an excess loss function $\varepsilon(h)$ with uniformly
  bounded mean and variance that satisfies both the witness condition
  for some $u>0$ and $c\in(0,1)$, and the central condition for which
  Bernstein's condition does not hold.

  Indeed, let $\calH=\nats$ and let the the excess loss
  $\varepsilon(i)$ be such that
  \begin{equation*}
    \varepsilon(i) =
    \begin{cases}
      i-1 &\mbox{with probability } \frac{1}{i^2} \\
      \frac{1}{i} &\mbox{with probability } 1-\frac{1}{i^2}.
    \end{cases}
  \end{equation*}
  Note that $\varepsilon(1)=0$ so that in this case $h^* = 1$ and that
  since the excess losses are non-negative, the strong Central
  Condition is satisfied. It is clear that $\varepsilon$ has uniformly
  bounded mean and variance. For this choice, the excess loss
  $\varepsilon(i)$ satisfies the witness condition for $u=1$ and
  $c=1/2$. This is because the excess loss $\varepsilon(i)$ and its
  trimmed version
  $\varepsilon'(h) = \varepsilon(h)\indicator{\varepsilon(h)\leq u}$
  satisfy
  \begin{align}
    \bbP[\varepsilon(i)] &= \frac{2}{i} - \frac{1}{i^2} - \frac{1}{i^3}\\
    \bbP[\varepsilon'(i)] &=  \frac{1}{i} - \frac{1}{i^3}
  \end{align}
  so that for $i\geq 2$
  \begin{equation*}
    \frac{\bbP[\varepsilon(i)]}{\bbP[\varepsilon'(i)]} = \frac{2i^2 - i -
      1}{i^2 - 1} \leq 2,
  \end{equation*}
  which means that
  \begin{equation*}
    c\bbP[\varepsilon(i)] \leq \bbP[\varepsilon'(i)].
  \end{equation*}
  The second moment of the excess loss and its trimmed version satisfy
  \begin{align}
    \bbP[\varepsilon^2(i)] &= 1 -\frac{2}{i} + \frac{2}{i^2} - \frac{1}{i^4}\\
    \bbP[\varepsilon'^2(i)] &=  \frac{1}{i^2} - \frac{1}{i^4}
  \end{align}
  so that on the one hand
  \begin{equation*}
    \frac{\bbP[\varepsilon'^2(i)]}{\bbP[\varepsilon'(i)]} = \frac{i^2 -
      1}{i^3 - i}\leq 1
  \end{equation*}
  which implies that the Bernstein condition holds for the trimmed
  excess loss with $B =1$. On the other hand for the excess loss
  \begin{equation*}
    \frac{\bbP[\varepsilon^2(i)]}{\bbP[\varepsilon(i)]}\asymp \frac{i}{2}
  \end{equation*}
  as $i\to\infty$, that is, Bernstein's condition does not hold.
\end{counterexample}

\section{A New Condition and Vapnik's Condition \label{sect:new_vapnik}}

In this section we show how a moment condition on the excess losses
makes Bernstein's condition and the strong Central Condition
equivalent. It was used by \citet[see][Lemma
6.1]{mendelson_learning_2014} as a sufficient condition to obtain
rates of convergence without using concentration inequalities. The
condition reads:
\begin{condition}\label{cond:new}
  $\varepsilon(h)$ is an excess loss function on a hypotheses class
  $\calH\ni h$ such that for some $r > 1$ it holds that
  \begin{equation*}
    \sup_{h\in\calH}\frac{(\bbP[|\varepsilon(h)|^{2r}])^{1/r}}{\bbP[\varepsilon^2(h)]}
    < \infty.
  \end{equation*}
  and
  \begin{equation*}
    \sup_{h\in\calH}  \bbP[\varepsilon^2(h)] <\infty
  \end{equation*}
\end{condition}
\begin{remark}
  Note that this condition implies that
  $\sup_{h}\bbP[\varepsilon(h)]<\infty$.
\end{remark}
The following example is adapted in part from
\citet[p. 208]{vapnik_statistical_1998} and it shows how this
condition holds in many practical settings.
\begin{example}\label{ex:new_cond_example}
  % Consider the regression problem. We seek to choose a function from a
  % set $\calF$ of functions $f:\calX\to\calY$ so as to minimize the
  % loss function $\ell(f, x, y)$ given by
  % \begin{equation*}
  %   \ell(f,x,y) = (y - f(x))^2
  % \end{equation*}
  Consider again the simple normal means model from Example
  \ref{ex:normal_means} where $Z\sim \calN(\mu^*, 1)$ and
  $\ell(\mu,Z) =\frac{1}{2}(Z-\mu)^2$. A quick calculation shows that
  \begin{equation*}
    \varepsilon(\mu,Z) = \frac{\mu^2 - \mu^{*2} }{2} + (\mu^*-\mu)Z
  \end{equation*}
  so that under $\bbP$ the excess loss $\varepsilon(h)$ has a normal
  distribution. There are also other common situations in which the
  excess loss is normally distributed, such as minimum squares
  regression with normal residuals. Thus, consider in general normally
  distributed excess losses $\varepsilon(h)$ with mean $\mu_h$ and
  variance $\sigma^2_h$ which are allowed to vary with $h$. Since for
  the normal distribution it holds that\footnote{$!!$ makes reference
    to the double factorial which is defined as
    $ n!! = n(n-2)(n-4)\dots 2$}
  $\bbP[(\varepsilon(h)-\mu_h)^n] = \sigma^n(n-1)!!$ for even $n$,
  then, defining $m_4 =\bbP[(\varepsilon(h)-\mu_h)^4]$ we obtain that
  \begin{equation*}
    \frac{m_4}{\sigma_h^2} = 3 .
  \end{equation*}
  With this in mind,
  \begin{align*}
    \frac{(\bbP[\varepsilon^4(h)])^{1/2}}{\bbP[\varepsilon^2(h)]}
    &=
      \frac{(\bbP[((\varepsilon(h) - \mu_h) +
      \mu_h)^4])^{1/2}}{\bbP[((\varepsilon(h) - \mu_h) + \mu_h)^2]} \\
    &=
      \frac{\sqrt{m_4 + 6\mu^2\sigma^2 + 3\sigma^4}}{ \sigma^2 + \mu^2}\\
    &=
      \frac{\sqrt{3\sigma^2 + 6\mu^2\sigma^2 + 3\sigma^4}}{\sigma^2 +
      \mu^2}\\
    &\leq \sqrt{3}.
\end{align*}
This means that if the excess losses are Gaussian under $\bbP$, then
the Condition \ref{cond:new} holds for $r=2$ and $\tau = \sqrt{3}$.
The same analysis holds for this choice of $\ell$ and for
distributions of $Z$ (and consequently of $\varepsilon$) that are
symmetric scale-location families. In that case, we recognize $\tau$
to be the square root of the kurtosis. This includes Laplacian,
uniform, logistic and many other distributions.
\end{example}

In the next Theorem we show how condition \ref{cond:new} makes the
strong central condition and Bernstein's condition equivalent.

\begin{theorem}\label{thm:bernstein_central_equivalence}
  Let $\varepsilon$ be an excess loss on a hypotheses class such that
  \eqref{eq:bounded_cumulant_2} holds for some $\eta>0$ and Condition
  \ref{cond:new} holds. Then Bernstein's condition and the strong
  central condition are equivalent.
\end{theorem}
\begin{proof}
  This proof will rest on the fact for each $\eta$ and a fixed
  $h\in\calH$
  where $$\sup_{h\in\calH}\psi_{-\varepsilon(h)}(\eta)<\infty$$ there
  exists an $\eta^*\in(0,1)$ (depending on $h$) such that
  \begin{equation}\label{eq:mgf_expansion}
    \bbP\rme^{-\eta \varepsilon(h)} = 1 - \eta\bbP \varepsilon(h) +
    \frac{1}{2}\eta^2\bbP\rme^{-\eta^*\varepsilon(h)}\varepsilon(h)^2.
  \end{equation}

  Fix $h\in\calH$ and suppose that Bernstein's Condition Holds. Define
  $r^* = (1-1/r)^{-1}$ and choose $\eta$ such that
  \begin{equation*}
    \sup_{h\in\calH}\psi(2r^*\eta)<\infty
  \end{equation*}
  By  \eqref{eq:mgf_expansion} it is enough to prove that
  for each $h$ one has
    \begin{equation*}
   \frac{1}{2}\eta^2\bbP[\rme^{-\eta^*\varepsilon(h)}\varepsilon(h)^2] \leq \eta\bbP [\varepsilon(h)].
  \end{equation*}
  By Hölder's Inequality and our assumptions
  \begin{equation*}
    \bbP[\rme^{-\eta^*\varepsilon(h)}\varepsilon(h)^2] \leq
    \bbP[\rme^{-r^*\eta^*\varepsilon(h)}]^{1/r^*}\bbP[|\varepsilon(h)|^{2r}]^{1/r}
    \leq C\bbP[\varepsilon(h)^2].
  \end{equation*}
  for some constant $C>0$ independent from $h$. The results follows
  from Bernstein's condition and taking $\eta$ sufficiently small.

  Fix $h\in\calH$ and suppose that the central condition holds for
  some $\eta>0$. Then by Equation \eqref{eq:mgf_expansion} for each
  $h$ there exists a $\eta^*$ such that
  \begin{equation*}
    \frac{1}{2}\eta^2\bbP[\rme^{-\eta^*\varepsilon(h)}\varepsilon(h)^2] \leq \eta\bbP[ \varepsilon(h)].
  \end{equation*}
  Let $K>0$. Note that
  \begin{align*}
    \expv{\rme^{-\eta^* \varepsilon(h)}\varepsilon(h)^2} &=
    \expv{\rme^{-\eta^* \varepsilon(h)}\varepsilon(h)^2\indicator{\varepsilon(h)>K}} +
    \expv{\rme^{-\eta^*
    \varepsilon(h)}\varepsilon(h)^2\indicator{\varepsilon(h)\leq K}}\\
    &\geq
     \rme^{-\eta K}\expv{\varepsilon(h)^2\indicator{\varepsilon(h)\leq K}}
  \end{align*}
  so it is enough to provide a constant $c\in(0,1)$ such that
  \begin{equation*}
    \expv{\varepsilon(h)^2\indicator{\varepsilon(h)\geq K}}\leq c\expv{\varepsilon(h)^2}.
  \end{equation*}
  We focus on proving this last relation. Use Hölder's inequality to
  obtain that
  \begin{equation*}
    \expv{\varepsilon(h)^2\indicator{\varepsilon(h)\geq K}}
    \leq
    (\prob{\varepsilon(h)\geq K})^{1/r*}\expv{|\varepsilon(h)|^{2r}}^{1/r}
  \end{equation*}
  We can use Markov's inequality to obtain that
  \begin{equation*}
    \prob{\varepsilon(h)\geq K}
    \leq
    \prob{|\varepsilon(h)|^{2r}\geq K^{2r}} \leq \frac{\bbP[|\varepsilon(h)|^{2r}]}{K^{2r}}
  \end{equation*}
  and that consequently
  \begin{equation*}
    \expv{\varepsilon(h)^2\indicator{\varepsilon(h)\geq K}}
    \leq
    \frac{\expv{|\varepsilon(h)|^{2r}}}{K^{2r/r^*}}
      \leq
      \paren{\frac{\expv{\varepsilon(h)^{2}}}{K^{2/r^*}} }^r
  \end{equation*}
  because $1/r + 1/r^* = 1$ and the assumption. Choose $K$ so big so
  that
  \begin{equation*}
    K > \max\bracks{\sup_{h\in\calH}\expv{\varepsilon(h)^{2}}^{r^*/2} , 1}.
  \end{equation*}
  and define $c = 1/K^{r^*/2}$. Together with the fact that
  $x^r\leq x$ for $x\in[0,1]$, this choice of $c$ leads to
  \begin{equation*}
    \expv{\varepsilon(h)^2\indicator{\varepsilon(h)\geq K}}
    \leq c\expv{\varepsilon(h)^{2}},
  \end{equation*}
  as it was to be shown.
\end{proof}

A close inspection of the last proof, shows that if Condition
\ref{cond:new} and Bernstein's condition (Condition
\ref{cond:bernstein}) hold, then the following condition holds
\begin{condition}\label{cond:vapnik}
  An excess loss function $\varepsilon(h)$ on a hypotheses class
  $\calH$ satisfies
  \begin{equation*}
    \sup_{h\in\calH}\frac{(\bbP[|\varepsilon(h)|^{2r}])^{1/2r}}{\bbP[\varepsilon(h)]}\leq
    \tau
  \end{equation*}
  for some $r>1$ and
  \begin{equation*}
    \sup_{h\in\calH}\expv{\varepsilon(h)}<\infty.
  \end{equation*}
\end{condition}

\citet[Section 5.7]{vapnik_statistical_1998} called the distributions
for which this condition holds \textit{light tailed} since the random
variables that satisfy it tend to have less mass on the
tails. Furthermore, \citet{vapnik_statistical_1998} found that this
condition implied bounds for the expected loss when the loss function
is required to satisfy it.

% \begin{remark}
%   We can interpret this condition using Paley–Zygmund inequality,
%   which can be easily derived. First, write for $\theta\in [0,1]$ and
%   $r^* = (1-1/2r)^{-1}$ and use Holder's inequality to obtain that
%   \begin{align*}
%     \bbP[\varepsilon(h)]
%     &=
%       \bbP[\varepsilon(h)\indicator{\varepsilon(h)\leq
%       \theta\bbP[\varepsilon(h)]}] +
%       \bbP[\varepsilon(h)\indicator{\varepsilon(h) >
%       \theta\bbP[\varepsilon(h)]}]\\
%     &\leq \theta\bbP[\varepsilon(h)] +
%       (\bbP[\varepsilon^{2r}(h)])^{1/2r}(\prob{\varepsilon(h) >
%       \theta\bbP[\varepsilon(h)]})^{1/r^*}
%   \end{align*}
%   so after rearrangement we obtain the inequality in question
%   \begin{equation*}
%     \prob{\varepsilon(h) >
%       \theta\bbP[\varepsilon(h)]}) \geq \paren{(1-\theta)\frac{\bbP[\varepsilon(h)]}{(\bbP[\varepsilon^{2r}(h)])^{1/2r}}}^{r^*}.
%   \end{equation*}
%   This means that Vapnik's condition implies that for all $h\in\calH$
%   and all $\theta\in [0,1]$
%   \begin{equation*}
%     \prob{\varepsilon(h) >
%       \theta\bbP[\varepsilon(h)]}
%     \geq \paren{\frac{1-\theta}{\tau}}^{r^*}
%   \end{equation*}
%   and in particular for $\theta=0$ one obtains that
%   \begin{equation*}
%     \prob{\ell(h) > \ell(h^*)}
%     \geq \paren{\frac{1}{\tau}}^{r^*}.
%   \end{equation*}
%   Then Vapnik's condition implies that the hypotheses in $\calH$ have
%   to be worse than $h^*$ with a probability bigger than a number
%   controlled by the parameter $\tau$. At an inuitive level, this is
%   due to the fact that if the losses of the hypotheses are not bigger
%   than those of $h^*$ with nonvanishing probability, it will be hard
%   to empirically find a good hypothesis.
% \end{remark}


We now prove that Condition \ref{cond:vapnik} is enough to obtain an
inequality of the form of Theorem \ref{thm:annealed_inverse_bound},
that is, of the form
\begin{equation*}
  A_{\eta}(h)\lesssim E(h).
\end{equation*}
Recall that $A_\eta(h)$ is the annealed expectation defined in
\eqref{eq:annealed_definition}. This means that, just as in Corollary
\ref{cor:risk_bound}, one can use the inequality from Theorem
\ref{thm:zhang_inequality} to obtain expected excess loss bounds, but
with a better leading constant factor. We further show that it implies
the witness condition (Condition \ref{cond:witness}), the Strong
central condition (Condition \ref{cond:central}) and Bernstein's
condition (Condition \ref{cond:bernstein}).

\begin{theorem}\label{thm:tighter_bound}
  Let $\varepsilon$ be an excess loss function on a hypotheses class
  $\calH$ such that Condition \ref{cond:vapnik} holds for some finite
  non zero $\tau$ and $r > 1$. Assume also that
  \eqref{eq:bounded_cumulant_2} holds for some $\eta>0$. Then there
  exists a positive constant $C$ such that if
  \begin{equation*}
    \bar{\eta} = \frac{1}{C\sup_{h\in\calH}E(h)}
  \end{equation*}
  and
  \begin{equation*}
    \mu(\eta) = \frac{1}{1-\frac{\eta}{\bar{\eta}}}
  \end{equation*}
  then
  \begin{equation*}
    E(h) \leq \mu(\eta)A_{\eta}(h).
  \end{equation*}
\end{theorem}
\begin{proof}
  Consider a fixed $h\in\calH$. We know that for each $\eta>0$ where
  $\sup_{h}\psi_{-\varepsilon(h)}$ it holds that
  \begin{equation*}
    \log\bbP[\rme^{-\eta\varepsilon(h)}] \leq -\eta
    \bbP[\varepsilon(h)] + \frac{1}{2}\eta^2\bbP[\rme^{-\eta^*\varepsilon(h)}\varepsilon^2(h)]
  \end{equation*}
  for some $\eta^*\in(0,\eta)$. Define $r^* = (1-1/r)^{-1}$. By
  Hölder's inequality, the assumption
  \begin{align*}
    \bbP[\rme^{-\eta^*\varepsilon(h)}\varepsilon^2(h)]
    &\leq
      \paren{\bbP[\rme^{-r^*\eta^*\varepsilon(h)}]}^{1/r^*} \paren{\bbP[\varepsilon^{2r}(h)]}^{1/r}\\
    &\leq C \paren{\bbP[\varepsilon(h)]}^{2}
  \end{align*}
  for some finite constant $C$ independent of $h$. Bringing together
  these two inequalities we obtain that
  \begin{align}
    \log\bbP[\rme^{-\eta\varepsilon(h)}]
    &\leq -\eta\bbP[\varepsilon(h)]
      + C\eta^2\bbP[\varepsilon(h)]^2\nonumber\\
    & = \eta\bbP[\varepsilon(h)](C\eta\bbP[\varepsilon(h)] - 1)\nonumber\\
    & \leq \eta\bbP[\varepsilon(h)](C\eta\sup_{h\in\calH}\bbP[\varepsilon(h)] - 1)\label{eq:vapnik_then_central}
  \end{align}
  Define $\bar{\eta}$ as
  \begin{equation*}
    \bar{\eta} = \frac{1}{C\sup_{h\in\calH}\bbP[\varepsilon(h)]}
  \end{equation*}
  and take $\eta<\bar{\eta}$. Rearrange to obtain that
  \begin{equation*}
    E(h)\leq \mu(\eta)A_{\eta}(h)
  \end{equation*}
  with
  \begin{equation*}
    \mu(\eta) = \frac{1}{1-\frac{\eta}{\bar{\eta}}}.
  \end{equation*}
\end{proof}

\begin{remark}
  Our claim that the bound in Theorem \ref{thm:tighter_bound} is
  tighter than that of \citet{grunwald_fast_2016} (see Theorem
  \ref{thm:annealed_inverse_bound}) is based on the fact the function
  $\mu$ is in this case smaller than that of Theorem
  \ref{thm:annealed_inverse_bound} and that as $\eta\downarrow 0$ the
  inequality becomes an equality because the leading constant is
  exactly $1$. In the same fashion as in Corollary
  \ref{cor:risk_bound}, we can obtain an excess risk bound chaining
  \citeauthor{zhang_information-theoretic_2006}'s inequality and the
  previous.
\end{remark}

\begin{corollary}\label{cor:tighter_excess_bound}
  Under the conditions of Theorem \ref{thm:tighter_bound} it holds for
  every prior distribution $\Pi_0$ and posterior distribution $\Pi_n$
  that with probability higher than $1-\delta$
  \begin{equation*}
    \Pi_n[E(h)]\leq \mu(\eta)\paren{\IC(\eta, n) +
      \frac{\log\frac{1}{\delta}}{\eta n}}
  \end{equation*}
  is satisfied for $\eta<\bar{\eta}$ where $\bar{\eta}$ and $\mu$ are
  as defined in Theorem \ref{thm:tighter_bound} and $\IC(\eta, n)$ is
  as defined in \eqref{eq:def_ic}.
\end{corollary}

We now present an example to show that this is indeed the case in
situations in which fast rates are possible, as in Example
\ref{ex:new_cond_example}.
\begin{example}
  As we saw in Example \ref{ex:new_cond_example}, there are many
  distributions with well behaved tails and hypotheses classes that
  satisfy Condition \ref{cond:new}. In the case that those
  distributions either satisfy the strong central or Bernstein's
  condition, and thus are in the fast rate regime, then they satisfy
  Condition \ref{cond:vapnik}. For instance, if $\varepsilon(h)$ is
  distributed according to a normal distribution with mean $\mu_h$ and
  variance $\sigma^2_h$ possible dependent on $h$, then
  \begin{equation*}
    \psi_{-\varepsilon(h)}(\eta) = -\mu_h\eta + \frac{1}{2}\sigma_h^2\eta^2
  \end{equation*}
  so that the central condition holds if and only if there is some
  $\eta$ such that for all $h$
  \begin{equation*}
    \sigma_h^2 \leq 2\eta\mu_h,
  \end{equation*}
  which can be seen to be equivalent to Bernstein's condition. In the
  case that this is satisfied, Condition \ref{cond:vapnik} holds and
  thus the tighter bound from Corollary \ref{cor:tighter_excess_bound}
  holds. This means that this result can be used in many practical
  applications.
\end{example}

Additionally, Condition \ref{cond:vapnik} implies the witness
condition, the strong central condition and Bernstein's condition.

\begin{lemma}\label{lem:vapnik_then_central}
  Condition \ref{cond:vapnik} implies the strong central condition
  (Condition \ref{cond:central}) if \eqref{eq:bounded_cumulant_2}
  holds for some $\eta>0$.
\end{lemma}
\begin{proof}
  A careful reading of the proof of Theorem \ref{thm:tighter_bound}
  shows that from Equation \eqref{eq:vapnik_then_central} it follows
  that the strong central condition holds for $\eta<\bar{\eta}$, where
  $\bar{\eta}$ is as defined in the same theorem.
\end{proof}

\begin{lemma}\label{lem:vapnik_then_witness}
  Condition \ref{cond:vapnik} implies the witness condition (Condition
  \ref{cond:witness})
\end{lemma}
\begin{proof}
  Suppose that $\varepsilon$ satisfies Condition
  \ref{cond:vapnik}. Let $u>0$ and define $r^* = (1-1/2r)^{-1}$. Use
  Hölder's and Markov's inequality to obtain that
  \begin{align*}
    \expv{\varepsilon(h)\indicator{\varepsilon(h) > u}}
    &\leq
     \expv{\varepsilon(h)^{2r}}^{1/2r}\prob{\varepsilon(h) >
      u}^{1/r^*} \\
    &\leq \frac{\expv{\varepsilon(h)^{2r}}}{u^{2r/r^*}}\\
    &\leq \tau \frac{\expv{\varepsilon(h)}^{2r}}{u^{2r/r^*}}.
  \end{align*}
  Choosing $u$ sufficiently large, specifically
  \begin{equation*}
    u > \max\bracks{\tau^{r^*/2r}\sup_{h\in\calH}\expv{\varepsilon(h)}^{r^*}, 1}
  \end{equation*}
  implies that
  \begin{equation*}
    \expv{\varepsilon(h)\indicator{\varepsilon(h) > u}}  \leq c \expv{\varepsilon(h)}
  \end{equation*}
  with $c\in(0,1)$ because $x^{2r}\leq x$ for $x\in[0,1]$. Thus, the
  witness condition holds.
\end{proof}

\begin{lemma}\label{lem:vapnik_then_bernstein}
  Condition \ref{cond:vapnik} implies Bernstein's condition (Condition
  \ref{cond:bernstein}).
\end{lemma}
\begin{proof}
  From Hölder's inequality and Condition \ref{cond:vapnik} we obtain
  that
  \begin{equation*}
    \expv{\varepsilon^2(h)}\leq \expv{\varepsilon^{2r}(h)}^{1/r} \leq
    \tau \expv{\varepsilon(h)}^2\leq \tau
    \sup_{h'}\expv{\varepsilon(h')} \expv{\varepsilon(h)},
  \end{equation*}
  that is, Bernstein's condition holds with
  $B = \tau \sup_{h}\expv{\varepsilon(h)}$.
\end{proof}


% \begin{lemma}
%   If an excess loss function on a hypotheses space $\calH$ satisfies
%   the the strong central condition (Condition \ref{cond:central}) and
%   $\sup_{h\in\calH}\bbP[\varepsilon^{2r}(h)]<\infty$ for some
%   $r>1$ then Condition \ref{cond:vapnik} holds.
% \end{lemma}
% \begin{proof}
%   Let $\eta$ such that the strong central condition holds. Then, by a
%   Taylor expansion it holds that there is a $\eta^*\in(0,\eta)$ such
%   that
%   \begin{equation*}
%     \frac{1}{2}\eta^2\bbP[\rme^{-\eta^*\varepsilon(h)}\varepsilon^2(h)]
%     \leq
%     \eta\bbP[\varepsilon(h)]
%   \end{equation*}
%   for each $K>0$ it holds that
%   \begin{align*}
%     \bbP[\rme^{-\eta^*\varepsilon(h)}\varepsilon^2(h)]
%     &\geq
%       \bbP[\rme^{-\eta^*\varepsilon(h)}\varepsilon^2(h)\indicator{\varepsilon(h)\leq
%       K}]\\
%     &\geq \rme^{-\eta
%       K}\bbP[\varepsilon^2(h)\indicator{\varepsilon(h)\leq K}]\\
%   \end{align*}
%   and thus it is enough to find a constant $c\in(0,1)$ such that
%   \begin{equation*}
%     \bbP[\varepsilon^2(h)\indicator{\varepsilon(h)\geq K}]
%     \leq
%     c(\bbP[\varepsilon^{2r}(h)])^{1/2r}.
%   \end{equation*}
%   In order to do so, use Hölder's and Markov's inequalities to obtain
%   that for $r^* = (1-1/r)^{-1}$
%   \begin{align*}
%     \bbP[\varepsilon^2(h)\indicator{\varepsilon(h)\geq K}]
%     &\leq
%     (\bbP[\varepsilon^{2r}(h)])^{1/r}(\prob{\varepsilon(h)\geq
%       K})^{1/r^*}\\
%     &\leq
%       \frac{\bbP[\varepsilon^{2r}(h)]}{u^{2r/r^*}}
%   \end{align*}
%   so that picking
%   \begin{equation*}
%     u>\max\bracks{\sup_{h\in\calH}(\bbP[\varepsilon^{2r}(h)])^{r^*/2r}, 1}
%   \end{equation*}
%   one obtains that
%   \begin{equation*}
%     \frac{\bbP[\varepsilon^{2r}(h)]}{u^{2r/r^*}} \leq
%     \frac{\bbP[\varepsilon^{2r}(h)]^{1/2r}}{u^{1/r^*}} = c\bbP[\varepsilon^{2r}(h)]^{1/2r}
%   \end{equation*}
%   with $c = u^{-1/r^*}\in(0,1)$ because $x\leq x^{1/2r}$ for $x\in(0,1)$.
% \end{proof}


\section{Slow Rates from the Witness
  Condition \label{section:tail_bounds}}

In this section we prove risk bounds for excess losses that satisfy
either the witness condition (Condition \ref{cond:witness} on page
\pageref{cond:witness}) or an alternative second moment condition
under the assumption that \eqref{eq:bounded_cumulant_2} holds for some
$\eta>0$. We do this by using Bernstein's moment condition (see
Theorem \ref{thm:bernstein_inequality}) in order to prove that uniform
subgamma bounds hold for certain random variables related to
$E(h) - \varepsilon(h)$.  Recall that we say that a random variable
has a subgamma right tail if for some constants $v,c>0$ it holds that
\begin{equation*}
  \psi_X(\eta)\leq \frac{1}{2}\frac{v\eta^2}{1-c\eta}
\end{equation*}
for $\eta\in(0,1/c)$. Recall also that we are interested in the
situation in which subgamma bounds hold uniformly over $\calH$, that
is, if $\{X_h\}_{h\in\calH}$, then
\begin{equation*}
  \sup_{h\in\calH} \psi_{X_h}(\eta)\leq \frac{1}{2}\frac{v\eta^2}{1-c\eta},
\end{equation*}
in which case we say that $X_h$ has a uniformly subgamma right tail.

Recall also that using Corollary \ref{cor:risk_subgamma}, one can use
bounds on the cumulant generating functions to obtain of order
$n^{-1/2}$. With this in mind the result reads:

\begin{theorem}\label{theo:cfg_bernstein_bound}
  Let $\varepsilon$ an excess loss over a hypotheses class $\calH$
  such that $$\sup_{h\in\calH}\psi_{-\varepsilon(h)}(\eta)<\infty$$
  for some $\eta>0$.
  \begin{enumerate}
  \item \label{itm:witness_condition} If the witness condition holds,
    that is, there exist $u>0$ and $c'\in[0,1]$ so that for every $h$
    \begin{equation*}
      c'\bbP[ \varepsilon(h)] \leq \bbP[ \varepsilon(h)\indicator{\varepsilon(h)\leq u}]
    \end{equation*}
    then $c'E(h)-\varepsilon(h)$ has a uniformly subgamma right tail.
  \item \label{itm:finite_moment_condition} If the second moment of
    the positive part
    $\varepsilon_+(h) = \max\bracks{0,\varepsilon(h)}$ of the excess
    loss is uniformly bounded, that is, if
    \begin{equation*}
      \sup_{h}\bbP[ \varepsilon(h)_+^2] <\infty.
    \end{equation*}
    Then $E(h)-\varepsilon(h)$ has a uniformly subgamma right tail.
  \end{enumerate}
\end{theorem}

\begin{proof}
  Suppose that \ref{itm:witness_condition} holds. Let $\varepsilon'$
  be the trimmed excess loss
  $\varepsilon'(h) = \varepsilon(h)\indicator{\varepsilon(h)\leq
    u}$.
  Let $\eta>0$ be so that $\sup_{h}\psi_{E(h)}(\eta)<\infty$. Note
  that for each $h$
  \begin{equation*}
    \bbP[\rme^{-\eta \varepsilon(h)}]\leq \bbP[\rme^{-\eta \varepsilon'(h)}]
  \end{equation*}
  so that
  $\psi_{-\varepsilon(h)}(\eta)\leq
  \psi_{-\varepsilon'(h)}(\eta)$. At the same time
  \begin{equation*}
    \bbP[\rme^{-\eta \varepsilon'(h)}] = \bbP[\rme^{-\eta \varepsilon'(h)}(\indicator{\varepsilon(h)\leq u} + \indicator{\varepsilon(h) > u})] \leq 1 + \bbP[\rme^{-\eta \varepsilon(h)}].
  \end{equation*}
  These two facts mean that a bound on $\psi_{\varepsilon'(h)}(\eta)$
  implies a bound on $\psi_{\varepsilon(h)}(\eta)$ and that the
  $\varepsilon'(h)$ also have bounded cumulant generating function at
  $\eta$ for each $h\in\calH$. Consequently assume that there is a
  finite $M$ such that
  \begin{equation*}
    \sup_{h\in \calH}\bbP[\rme^{-\eta\varepsilon'(h)}]\leq M
  \end{equation*}
  Since the $\varepsilon'(h)$ are bounded by $u$ from the right,
  $\bbP \varepsilon'_{+}(h)^2 \leq u^2$ and one can write
  \begin{align*}
    \bbP[\varepsilon'(h)^n_{-}]
    &=  \bbP[\varepsilon(h)_{-}^n]\\
    &=
      \bbP\int_0^{\varepsilon_{-}(h)}n t^{n-1} \rmd t \\
    &=
    \int_0^\infty\prob{\varepsilon_-(h)>t} n t^{n-1}\rmd t\\
    &\leq
    M n\int_0^\infty \rme^{-\eta t} t^{n-1}\rmd t\\
    & = M\frac{1}{\eta^n}n\Gamma(n)\\
    & = M\frac{1}{\eta^n}n!
  \end{align*}
  For each $n\geq2$. Thus
  \begin{equation}\label{eq:second_moment_bound}
    \bbP [\varepsilon'(h)^2] = \bbP[ \varepsilon'(h)_-^2 ]+ \bbP[ \varepsilon'(h)_+]^2 \leq  M\frac{2}{\eta^2} + u^2.
  \end{equation}
  Consequently define $v = M\frac{2}{\eta^2} + u^2$ and
  $c' = \frac{1}{\eta}$ and to obtain that
  \begin{equation}\label{eq:bernstein_moment_condition}
    \bbP \varepsilon'_-(h)^n\leq \frac{1}{2}n!vc'^{n-2},
  \end{equation}
  that is, $\varepsilon'$ satisfies Bernstein's moment condition and,
  by Theorem \ref{thm:bernstein_inequality} the result follows.

  In the second case, the analogue of Equation
  \eqref{eq:bernstein_moment_condition} can be obtained for
  $\varepsilon(h)$ directly by choosing
  $v = M\frac{2}{\eta^2} + \sup_{h}\bbP[ \varepsilon(h)_+^2]$ and
  $c = \frac{1}{\eta}$, so the result also follows in the same manner.
\end{proof}

Consequently, the result from Theorem \ref{theo:cfg_bernstein_bound}
can be used directly in conjunction with Corollary
\ref{cor:risk_subgamma} to obtain slow rates.

\begin{remark}
  The two conditions for the previous theorem are not not comparable,
  that is, one does not imply the other. Take $\calH = \{h\}$ a
  hypotheses class with one element. Choose $\varepsilon(h)$ to have a
  Gaussian distribution on $\reals$ with mean zero and unit
  variance. Then for every $u>0$ it happens that
  $\bbP E\indicator{E\leq u}< 0$. This means that the witness
  condition does not hold but $\varepsilon(h)$ has bounded second
  moment. On the other hand take again a class of hypotheses $\calH$
  with a single element $h$ and let $\varepsilon(h)$ distributed on
  the positive integers with distribution function so that
  $\prob{E = n}\propto n^{-3}$. It follows that $E$ has no second
  moment but $\bbP E\indicator{E\geq u}>0$ for each $u>0$, that is, it
  satisfies the witness condition.
\end{remark}

\begin{remark}
  We proved the last theorem using Bernstein's moment condition. This
  means under both the assumptions of Theorem
  \ref{theo:cfg_bernstein_bound} that
  \begin{equation*}
    \sup_{h\in\calH}\psi_{-\varepsilon(h)}(\eta)\leq \frac{1}{2}\frac{v\eta^2}{1-c\eta}
  \end{equation*}
  for the constants given in the proof. This is a consequence of the
  fact that $\bbP[\varepsilon(h)]\geq 0$. This means in the terms of
  the of the (weaker) central condition (Condition \ref{cond:central},
  on page \pageref{cond:central}) that it is satisfied with a function
  $\eta(\epsilon)$ which is linear asymptotically as
  $\epsilon\downarrow 0$. More specifically, inversion shows that
  $\eta(\epsilon) = \frac{\epsilon}{\frac{v}{2} + c\epsilon } =
  O(\epsilon)$
  as $\epsilon\downarrow 0$. \citet{grunwald_fast_2016} also proved
  that slow rates were attainable using this point of view.
\end{remark}



%\section{Del Lado de Allá}


% \subsection{An Information Theoretic Bound}


% In this section we suppose that the expeced loss is bounded over
% $\calH$, that is,
% \begin{equation}
%   \sup_{h\in\calH}|L(h)|<\infty,
% \end{equation}
% and that there exists an element $h^*\in\calH$ that minimizes $L(h)$.

% \subsu
%\section{The Central Condition}


% \begin{definition}[The Central Condition and its strong version]
% \label{def:v_central}
% The excess loss $\varepsilon(h)$ on the hypotheses class $\calH$
% satisfies the central condition if there exists a bounded
% non-decreasing function $\eta:[0,\infty)\to[0,\infty)$ and an element
% $h^*\in\calH$ so that if $\epsilon>0$ then for $\eta = \eta(\epsilon)$
% \begin{equation}\label{eq:def_central_condition}
% \sup_{h\in \calH}\log\bbP\rme^{-\eta(\ell(h) - \ell(h^*))}\leq \eta\epsilon.
% \end{equation}
% If it happens that $\eta>0$ can be chosen not depend on $\epsilon$,
% then
% \begin{equation}
% \sup_{h\in \calH}\log\bbP\rme^{-\eta(\ell(h) - \ell(h^*))}\leq 0.
% \end{equation}
% and we say that the strong central condition holds.
% \end{definition}

% \begin{remark}
%   In both its strong and weaker version after using Jensen's
%   inequality, the central condition implies that for any $h$
%   \begin{equation}
%     L(h) \geq L(h^*),
%   \end{equation}
%   that is, $h^*$ minimizes the expected loss. In this case
%   \begin{equation}
%     h^* = \argmin_{h\in\calH}L(h),
%   \end{equation}
%   and one can write for $h\in \calH$ the excess loss as
%   \begin{equation}
%     E(h) = L(h) - L(h^*) = \bbP[\ell(h) - \ell(h^*)].
%   \end{equation}
%   Define the excess loss as the random variable in the last
%   expectation, that is, as the function
%   $\varepsilon: \calH \times \calZ\to \reals$ given by
%   \begin{equation*}
%     \varepsilon(h) = \ell(h) - \ell(h^*),
%   \end{equation*}
%   so that $E(h) = \bbP\varepsilon(h)$ (which explains the choice of
%   symbol).  Note that this definition implies that for each fixed
%   $h\in\calH$ the random variable $\varepsilon(h)$ has non negative
%   expectation, but it can nevertheless be negative. Define the
%   empirical excess risk $E_n(h)$ as
%   \begin{equation}
%     E_n(h) = \bbP_n\varepsilon (h).
%   \end{equation}
%   Note that at an empirical risk minizer $h^*_n$, the empirical excess
%   risk $E_n(h^*_n)$ is negative while the excess risk $E(h^*_n)$
%   itself is positive. For each $h\in\calH$ let
%   \begin{equation}
%     \psi_{\varepsilon(h)}^-(\eta) = \log \bbP\rme^{-\eta\varepsilon(h)}
%   \end{equation}
%   be the cumulant generating function of $-\varepsilon(h)$ and write
%   $\psi_{h}^- = \psi_{\varepsilon(h)}^-$ for short when there is no
%   ambiguity. Thus the Equation \eqref{eq:def_central_condition} in the
%   definition of the central condition can be written as
%   \begin{equation}
%     \sup_{h\in\calH}\psi_{h}^-(\eta)\leq \eta\epsilon.
%   \end{equation}
% \end{remark}

% \subsection{Rates for the Empirical Exces Risk}

% Using the Central Condition, it is straightforward to obtain rates of
% convergence for the empirical excess risk $E_n(h)$. Let us focus on
% hypotheses classes$\calH$ of finite size $N$. We will be able to
% extend these results to certain infinite classes of hypotheses, as
% shown in Section \ref{sect:chaining}. Consider a hypotheses
% class $\calH$ with $N$ elements $h_1,\dots, h_N$ and its excess losses
% $\varepsilon(h) = \ell(h) - \ell(h^*)$ with $h^*$ a minimizer of the
% expected loss $L(h) = \bbP\ell(h)$.  Let $\epsilon>0$. By the Cramér
% Chernoff-method we obtain that
% \begin{equation}
%   \prob{ E_n(h) \leq -\epsilon}  \leq
%   \rme^{-n(\eta \epsilon - \psi_h^-(\eta) )}.
% \end{equation}
% If it happens that for some $\eta > 0$ the strong central
% condition holds and thus all of the $\psi_{h}(\eta)$ are negative,then
% \begin{equation}
%   \prob{ E_n(h) \leq -\epsilon}\leq \rme^{- n\eta \epsilon}.
% \end{equation}
% A use of the union bound leads to
% \begin{align}
%   \prob{ E_n(h^*_n) \leq -\epsilon}
%   &= \prob{ \inf_{h\in\calH}E_n(h) \leq -\epsilon} \\
%   &\leq \sum_{h\in\calH} \prob{E_n(h) \leq -\epsilon}\\
%   &\leq N\rme^{- n\eta \epsilon}
% \end{align}
% which after inversion leads to the conclusion that for each $\delta\in
% (0, 1]$ it is true that with probability higher than $1-\delta$
% \begin{equation}
%   E_n(h^*_n) \geq -\frac{1}{n\eta}\log\frac{N}{\delta}.
% \end{equation}
% Since
% \begin{equation}
%   E_n(h^*_n) = \inf_{h\in\calH}\bbP_{n}\ell(h) - \bbP_{n}\ell(h^*)\leq
%   0,
% \end{equation}
% together these two equations mean that with probability higher than
% $1-\delta$,
% \begin{equation}
%   |E_n(h^*_n)| \leq \frac{1}{n\eta}\log\frac{N}{\delta}
% \end{equation}
%  that is, the empirical excess loss tends to 0 in
% probability at a $n^{-1}$ rate.


% For each $h\in\calH$ it is true that $\psi_h^-(0) = 0$, but the
% requirement that they are non positive for some $\eta > 0$ might be
% too stringent. If instead they tend to zero at an uniform rate, one
% can still obtain rates of convergence for the empirical risk in an
% analogous way. This is, if the central condition holds with
% $\eta = \eta(\epsilon)$, then
% \begin{equation}
%   \psi_h^-(\eta) \leq \eta\epsilon
% \end{equation}
% then one can obtain for each $h\in\calH$ an analogue of Equation
% \begin{equation}
%   \prob{E_n(h)\leq - 2\epsilon}\leq \rme^{-n( 2\eta\epsilon  -
%     \psi_h^-(\eta))} \leq \rme^{-n\eta\epsilon}
% \end{equation}
% If $\eta(\epsilon) = O(\epsilon^{\beta})$ as $\epsilon\downarrow 0$
% for some $\beta\in[0,1]$, then a similar analyis leads to the
% conclusion that with probability higher than $1-\delta$ the inequality
% \begin{equation}
% |E_n(h^*_n)|\leq C\paren{\frac{1}{n}\log\frac{N}{\delta}}^{\frac{1}{1+\beta}}
% \end{equation}
% holds for some constant $C$ and $n$ big enough. This means that the
% empirical excess risk $E_n$ tends to zero at a $n^{-1/(1+\beta)}$
% rate. This corresponds to a $n^{-1/2}$ rate in the case that
% $\beta = 1$, that is, when $\eta(\epsilon)$ is linear close to the
% origin; and a $n^{-1}$ rate when $\beta = 0$, when the strong central
% condition holds.

% We have obtained rates of convergence of the empirical excess risk for
% the ERM rule for finite hypotheses classes $\calH$. However, we are
% interested in also studying rates of convergence for its expected
% excess risk $E(h^*_n)$. Since $E(h^*_n)$ is positive while its
% empirical version $E_n(h^*_n)$ is negative, it would be enough to
% prove the one-sided convergence of $E(h^*_n) - E_n(h^*_n)$ at a
% $n^{-1}$ rate to also obtain fast rates for $E(h^*_n)$. One standard
% method for obtaining deviation inequalities is the Cramér-Chernoff
% method.

% However, as the Cramér-Chernoff method is suited for centered random
% variables and the Central Condition alone does not give information
% about the behaviour the variables $E(h)-\varepsilon(h)$ when they take
% small values, it does not lead to faster rates and thus other methods
% need to be used.






% \subsection{Zhang's Bound}

% In this section we describe the bounds for the expected excess loss
% obtained by \citet{grunwald_fast_2016} using information theoretic
% inequalities previously obtained by
% \citet{zhang_information-theoretic_2006}. These bounds have as
% advantage that they do not only hold for ERM but also for general
% algorithms that output a distribution over the space of hypothesis.


% \citet{grunwald_fast_2016} obtained bounds for the expected excess
% loss in two steps.

% The first step is, under suitable conditions, bounding the excess loss
% $E(h)$ by the \textit{annealed expectation} $A_\eta(h)$ given by
% \begin{equation}\label{eq:annealed_definition}
%   A_\eta(h) := -\frac{1}{\eta}\log \bbP \rme^{-\eta \varepsilon(h)} = -\frac{1}{\eta}\psi^-_h(\eta).
% \end{equation}
% This is not straighforward as an application of Jensen's inequality to
% $A_\eta(h)$ results in
% \begin{equation}
%  A_\eta(h) \leq E(h).
% \end{equation}
% This means a bound on $A_\eta(h)$ does not imply in general a bound on
% the expectated the excess risk $E(h)$. The authors prove that under
% the central condition and an additional weak tail condition, called
% the \textit{witness condition}, the inequality can be reversed. The
% condition reads
% \begin{definition}[Witness Condition]
%   Let $u>0$ and $c\in(0,1]$. The excess loss $\varepsilon$ o satisfies
%   the $(u,c)$-witness condition if for all $h\in \calH$ it is true
%   that
% \begin{equation}
%   c\bbP\varepsilon(h) \leq \bbP \varepsilon(h)\indicator{\varepsilon(h)\leq u}.
% \end{equation}
% \end{definition}


% The second step is bounding the annealed expectation $A_\eta$ using
% the bounds originally obtained by
% \citet{zhang_information-theoretic_2006}. The bound implies, that with
% $\bbP$-probability higher than $1-\delta$
% \begin{equation}
%   \Pi_nA_\eta \leq \IC(\eta,n) + \frac{1}{\eta n}\log\frac{1}{\delta},
% \end{equation}
% where $\IC$ is the \textit{Information Complexity} defined by
% \begin{equation}
%   \IC(\eta,n) = \Pi_nE_n  + \frac{1}{\eta n}\KL(\Pi_n|| \Pi_0)
% \end{equation}
% where $E_n(h) = \bbP_n\varepsilon(h)$ is the empirical excess risk and
% $\KL$ is the Kulback-Leibler divergence
% \begin{equation}
%   \KL(\Pi_n|| \Pi_0) = \Pi_n\log\frac{\rmd\Pi_n}{\rmd\Pi_0}.
% \end{equation}


% \subsection{The Witness Condition}

% Here we give a characterization of the random variables that, in a
% certain sense, satisfy the witness condition. We prove that they
% satisfy the central condition for a choice of $\eta$ that is linear
% close to the origin and constant far from it.

% \begin{theorem}\label{theo:cfg_bernstein_bound}
%   Let $\varepsilon$ an excess loss over a hypotheses class $\calH$ so
%   that either
%   \begin{enumerate}
%   \item \label{itm:witness_condition} The witness condition holds,
%     that is, there exist $u>0$ and $c\in[0,1]$ so that for every $h$
%     \begin{equation}
%       c\bbP \varepsilon(h) \leq \bbP \varepsilon(h)\indicator{\varepsilon(h)\leq u}
%     \end{equation}
%     or
%   \item \label{itm:finite_moment_condition} The second moment of the
%     positive part of the excess loss is uniformly bounded, that is,
%     \begin{equation}
%       \sup_{h}\bbP \varepsilon_{+}(h)^2 <\infty.
%     \end{equation}
%   \end{enumerate}
%   Then if there exists some positive $\eta$ such that
%   $\sup_{h}\psi_h(\eta)<\infty$ there are positive constants $v$
%   and $c$ so that
%   \begin{equation}
%     \sup_{h\in \calH}\psi_h(\eta)\leq \frac{1}{2}\frac{v\eta^2}{1-c\eta}
%   \end{equation}
%   for $0\leq c\eta < 1$.
% \end{theorem}

% \begin{proof}
%   Suppose that that \ref{itm:witness_condition} holds. Let
%   $\varepsilon'$ be the trimmed excess loss
%   $\varepsilon'(h) = \varepsilon(h)\indicator{\varepsilon(h)\leq u}$. Let $\eta>0$ be so that
%   $\sup_{h}\psi_{E(h)}(\eta)<\infty$. Note that for each $h$
%   \begin{equation}
%     \bbP\rme^{-\eta \varepsilon(h)}\leq \bbP\rme^{-\eta \varepsilon'(h)}
%   \end{equation}
%   so that $\psi_{\varepsilon'(h)}(\eta)\leq \Lambda_{\varepsilon'(h)}(\eta)$. At the same
%   time
%   \begin{equation}
%     \bbP\rme^{\eta Y_h} = \bbP\rme^{\eta Y_h}(\indicator{Y_h\leq u} + \indicator{Y_h > u}) \leq 1 + \bbP\rme^{\eta X_h}
%   \end{equation}
%   so that the cfg of the $\varepsilon'(h)$ is also bounded for each
%   $h\in\calH$. These two facts mean that a bound on
%   $\psi_{\varepsilon'(h)}(\eta)$ implies a bound on
%   $\psi_{\varepsilon(h)}(\eta)$ and that the $\varepsilon'(h)$ also
%   have bounded cgf for each $h\in\calH$. Since the $\varepsilon'(h)$
%   are bounded by $u$ from the right, $\bbP \varepsilon'_{+}(h)^2 \leq u^2$ and one
%   can repeat the trick in Equation \eqref{eq:alpha_bound} to obtain that
%   for each integer $n>0$
%   \begin{equation}
%     \bbP \varepsilon'_+(h)^n\leq M\frac{1}{\eta^n}n!,
%   \end{equation}
%   which can be used for $n = 2$ to obtain that
%   \begin{equation}\label{eq:second_moment_bound}
%     \bbP \varepsilon'(h)^2 = \bbP \varepsilon'(h)^2 + \bbP \varepsilon'_+(h)^2 \leq  M\frac{1}{\eta^n}n! + u^2.
%   \end{equation}
%   Thus, define $v = M\frac{1}{\eta^2} + u^2$ and $c = \frac{1}{\eta}$
%   and note that in that case
%   \begin{equation}\label{eq:bernstein_moment_condition}
%     \bbP \varepsilon'_-(h)^n\leq \frac{1}{2}n!vc^{n-2},
%   \end{equation}
%   that is, $\varepsilon'$ satisfies Bernstein's moment condition and, by
%   Corollary \ref{cor:uniform_bernstein}, it is true that
%   \begin{equation}
%   \sup_{h\in \calH}\psi_{\varepsilon(h)}(\eta)\leq \frac{1}{2}\frac{v\eta^2}{1-c\eta}
% \end{equation}
% for $0<c\eta<1$ and that consequently the same bound holds for
% $\sup_{h}\psi_{\varepsilon(h)}(\eta)$.

% In the case that \ref{itm:finite_moment_condition} holds, the analogue
% of Equation \eqref{eq:bernstein_moment_condition} can be obtained for
% $E(h)$ directly by choosing
% $v = M\frac{1}{\eta^2} + \sup_{h}\bbP E(h)_{+}^2$ and
% $c = \frac{1}{\eta}$, so the result also follows from Corollary
% \ref{cor:uniform_bernstein}.
% \end{proof}

% \begin{remark}
%   The two conditions for the previous theorem are not not comparable,
%   that is, one does not imply the other. Take $\calH = \{h\}$ a
%   hypotheses class with one element. Choose $\varepsilon(h)$ to have a
%   Gaussian distribution on $\reals$ with mean zero and unit
%   variance. Then for every $u>0$ it happens that
%   $\bbP E\indicator{E\leq u}< 0$. This means that the witness
%   condition does not hold but $\varepsilon(h)$ has bounded second
%   moment. On the other hand take again of hypotheses with one element
%   and let $\varepsilon(h)$ distributed on the positive integers with
%   distribution function so that $\prob{E = n}\propto n^{-3}$. It
%   follows that $E$ has no second moment but
%   $\bbP E\indicator{E\geq u}>0$ for each $u>0$, that is, it satisfies
%   the witness condition.
% \end{remark}

% \begin{corollary}
%   Let $\varepsilon$ be an excess loss over a hypotheses class $\calH$
%   so that either $\sup_{h}\bbP \varepsilon_+(h)^{2}<\infty$ or it
%   satisfies the witness condition. Then there exists $\eta>0$ such
%   that $\sup_{h}\psi_h(\eta)<\infty$ if and only if there are positive
%   constants $v$ and $c$ such that
% \begin{equation}
%   \sup_{h\in \calH}\psi_h(\eta)\leq \frac{1}{2}\frac{v\eta^2}{1-c\eta}
% \end{equation}
% for $0\leq c\eta < 1$.
% \end{corollary}
% \begin{proof}
%   One implication is Theorem \ref{theo:cfg_bernstein_bound}. The other
%   is obvious.
% \end{proof}

% \begin{corollary}
%   Let $\varepsilon$ be family of excess losses so that
%   $\sup_{h}\bbP \varepsilon_-(h)^{2}<\infty$. Then $\calE$ satisfies
%   the central condition with $\eta(\epsilon) \leq O(\epsilon)$ as
%   $\epsilon\downarrow 0$ if and only if
%   $\sup_{h}\Lambda_{h}(\eta) < \infty$.
% \end{corollary}
% \begin{proof}
%   Because of the previous corollary $\sup_h\Lambda(\eta)<\infty$ if
%   and only if there are constants $c$ and $v$ so that for all $h$
% \begin{equation}
%   \frac{1}{\eta}\psi_h(\eta)\leq \frac{1}{2}\frac{v\eta}{1-c\eta} = O(\eta)
% \end{equation}
% as $\eta\downarrow 0$.  Inversion shows that
% $\eta(\epsilon) = \frac{\epsilon}{\frac{v}{2} + c\epsilon } =
% O(\epsilon)$ as $\epsilon\downarrow 0$.
% \end{proof}


% \subsection{Bernstein's Condition}

% \citet{bartlett_empirical_2006} found that an additional condition,
% called Bernstein's condition, led to faster rates of
% convergence. Their technique relies on a celebrated generalization of
% Bernstein's inequality for empirical processes due to
% Talagrand. However, this analysis is restricted to bounded excess loss
% functions. Later, \citet{audibert_fast_2009} proved that under the
% same condition fast rates were also attainable for probabilistic
% algorithms over finite hypotheses classes. Bernstein's condition reads
% \begin{definition}
%   An excess loss $\varepsilon(h)$ satisfies the $(B,\beta)$-Bernstein
%   condition if there exist $B>0$ and $\beta\in(0,1]$ such that for all
%   $h\in\calH$ it holds that
%   $\bbP \varepsilon(h)^2\leq B(\bbP \varepsilon(h))^\beta$.
% \end{definition}

% Since Bernstein's condition for $\beta = 1$ and the strong central
% condition yield $O_{\bbP}(n^{-1})$ rates of convergence, the question
% of their equivalence arises.

% \citet{van_erven_fast_2015} proved that
% in the case that the excess losses are totally bounded, that is, in
% the case that $\bbP$-almost surely for some $M<\infty$
% \begin{equation}
%   \sup_{h\in\calH}|\varepsilon(h)| < M,
% \end{equation}
% they are equivalent.


% However, this equivalence breaks in the case that excess losses are
% are unbounded. Bernstein's condition implies that both the mean and
% the variance of the excess loss $\varepsilon$ is uniformly bounded
% over $\calH$. This is the case for the variance because since
% \begin{equation}
%   \bbP \varepsilon(h)^2 = \var \varepsilon(h)  + (\bbP \varepsilon(h))^2 \leq B(\bbP \varepsilon(h))^\beta
% \end{equation}
% it holds that
% \begin{equation}\label{eq:bernstein_variance}
% \var \varepsilon(h) \leq B(\bbP \varepsilon(h))^\beta -  (\bbP \varepsilon(h))^2.
% \end{equation}
% Since $\bbP \varepsilon(h) \geq 0$ and the function $x\mapsto Bx^\beta - x^2$ is
% bounded by its value at
% $x^* = \paren{\frac{B\beta}{2}}^\frac{1}{2-\beta}$, we can conclude
% that
% \begin{equation}
%   \var \varepsilon(h) \leq B\paren{\frac{B\beta}{2}}^\frac{\beta}{2-\beta} - \paren{\frac{B\beta}{2}}^\frac{2}{2-\beta}.
% \end{equation}
% which is a uniform bound over all $h$. In turn Equation
% \eqref{eq:bernstein_variance} implies that
% \begin{equation}
% B(\bbP \varepsilon(h))^\beta -  (\bbP \varepsilon(h))^2 \geq 0
% \end{equation}
% which means that
% \begin{equation}
% \bbP \varepsilon(h) \leq B^\frac{1}{2-\beta}.
% \end{equation}
% Since there are excess loss functions without an uniformly bounded
% variance or mean that satisfy the Central Condition, it cannot imply
% Bernstein's condition. On the other hand, Bernstein's condition cannot
% imply the Central Condition as the latter implies the existence of all
% moments of the random variables $\varepsilon_-(h)$ and Bernstein's
% condition is only a relationship between the first and the second
% moments of $\varepsilon(h)$. Indeed, the Bernstein condition is
% satisfied trivially for any class of hypotheses of finite size. It
% then sufices to take excess losses for which $\var \varepsilon(h)< \infty$ but
% but for which the third moment of its left tail is inifinite, that is,
% $\bbP \varepsilon_-(h)^3 = \infty$.

% Even in the case that the variance and the mean of the of the excess
% losses are bounded the strong Central Condition does not imply
% Bernstein's condition. Indeed, let $\calH = \nats$ and let the excess
% loss $\varepsilon$ such that
% \begin{equation}
%   \varepsilon(i)  =
%   \begin{cases}
%     -\log^2 i &\mbox{with probability } \frac{1}{i} \\
%     1  &\mbox{with probability } 1-\frac{1}{i}.
%   \end{cases}
% \end{equation}
% Its mean is
% \begin{equation}
%   \bbP\varepsilon(i) = 1 - \frac{1 + \log^2 i}{i}
% \end{equation}
% and its second moment
% \begin{equation}
%   \bbP\varepsilon^2(i) = 1 + \frac{\log^4 i - 1}{i}
% \end{equation}
% Note that $\varepsilon(1) = 0$ and for any other $i\in\calH$
% $\bbP\varepsilon(i)>0$ so that in this case $h^* = 0$. It is possible
% to show that Bernstein's condition holds for $B = 11$. Never the less,
% the strong central condition does not hold. Rewrite the definition of
% $\varepsilon(i)$ as
% \begin{equation}
%   \varepsilon(i)  =
%   \begin{cases}
%     -a_i &\mbox{with probability } \rme^{-\eta_i a_i} \\
%     1  &\mbox{with probability } 1-\rme^{-\eta_i a_i}
%   \end{cases}
% \end{equation}
% with $\eta_i = 1/\log i$ and $a_i = \log^2 i$. Then, fot any
% $\eta>0$
% \begin{equation}
%   \bbP\rme^{-\eta\varepsilon(i)}\geq \rme^{(\eta-\eta_i)a_i}.
% \end{equation}
% Thus, we can chose $i$ big enough so that $\eta-\eta_i > 0$ because
% $\eta_i\to 0$ as $i\to\infty$. Consequently
% \begin{equation}
%   \bbP\rme^{-\eta \varepsilon(i)}\geq 1
% \end{equation}
% for big enough $i$, which means that the strong central condition does not hold.

% \subsection{Witness + Central Implies Bernstein for Trimmed Losses}

% Let $\varepsilon$ be an excess loss function that satisfies both the
% witness condition for some $u$ and $c$, and the strong central
% condition. Let $\eta$ such that for all $h\in\calH$ it happens that
% $\psi_h^-(\eta)\leq 0$. This means that
% $\bbP\rme^{-\eta\varepsilon(h)}\leq 1$. By Taylor's theorem for each
% $h\in\calH$ there exists $\eta_h\in(0,\eta)$ depending on $h$ such
% that
% \begin{equation}
%   \bbP\rme^{-\eta\varepsilon(h)} = 1 - \eta\bbP\varepsilon(h) +
%   \frac{\eta^ 2}{2}\bbP\rme^{-\eta_h\varepsilon(h)}\varepsilon^2(h),
% \end{equation}
% which in addition to the central condition means that
% \begin{equation}
%   \frac{\eta}{2}\bbP\rme^{-\eta_h\varepsilon(h)}\varepsilon^2(h)
%   \leq \bbP\varepsilon(h)
% \end{equation}
% By the witness condition the expected value of $\varepsilon(h)$ can be
% bounded by that of its trimmed verion $\varepsilon'(h)$ so that
% \begin{equation}
%   \bbP\rme^{-\eta_h\varepsilon(h)}\varepsilon^2(h) \leq C\bbP\varepsilon'(h)
% \end{equation}
% for some constant $C$ independent of $h$. Thus it is enough to prove
% that $\bbP\varepsilon'^ 2(h)$ can be bounded by the term in the left
% hand of the previous equation. Write
% \begin{align}
%   \bbP\rme^{-\eta_h\varepsilon(h)}\varepsilon^2(h) &=
%   \bbP\rme^{-\eta_h\varepsilon'(h)}\varepsilon'^2(h) +
%   \bbP\rme^{-\eta_h\varepsilon(h)}\varepsilon^2(h)\indicator{\varepsilon(h)\geq
%   u}\\
%   &\geq \bbP\rme^{-\eta_h\varepsilon'(h)}\varepsilon'^2(h)\\
%   &\geq \rme^{-\eta u}\bbP\varepsilon'^2(h).
% \end{align}
% Conclude that
% \begin{equation}
%   \bbP\varepsilon'^2(h) \leq B \bbP\varepsilon'(h)
% \end{equation}
% for some constant $B$.

% \subsection{Bernstein for the Trimmed Losses Does not Imply
%   Bernstein For the Full Losses}

% (Which added with the previous section, means that Witness +
% Central does not imply Bernstein)

% Let $\varepsilon$ be an excess loss function on a hypotheses space
% $\calH$ such that they satisfy the witness condition for some
% $u,c>0$. We showed in the previous section that the strong central
% condition implies that the trimmed version of the losses
% $\varepsilon'(h) = \varepsilon(h)\indicator{\varepsilon(h)\leq u}$
% satisfy the strongest version of Bernstein's condition, that is, there
% exists a constant $B>0$ such that
% \begin{equation}
%   \bbP\varepsilon'^2(h) \leq B\bbP\varepsilon'(h).
% \end{equation}

% Nevertheless there are situations in which this does not imply that
% the original excess loss satisfy Bernstein's condition. Indeed, let
% $\calH=\nats$ the excess loss $\varepsilon(i)$ such that
% \begin{equation}
%   \varepsilon(i) =
%   \begin{cases}
%     i-1 &\mbox{with probability } \frac{1}{i^2} \\
%     \frac{1}{i}  &\mbox{with probability } 1-\frac{1}{i^2}.
%   \end{cases}
% \end{equation}
% Note that $\varepsilon(1)=0$ so that in this case $h^* = 1$ and that
% since it is nonnegative, it satisfies the Central Condition.

% For this choice, the excess loss $\varepsilon(i)$ satisfies the
% witness condition for $u=1$ and $c=1/2$. This is because the excess
% loss $\varepsilon(i)$ and its trimmed version
% $\varepsilon'(h) = \varepsilon(h)\indicator{\varepsilon(h)\leq u}$
% satisfy
% \begin{align}
%   \bbP\varepsilon(i) &= \frac{2}{i} - \frac{1}{i^2} - \frac{1}{i^3}\\
%   \bbP\varepsilon'(i) &=  \frac{1}{i} - \frac{1}{i^3}
% \end{align}
% so that for $i\geq 2$
% \begin{equation}
%   \frac{\bbP\varepsilon(i)}{\bbP\varepsilon'(i)} = \frac{2i^2 - i -
%     1}{i^2 - 1} \leq 2,
% \end{equation}
% which means that
% \begin{equation}
%   c\bbP\varepsilon(i) \leq \bbP\varepsilon'(i).
% \end{equation}
% The second moment of the excess loss and its trimmed version satisfy
% \begin{align}
%   \bbP\varepsilon^2(i) &= 1 -\frac{2}{i} + \frac{2}{i^2} - \frac{1}{i^4}\\
%   \bbP\varepsilon'^2(i) &=  \frac{1}{i^2} - \frac{1}{i^4}
% \end{align}
% so that on the one hand
% \begin{equation}
%   \frac{\bbP\varepsilon'^2(i)}{\bbP\varepsilon'(i)} = \frac{i^2 -
%     1}{i^3 - i}\leq 1
% \end{equation}
% which implies that the Bernstein condition holds for the trimmed
% excess loss with $B =1$. On the other hand for the excess loss
% \begin{equation}
%   \frac{\bbP\varepsilon^2(i)}{\bbP\varepsilon(i)}\asymp \frac{i}{2}
% \end{equation}
% as $i\to\infty$, that is, Bernstein's condition does not hold.


% \section{Uniform Square Integrability makes Bernstein and Central Equivalent}
% \begin{remark}
%   In order to prove that Central and Bernstein condition are
%   equivalent for a bounded excess loss, the authors use the fact that
%   for each $\eta$ the inequality
%   \begin{equation}
%     \expv{\rme^{-\eta \varepsilon(h)}\varepsilon(h)^2}\leq \norm{\rme^{-\eta \varepsilon(h)}}_\infty\norm{\varepsilon(h)^2}_1
%   \end{equation}
% \end{remark}
% We now look at $\{\varepsilon(h)\}_{h\in\calH}$ as a set of random
% variables. By a slight abuse of notation, we say that the excess loss
% function $\varepsilon$ on a hypotheses class $\calH$ is uniformly
% square integrable if and only if
% \begin{equation}
%   \lim_{C\to\infty}\sup_{h\in\calH}\expv{\varepsilon(h)^2\indicator{|\varepsilon(h)|>C}}
%   = 0
% \end{equation}

% We will use a characterization of Uniform integrability given by de la
% Vallé Poussin
% \begin{theorem}[de la Vallé Poussin]
%   A family of random variables $\{X_i\}_{i\in I}$ is uniformly
%   integrable if and only if there exists a function
%   $\Phi:\reals\to\reals^+$ such that
%   \begin{enumerate}
%   \item $\Phi$ is convex
%   \item $\Phi(0) = 0$
%   \item $\Phi$ is symmetric, that is,  $\Phi(x) = \Phi(-x)$
%   \item $\Phi$ tends to infinity faster than a linear function, that
%     is, $\Phi(x)/x\to \infty$ as $x\to\infty$
%   \end{enumerate}
%   and
%   \begin{equation}
%     \sup_{i\in I}\expv{\Phi(X_i)} < \infty
%   \end{equation}
% \end{theorem}

% Aditionaly, it can be shown (See \citet{meyer_sur_1978}) that in the
% case that a family of random variabels is uniformly integrable the
% function $\Phi$ can be chosen to have \textit{moderate growth}, that
% is, there is a constant $c$ such that
% \begin{equation}
%   \Phi(2x)\leq c\Phi(x)
% \end{equation}
% and consequently
% \begin{equation}
%   \Phi(x)\leq x^2
% \end{equation}


% The convex functions $\Phi$ that satisfy those conditions are
% called Young functions. To each of these functions one can define the
% norm $\norm{\cdot}_{\Phi}$, called Luxemburg norm, given by
% \begin{equation}
%   \norm{X}_{\Phi} = \inf_{c}\bracks{c:\expv{\Phi(X/c)}\leq 1}.
% \end{equation}
% Define the space of functions $L^{\Phi}$ as the set of random
% variables for which the norm $\norm{\cdot}_{\Phi}$ is finite with
% the usual identification according to almost sure equality.

% It is possible to prove that
% \begin{equation}
%   L^{\Phi}\subset L^1,
% \end{equation}
% that $L^{\Phi}$ is a Banach space and that the inclussion
% $L^{\Phi}\hookrightarrow L^1$ is continuous, that is, for a
% $X\in L^\Phi$ it is true that for some nonzero constant $a$
% \begin{equation}
%   a\norm{X}_1\leq \norm{X}_\Phi.
% \end{equation}

% We will consider situations in which this last equation can be
% reversed, that is, situations in which there is a constant $b$ such
% that
% \begin{equation}
%   \norm{X}_\Phi \leq b\norm{X}_1.
% \end{equation}

% \begin{remark}
%   In the case that $\{\varepsilon(h)\}_{h\in\calH}$ is square
%   uniformly integrable with test function $\Phi$ and there exists a
%   set $S\subseteq L^1$ such that $S$ is Banach subspace of $L^1$ then
%   \begin{equation}
%     \norm{\varepsilon(h)^2}_\Phi\leq c_2\norm{\varepsilon(h)^2}_1 = \bbP\varepsilon(h)^2.
%   \end{equation}
%   by the open mapping theorem. This is the case bounded excess loss
%   functions, for instance.
% \end{remark}

% Let $\Phi^*$ be the convex conjugate of $\Phi$ defined by
% \begin{equation}
%   \Phi^*(y) = \sup_x (xy - \Phi(x)).
% \end{equation}
% It is possible to show that $\Phi^*$ is a Young function if and only
% if $\Phi$ is a Young function. As a consequece of Young's inequality
% \begin{equation}
%   xy\leq \Phi(x) +\Phi(y)
% \end{equation}
%  if $X\in L^{\Phi}$ and $Y\in L^{\Phi^*}$A Hölder type inequality holds
% \begin{equation}
%   \norm{XY}_1\leq 2\norm{X}_{\Phi}\norm{Y}_{\Phi^*}.
% \end{equation}

% \begin{theorem}
%   Let $\varepsilon$ be an excess loss on a hypotheses class such that
%   \begin{enumerate}
%   \item $\varepsilon$ is uniformly square integrable for a test
%     function $\Phi$.
%   \item $\sup_h\norm{\rme^{-\eta \varepsilon(h)}}_{\Phi^*}<\infty$
%   \item There exists some constant $c$ such that for each $h$
%     \begin{equation}
%       \norm{\varepsilon(h)^2}_{\Phi}\leq c \norm{\varepsilon(h)^2}_1 = c\bbP\varepsilon(h)^2
%     \end{equation}
%   \end{enumerate}
%   Then Bernstein's condition and Central Condition are equivalent
% \end{theorem}

% \begin{proof}
 %  This proof will rest on the fact for each $\eta$ where
%   $\sup_h\bbP\rme^{-\eta\varepsilon(h)}<\infty$, there exists an
%   $\eta^*$ depending on $h$ such that
%   \begin{equation}\label{eq:mgf_expansion}
%     \bbP\rme^{-\eta \varepsilon(h)} = 1 - \eta\bbP \varepsilon(h) +
%     \frac{1}{2}\eta^2\bbP\rme^{-\eta^*\varepsilon(h)}\varepsilon(h)^2.
%   \end{equation}

%   Suppose that Bernstein's Condition Holds. Then, by Equation
%   \eqref{eq:mgf_expansion} it is enough to prove that for each $h$ one has
%     \begin{equation}
%    \bbP\rme^{-\eta^*\varepsilon(h)}\varepsilon(h)^2 \leq \frac{2}{\eta}\bbP \varepsilon(h).
%   \end{equation}
%   By Hölder's Inequality and our assumptions
%   \begin{equation}
%     \bbP\rme^{-\eta^*\varepsilon(h)}\varepsilon(h)^2 \leq
%     \norm{\rme^{-\eta^*\varepsilon(h)}}_{\Phi^*}\norm{\varepsilon(h)^2}_{\Phi}
%     \leq C\bbP\varepsilon(h)^2
%   \end{equation}
%   for some constant $C>0$. The results follows from Bernstein's
%   condition.

%   Suppose that the central condition holds for a $\eta>0$. Then by
%   Equation \eqref{eq:mgf_expansion} for each $h$ there exists a $\eta^*$
%   such that
%   \begin{equation}
%    \bbP\rme^{-\eta^*\varepsilon(h)}\varepsilon(h)^2 \leq \frac{2}{\eta}\bbP \varepsilon(h)
%   \end{equation}
%   Let $K>0$. Note that
%   \begin{align*}
%     \expv{\rme^{-\eta^* \varepsilon(h)}\varepsilon(h)^2} &=
%     \expv{\rme^{-\eta^* \varepsilon(h)}\varepsilon(h)^2\indicator{\varepsilon(h)>K}} +
%     \expv{\rme^{-\eta^*
%     \varepsilon(h)}\varepsilon(h)^2\indicator{\varepsilon(h)\leq K}}\\
%     &\geq
%      \rme^{-\eta K}\expv{\varepsilon(h)^2\indicator{\varepsilon(h)\leq K}}
%   \end{align*}
%   so it is enough to prive that there is a constant $C$ such that
%   \begin{equation}
%     \expv{\varepsilon(h)^2\indicator{\varepsilon(h)\geq K}}\leq C\expv{\varepsilon(h)^2}
%   \end{equation}
%   Use Hölder's inequality to obtain that
%   \begin{equation}
%     \expv{\varepsilon(h)^2\indicator{\varepsilon(h)\geq K}} \leq <2\norm{\indicator{\varepsilon(h)\geq K}}_{\Phi^*}\norm{\varepsilon(h)^2}_{\Phi}
%   \end{equation}
%   Since
%   \begin{equation}
%     \norm{\indicator{\varepsilon(h)\geq K}}_{\Phi^*}  = (\Phi^*)^{-1}(\prob{\varepsilon(h)>K})
%   \end{equation}
%   where $(\Phi^*)^{-1}$ is the generalized inverse of $\Phi^*$.  We can
%   use Markov's inequality to obtain that
%   \begin{equation}
%     \norm{\indicator{\varepsilon(h)\geq K}}_{\Phi^*} \leq
%   \end{equation}
%   Since
%   $\Phi^*$ is a Young function, it is nondecreasing  we
%   have that
%   \begin{equation}
%     \expv{\varepsilon(h)^2\indicator{\varepsilon(h)\geq K}} \leq
%   \end{equation}
% \end{proof}

% \subsection{Uniform Square Integrability makes Central Imply Bernstein}


% \subsection{Uniform Square Integrability makes Bernstein Imply Central}


% \subsection{The Condition of Vapnik}

% Nevertheless conditions \ref{cond:central} and \ref{cond:witness} are
% not necessary to obtain inequalities such as that of Theorem
% \ref{thm:annealed_inverse_bound}. In this section we show that a
% condition on the tails of the excess losses is enough to obtain such
% inequality


% \begin{theorem}
%   Let $\varepsilon$ be an excess loss function such that
%   \begin{equation*}
%     \sup_{h\in\calH}\frac{(\bbP[\varepsilon^{\alpha}(h)])^{1/\alpha}}{\bbP[\varepsilon(h)]}\leq
%     \tau
%   \end{equation*}
%   for some finite non zero $\tau$ and $\alpha > 2$. Then with
%   probability higher than $1-\delta$ it holds that
%   \begin{equation*}
%     E(h) \leq \mu(\eta)A_{\eta}(h)
%   \end{equation*}
%   with
% \begin{equation*}
%   \mu(\eta) = \frac{1}{1-\frac{\eta}{\eta*}}
% \end{equation*}
% where
% \begin{equation*}
%   \eta^* = \frac{1}{C\sup_{h\in\calH}E(h)}
% \end{equation*}
% for some constant $C$.
% \end{theorem}
% \begin{proof}
%   Consider a fixed $h\in\calH$. We know that for each $\eta>0$ it
%   holds that
%   \begin{equation*}
%     \log\bbP[\rme^{-\eta\varepsilon(h)}] \leq -\eta
%     \bbP[\varepsilon(h)] + \frac{1}{2}\eta^2\bbP[\rme^{-\eta^*\varepsilon(h)}\varepsilon^2(h)]
%   \end{equation*}
%   for some $\eta^*\in(0,\eta)$. By Hölder's inequality, the assumption
%   \begin{align*}
%     \bbP[\rme^{-\eta^*\varepsilon(h)}\varepsilon^2(h)]
%     &\leq
%       \paren{\bbP[\rme^{-\eta^*\varepsilon(h)/(1-2/\alpha)}]}^{1-\alpha/2} \paren{\bbP[\varepsilon^\alpha(h)]}^{2/\alpha}\\
%     &\leq \paren{\bbP[\rme^{-\eta^*\varepsilon(h)/(1-2/\alpha)}]}^{1-\alpha/2} \paren{\bbP[\varepsilon(h)]}^{2}
%   \end{align*}
%   Both our choice of $\eta$ and the fact that the function
%   $\eta \mapsto \bbP[\rme^{-\eta\varepsilon(h)}}]$ is increasing
% because $\varepsilon(h)$ has nonnegative expectation imply that
% \begin{equation*}
%   \paren{\bbP[\rme^{-\eta^*\varepsilon(h)/(1-2/\alpha)}]}^{1-\alpha/2}
% \leq \paren{\bbP[\rme^{-\bar{\eta}\varepsilon(h)}]}^{1-\alpha/2} < M
% \end{equation*}
% for some finite constant $M$ independent of $h$. Bringing together
% these three inequalities we obtain that there is a constant $C$
% independent of $h$ for which
% \begin{align*}
%   \log\bbP[\rme^{-\eta\varepsilon(h)}]
%   &\leq -\eta\bbP[\varepsilon(h)]
%     + C\eta^2\bbP[\varepsilon(h)]^2\\
%   & = \eta\bbP[\varepsilon(h)](C\eta\bbP[\varepsilon(h)] - 1)\\
%   & \leq \eta\bbP[\varepsilon(h)](C\eta\sup_{h\in\calH}\bbP[\varepsilon(h)] - 1)
% \end{align*}
% \end{proof}



\chapter{Conclusion\label{sect:conclusion}}

We considered the abstract learning problem. Using a $\calZ$-valued
iid sample $Z_1,\dots,Z_n$, the problem is choosing a hypothesis $h$
from a hypotheses class $\calH$ that minimizes the expected loss
$L(h) = \bbP[\ell(h,Z)]$ according to the loss function
$\ell:\calH\times\calZ\to\reals$

In this work we considered conditions for finding bounds on the
expected excess loss
\begin{equation*}
  E(h) = L(h)-\inf_{h\in\calH}(h)
\end{equation*}
in the case in which there exists a $h^*\in\calH$ at which the
expected loss $L(h)$ is minimized, when one can write
\begin{equation*}
  E(h) = \bbP[\varepsilon(h,Z)]
\end{equation*}
where $\varepsilon(h,Z) = \ell(h,Z) - \ell(h^*, Z)$ is the excess
loss. We consider conditions under which the order of the bounds can
be improved from $n^{-1/2}$ to $n^{-1}$ up to logarithmic factors,
which is significant for practice. The conditions that we considered
are on the excess losses themselves and the bounds that we reviewed
are valid both for the method of Empirical Risk Minimization and in
the PAC-Bayesian setting.

In the case that the excess losses are bounded it is known that a
condition, called Bernstein's condition (Condition
\ref{cond:bernstein} on page \ref{cond:bernstein}) gives fast rates
and that it is equivalent to the strong central condition (Condition
\ref{cond:central} on page \pageref{cond:central}). However, when
excess losses might be unbounded, this landscape changes. Under an
additional condition on the right tail of the distribution of the
excess loss (the witness condition, Condition \ref{cond:witness} on
page \pageref{cond:witness}) it is known that the central condition
leads to fast rates both for ERM and in the PAC-Bayesian setting
\citep{grunwald_fast_2016} while the same is only known under
Bernstein's condition for a nonstandard randomized algorithm
\citep{audibert_fast_2009}.  Both conditions, however, are no longer
equivalent in the unbounded case, even under stringent tail and moment
conditions, which we showed in Section
\ref{sect:central_witness_bernstein}.

We considered a natural weakening of the case in which excess losses
are bounded, the situation in which the probability of any nonoptimal
hypotheses being better than the optimal is exponentially small. In
this regime, in Section \ref{sect:new_vapnik}, nevertheless, a mild
condition on the moments of the excess loss (Condition \ref{cond:new}
on page \pageref{cond:new}) made both the central and Bernstein's
conditions equivalent. Also in this regime, we showed that when the
strong central condition holds (and thus fast rates are possible)
tighter bounds than those of \citet{grunwald_fast_2016} (Section
\ref{sect:new_vapnik}), are possible, although under a stronger
assumption. We showed that this is indeed the regime of many practical
situations, which means that our new bound is relevant for
applications. Finally, in Section \ref{section:tail_bounds} we showed
that under our weakest assumptions, slow bounds could also be proven.

The ultimate goal of this line of research would be to completely
characterize when fast rates are attainable in the regime that we
studied. With this in mind, we present some open questions.
\begin{description}
\item[Open Questions] As we showed, in the regime in which excess
  losses are negative with exponentially small tails (Condition in
  \eqref{eq:bounded_cumulant_2}), under a moment condition both
  Bernstein's and the strong central condition are equivalent and lead
  to fast rates. This leads to the first question:
  \begin{enumerate}
  \item Are there other conditions that lead to fast rates in this
    regime?
  \item In relation to the last question, do central and Bernstein's
    condition characterize completely fast rates in this case, that
    is, is it possible to prove that if fast rates are achieved, then
    these conditions must hold?
  \end{enumerate}
  We also proved in Theorem \ref{theo:cfg_bernstein_bound} that in the
  case that the witness or a second moment condition holds then slow
  rates are also possible.
  \begin{enumerate}
    \setcounter{enumi}{2}
  \item Is this second moment condition also sufficient to achieve
    fast rates in the case that the strong central condition holds?
    This is relevant as this condition might be easier to check.
  \end{enumerate}
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Appendixes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{appendices}

\chapter{Rates of Convergence in Probability\label{app:basic_rates}}

Let $\{X_n\}_{n\in\nats}$ be a sequence of random variables and let
$X$ be a random variable. We say that $X_n$ tends to $X$ in
probability if for any $\epsilon>0$
\begin{equation*}
  \prob{|X_n - X| >\epsilon}\to 0
\end{equation*}
as $n\to 0$. We denote this as $X_n\toinP X$ for short. For some real
function $r(n)$ we say that $X_n$ is of order $r(n)$ in probability or
$X_n = O_{\bbP}(r(n))$ for short, if for each $\delta>0$ there is a
finite $M>0$ such that with probability higher than $1-\delta$
\begin{equation*}
  |X_n - X|  \leq M |r(n)|
\end{equation*}
for $n$ big enough. In the special case that $r(n)\to 0$ as
$n\to\infty$ we say that $X_n$ tends to $X$ at a a rate $r(n)$ (in
probability).

We are interested in bounds of the form
\begin{equation*}
  \prob{|X_n - X| > \epsilon}\leq a\rme^{-nf(\epsilon)}
\end{equation*}
for some constant $a$ and positive function $f(\epsilon)$ tending to 0
as $\epsilon\downarrow 0$. Set $\delta = a\rme^{-nf(\epsilon)}$. If
$f$ is one-to-one, inversion shows that
\begin{equation*}
  \epsilon = f^{-1}\paren{\frac{1}{n}\log\frac{a}{\delta}},
\end{equation*}
and that consequently with probability higher than
$1-\delta$
\begin{equation*}
  |X_n- X| \leq f^{-1}\paren{\frac{1}{n}\log\frac{a}{\delta}}.
\end{equation*}
In the case that for some $\alpha> 0$ the function $f^{-1}$ satisfies
$f^{-1}(x) = O(x^{\alpha})$ as $x\to 0$, the last equation implies
that $X_n$ tends to $X$ at a $n^{-\alpha}$ rate. Thus, the behavior
of the inverse and thus of $f(x)$ itself as $x\downarrow 0$ determines
the rate of convergence in probability of the sequence $X_n$. In
particular if $f(x) = O(x^{1+\beta})$ as $x\downarrow 0$, for some
$\beta\in [0,1]$, then the convergence happens at a $n^{-1/(1+\beta)}$
rate, which leads to rates between $n^{-1}$ for $\beta = 0$, and
$n^{-1/2}$ for $\beta = 1$.

\chapter{The Cramér-Chernoff Method \label{app:cramer_chernoff}}

\section{Cumulant Generating Functions}
The Cramér-Chernoff method is behind many basic (and advanced) methods
for obtaining concentration inequalities. It is based on the use of
the cumulant generating function to obtain tail bounds. Let $X$ be a
random variable with mean $\bbP X$ and let $\psi_{X}(\eta)$ defined by
\begin{equation*}
  \psi_{X}(\eta) = \log\bbP[\rme^{\eta X}]
\end{equation*}
be its cumulant generating function.

If there is some $\eta^*>0$ such that $\psi_X(\eta^*)<\infty$, then
$\psi_X(\eta)$ exists for all $\eta$ in the interval $[0, \eta^*]$.
This is a consequence of the exponential function's convexity. Indeed,
for any $\eta\in[0,\eta^*]$ it is true that $\eta = \theta\eta^*$ for
some $\theta\in [0,1]$ and consequently
$\bbP\rme^{\eta X}\leq \theta \bbP\rme^{\eta^*
  X}<\infty$.
Furthermore, $\psi_X(0) = 0$ and it is smooth.

Define the probability measure $\bbP_\eta$ by its density with respect
to $\bbP$ given by
\begin{equation*}
  \frac{\rmd\bbP_\eta}{\rmd\bbP} = \frac{\rme^{\eta
      X}}{\bbP[\rme^{\eta X}]}.
\end{equation*}
Direct calculation shows that
\begin{equation*}
  \psi_X'(\eta) = \bbP_\eta X
\end{equation*}
and that
\begin{equation*}
  \psi_X''(\eta) = \var_\eta X,
\end{equation*}
where $\var_{\eta}$ makes reference to the variance taken with respect
to the probability distribution $\bbP_\eta$. This means that the
function $\eta\mapsto \psi_X(\eta)$ is convex.

\section{Cramér - Chernoff Tail Bounds}

The Cramér-Chernoff method is based on a smart use of Markov's
inequality. If $\psi(\eta)<\infty$ for some $\eta>0$, for $t>0$
Markov's inequality gives
\begin{align}
  \prob{X \geq t} &= \prob{ \rme^{\eta X} \geq
                           \rme^{\eta t} }\\
  &\leq \rme^{\psi_X(\eta) - \eta t}
\end{align}
In any case, one can optimize the upper bound and obtain that
\begin{equation*}
  \prob{ X \geq t } \leq \rme^{-\psi_X^*(t)}
\end{equation*}
where
\begin{equation*}
  \psi_X^*(t) = \sup_{\eta > 0}\eta t - \psi_X(\eta).
\end{equation*}
By Jensen's inequality $\psi_X(\eta) \geq \eta\bbP X$, so that the
bound becomes trivial for $t\leq \bbP X$ because in that case
$\eta\epsilon - \psi_X(\eta)\leq 0$. Thus, one might extend the
supremum in the definition of $\psi^{*}$ to
\begin{equation*}
  \psi_X^*(t) = \sup_{\eta \in \reals}\eta\epsilon - \psi_X(\eta).
\end{equation*}
This means that for $t\geq \bbP X$ the function $\psi^*(t)$ coincides
with the convex conjugate of $\psi(\eta)$, which is also called the
Fenchel-Legendre dual function.


\section{Possible Rates Via The Cramér-Chernoff Method}

The Cramér-Chernoff method is behind many basic (and advanced)
concentration inequalities for random variables. Here we show that
under no additional assumptions, it is not possible to obtain faster
rates than $n^{-1/2}$ from it. Even though not necessarily in the
context of the search for faster rates, this fact has been previously
noted. We follow part of the development of
\citet{mcallester_concentration_2003} in this section. A more detailed
description of the Cramér-Chernoff method can be found in
\citet[Chapter 2]{boucheron_concentration_2013}.

With the observations of Appendix \ref{app:basic_rates} and of the
previous section, in many situations the the rate of convergence of a
succession of random variables depends on the behavior of the function
$\psi_{X-\bbP X}^*(\epsilon)$ for small $\epsilon$. In the following
we show for small $\epsilon$, the function $\psi^*_{X-\bbP X}$ behaves
as a quadratic function. Given the discussion of Appendix
\ref{app:basic_rates}, this implies rates of order $n^{-1/2}$. We show
this by noticing that the zeroth and first order terms in its Taylor
approximation of $\psi^*_{X-\bbP X}$ are zero.

First we write a more explicit expression for
$\psi^*_{X-\bbP X}(\eta)$ from which we can obtain more
information. Since $\psi_{X-\bbP X}$ is smooth, the infimum that
defines its dual $\psi^*_{X-\bbP}$ can be found by
differentiation with respect to $\eta$. The infimum is attained at the
$\eta$ such that
$\epsilon =  \bbP_{\eta}X-\bbP X $. Such $\eta$
exists because the function $\eta\mapsto\bbP_\eta\varepsilon$ is
monotonously increasing. By a slight abuse of notation, refer to that
special $\eta$ as $\eta(\epsilon)$. With this in mind
$\psi_{E-\varepsilon}^*$ can be written as
\begin{equation*}
  \psi_{X-\bbP X}^*(\epsilon) = \eta(\epsilon)\epsilon -
  \psi_{X-\bbP X}(\eta(\epsilon)).
\end{equation*}

We can now proceed to study the behavior of
$\psi_{E-\varepsilon}^*(\epsilon)$ for small $\epsilon$ through its
Taylor approximation. Note that for a sufficiently smooth function
$f$, we can write
\begin{equation*}
  f(x) =  f(0) + xf'(0)  + x^2\int_0^1\int_0^tf''(xs)\rmd s\rmd t
\end{equation*}
and that by Taylor's theorem the last term is approximately quadratic
for small $x$, more precisely,
\begin{equation*}
  x^2\int_0^1\int_0^tf''(xs)\rmd s\rmd t = \frac{f''(0)}{2}x^2 + o(x^2)
\end{equation*}
as $x\downarrow 0$. Note also that because $\eta(0) = 0$ and
$\psi_{E-\varepsilon}(0) = 0$, we have that
$\psi_{E-\varepsilon}^*(0) = 0$ and that
$(\psi_{E-\varepsilon}^*)'(0) = \eta(0) = 0$. Additionally, direct
computation reveals that
$$(\psi_{X-\bbP X}^*)''(\epsilon) =
\frac{1}{\var_{\eta(\epsilon)}X}.$$
Together these observations imply that
\begin{equation*}
  \psi_{X-\bbP X}^*(\epsilon) = \epsilon^2\int_0^1\int_0^t\frac{1}{\var_{\eta(\epsilon s)}X}\rmd s\rmd t.\end{equation*}
and that for small $\epsilon$
\begin{equation*}
  \psi_{X-\bbP X}^*(\epsilon) = \frac{1}{2\var{\varepsilon}} \epsilon^2 + o(\epsilon^2)
\end{equation*}
as $\epsilon\downarrow 0$, a quadratic function.


% \section{Subgamma Random Variables}


% \begin{theorem}\label{theo:sub_gamma_carachterization} Let $\calX =
% \{X_i\}_{i\in I}$ be a family of random variables. Then the following
% are equivalent
% \begin{enumerate}
% \item \label{itm:tail_condition} There exists two positive constants
% $M$ and $\eta$ such that
% 	\begin{equation*} \sup_{i\in I}\prob{X_i > x}\leq M\rme^{-\eta
% x}
% 	\end{equation*}
% \item \label{itm:mfg_condition} There exists $\eta>0$ so that
% 	\begin{equation*} \sup_{i\in I}\bbP \rme^{\eta X_i}<\infty
% 	\end{equation*}
% \item \label{itm:alpha_condition} There exist positive $\alpha$ and
% $\eta$ so that
% 	\begin{equation*} \sup_{i\in I} \bbP \rme^{\eta
% X_i}|X_i|^\alpha < \infty
% 	\end{equation*}
% \end{enumerate}
% \end{theorem}


% \begin{proof} First prove that \ref{itm:tail_condition} and
%   \ref{itm:mfg_condition} are equivalent.

% Suppose that \ref{itm:mfg_condition} is true for some $\eta>0$. By
% Markov's inequality
% \begin{align} \prob{X_i > x} = \prob{\rme^{\eta X_i} > \rme^{\eta
% x}}\leq M\rme^{-\eta x}
% \end{align} with $M = \sup_{i\in I} \bbP \rme^{\eta X_i}$. Suppose now
% that \ref{itm:tail_condition} is true for a given $\eta^*$ and pick a
% positive $\eta < \eta^*$. Write $X$ as the difference of its positive
% part $X_+=\max\bracks{0, X}$ and its negative part
% $X_-=\max\bracks{0,-X}$. By Hoelder's inequality
% \begin{equation*} \bbP\rme^{\frac{\eta}{2} X} \leq \sqrt{\bbP\rme^{\eta
% X_+}\bbP\rme^{-\eta X_-}}
% \end{equation*} and since $\rme^{-x}\leq 1$ for $x\geq 0$ bound further
% and obtain that
% \begin{equation*} \bbP\rme^{\frac{\eta}{2} X} \leq
% \sqrt{\bbP\rme^{\eta X_+}}.
% \end{equation*} Focus now on $\bbP\rme^{\eta X^+}$. Write
%  \begin{align*} \bbP\rme^{\eta X_+}-1 &=
% \bbP\int_0^{X^+}\eta\rme^{\eta s}\rmd s\\ &= \bbP\int_0^\infty
% \indicator{X_+\geq s}\eta\rme^{\eta s}\rmd s \\ &= \int_0^\infty
% \prob{X_+\geq s}\eta\rme^{\eta s}\rmd s \\ &\leq M
% \int_0^\infty\eta\rme^{(\eta-\eta^*)s}\rmd s \\ &= M\frac{\eta}{\eta^*
% - \eta}.
%  \end{align*} Conclude then that
%  \begin{equation*} \bbP\rme^{\frac{\eta}{2} X}\leq \sqrt{1 +
% M\frac{\eta}{\eta^* -\eta}},
%  \end{equation*} so that \ref{itm:mfg_condition} holds.

% Prove now that \ref{itm:alpha_condition} and \ref{itm:mfg_condition}
% are equivalent.

% Suppose that \ref{itm:alpha_condition} is true for some $\alpha>0$ and
% $\eta^*>0$. Write
% \begin{align} \bbP\rme^{\eta^* X_i} &= \bbP\rme^{\eta^*
% X}\indicator{\abs{X_i}\leq 1} + \bbP\rme^{\eta^*
% X}\indicator{\abs{X_i}> 1} \\ &\leq \bbP\rme^{\eta^*
% X_i}\indicator{\abs{X_i}\leq 1} + \bbP\rme^{\eta^*
% X_i}|X_i|^{\alpha^*}\\ &<\infty.
% \end{align} Take the supremum over $i$ on both sides conclude that
% \ref{itm:mfg_condition} holds for the same $\eta^*$.

% Suppose that \ref{itm:mfg_condition} holds for $\eta^*>0$ and let
% $\eta$ be positive and strictly smaller than $\eta^*$. Write
% \begin{equation*} \bbP\rme^{\eta X_i}|X_i|^\alpha = \bbP\rme^{\eta
% X_{_i+}}X_{i+}^\alpha + \bbP\rme^{-\eta X_{i-}}X_{_i-}^\alpha.
% \end{equation*} The rightmost term is bounded because for $x>0$ the
% function $x\mapsto \rme^{-\eta x}x^\alpha$ is bounded by
% $\paren{\frac{\alpha}{\eta\rme}}^\alpha$ (the maximum is attained at
% $x^*=\frac{\alpha}{\eta}$) . Deal with the first one. Use
% \ref{itm:tail_condition} to obtain that
% \begin{align*}
%   \bbP\rme^{\eta X_{i+}}X_{i+}^\alpha
%   &=
%     \bbP\int_0^{X_{i+}}\eta\rme^{\eta x}x^\alpha + \alpha\rme^{\eta
%     x}x^{\alpha-1}\rmd x \\
%   &=
%     \int_0^\infty\prob{X_{i+}>x}(\eta\rme^{\eta
%     x}x^\alpha + \alpha\rme^{\eta x}x^{\alpha-1})\rmd x\\
%   &\leq
%     M\int_0^\infty\eta\rme^{(\eta -\eta^*)x}x^\alpha +
%     \alpha\rme^{(\eta-\eta^*)x}x^{\alpha-1}\rmd x\\
%   &=
%     M\bracks{\evat{\rme^{(\eta - \eta^*)x}x^\alpha}{0}^\infty
%     +\int^\infty_0\eta^*\rme^{(\eta - \eta^*)x}x^\alpha \rmd x} \\
%   &=  M\frac{\eta^*}{(\eta^*-\eta)^{\alpha +
%     1}}\Gamma(\alpha + 1)\\
%   &<
%     \infty
% \end{align*}


% and conclude that \ref{itm:alpha_condition} holds for
% $0<\eta<\eta*$.
% \end{proof}


\chapter{Subgamma Random Variables and Bernstein's
  Inequality\label{sect:bernstein_inequality}}

Call $\psi_X$ the cumulant generating function of a random variable
$X$, that is,
\begin{equation*}
  \psi_X(\eta) = \log \bbP[ \rme^{\eta X}].
\end{equation*}
Following \cite{boucheron_concentration_2013}, we say that a random
variable $X$ has a subgamma right tail with variance factor and scale
parameter $c$ if its cumulant generating function satisfies
\begin{equation}\label{eq:def_subgamma_tail}
  \psi_{X}(\eta) \leq \frac{1}{2}\frac{v\eta^2}{1-c\eta}
\end{equation}
for $0<c\eta<1$.

The following is Theorem 2.10 in \cite{boucheron_concentration_2013},
and it proves that a certain condition, which we call
\textit{Bernstein's moment condition}, implies that the centered
version $X-\bbP[X]$ of a (possibly unbounded) random variable $X$ has
a subgamma right tail.

\begin{theorem} [Bernstein's Inequality] \label{thm:bernstein_inequality}
  Let $X$ be a random variable. If there exists $v>0$ so that
  $\bbP X^2\leq v$ and a positive $c$ so that for each $n \geq 3$
  \begin{equation}%\label{itm:moment_condition}
    \bbP X_{+}^n\leq\frac{1}{2}n! vc^{n-2}
  \end{equation}
  then the centered random variable $X-\bbP[X]$ has a subgamma right
  tail, that is,
  \begin{equation}%\label{itm:subgamma_condition}
    \psi_{X-\bbP[X]}(\eta)\leq \frac{1}{2}\frac{v\eta^2}{1-c\eta}
  \end{equation} for $0<c\eta< 1$.
\end{theorem}

Using Cramér-Chernoff's method from Appendix
\ref{app:cramer_chernoff}, it is possible to show that if $X$ has a
subgamma right tail (it satisfies \eqref{eq:def_subgamma_tail}), then
for each $t>0$ it holds that
\begin{equation*}
  \prob{X>\sqrt{2vt} + ct}\leq \rme^{-t},
\end{equation*}
which can also be written in the form
\begin{equation*}
  \prob{X > \tau\paren{\sqrt{t} + \frac{L}{2}t }}\leq \rme^{-t}
\end{equation*}
for $\tau = \sqrt{2v}$ and $L = 2c/\sqrt{2v}$. Equivalently, by
inverting the function $x\mapsto \sqrt{t} + \frac{L}{2}t$
\begin{equation*}
  \prob{X > t}\leq \rme^{-\phi_{\text{Bern}}^L(t/\tau)}
\end{equation*}
with
\begin{equation*}
  \phi_{\mathrm{Bern}}^L(t/\tau)(t)
  = \paren{\frac{\sqrt{1 + 2Lt} - 1}{L}}^2.
\end{equation*}
Additionally, using simple properties of cumulant generating functions
it follows that if $X_1,\dots,X_n$ are iid copies of a random variable
with subgamma right tail, then it holds that
\begin{equation*}
  \prob{n^{1/2}\bbP_n[X] > t}\leq \rme^{-\phi_{\text{Bern}}^{L/\sqrt{n}}(t/\tau)}
\end{equation*}
where
\begin{equation*}
  \bbP_n[X] = \frac{1}{n}\sum_{i=1}^nX_i.
\end{equation*}


\chapter{Rates for Infinite Classes: Chaining \label{sect:chaining}}

Families of random variables that satisfy concentration inequalities
as discussed in Section \ref{sect:concentration_inequalities} can be
characterized in terms of a certain type of norms called Orlicz
norms. These norms are a generalization of $p$-norms that arises
naturally in the study of uniform integrability \citep[see de la
Vallé-Poussin's theorem in][Chapter 1]{rao_theory_1991}. We first
define these norms, then show how they have been used to study suprema
of stochastic processes for infinite families through a technique
called chaining. We only point at the main results, which are
classical. The first ideas in these direction are conventionally
attributed to the work of Kolmogorov and we point at the expositions
of \citet[Chapter 2]{van_der_vaart_weak_1996}, \citet[Section
7.2]{pollard_convergence_1984}, and the generalization called generic
chaining, by \citet[Chapter 2]{talagrand_upper_2014}. We view the
excess loss as a stochastic process $h\mapsto\varepsilon(h)$ and we
present the results in the general setting.

The main idea is to approximate the suprema of stochastic processes
using finite sets in a sequential manner. For this covering numbers
and metric entropies are used.
\begin{definition}[Covering Number and Metric Entropy]\label{def:metric_entropy}
  Let $(S,d)$ be a semimetric\footnote{Recall that a semimetric $d$
    over a set $S$ is a symmetric function $d:S\times S\to\reals^+$
    that satisfies the triangle inequality and $d(s,s)=0$ for each
    $s\in S$. Note that the fact that $d(s_1,s_2)=0$ does not imply
    that $s_1=s_2$, which is why semimetrics fail in general to be
    metrics.} space. For $\epsilon>0$ the \textit{covering number}
  $N(d,S,\epsilon)$ is the minimum amount of balls of radius
  $\epsilon$ with centers in $S$ whose union contains $S$. The
  logarithm of the covering number $\log N(d,S,\epsilon)$ is called
  \textit{metric entropy}.
\end{definition}

Let $\Phi:[0,\infty)\to [0,\infty)$ a function that is positive,
convex, increasing, and satisfies $\Phi(0) = 0$. Then we define the
$\Phi$-Orlicz semi norm  $\norm{\cdot}_{\Phi}$ as
\begin{equation}\label{eq:def_orlicz_norm}
  \norm{X}_{\Phi} = \inf\bracks{t:\bbP\Phi(\abs{X} / t) \leq 1}.
\end{equation}
By dominated convergence a random variable $X$ has finite $\Phi$-norm
if and only if $\bbP\Phi(|X|)<\infty$. Just as in the case of
$\calL^p$ spaces of $p$-integrable random variables, the space
$\calL^{\Phi}$ of random variables for which the norm
$\norm{\cdot}_{\Phi}$ is finite is a (Banach) normed space after
identification of random variables according to almost sure equality.

A bound on the Orlicz norm of a random variable implies rates of decay
for its tails. Indeed, if $\norm{X}_{\Phi}\leq \tau$, then by Markov's
inequality
\begin{equation*}
  \prob{|X|\geq t} = \prob{\Phi(|X| / \tau )\geq \Phi(t/\tau)} \leq \frac{1}{\Phi(t/\tau)}.
\end{equation*}
This means that if $\Phi$ is of the form
\begin{equation}\label{eq:phi_orlicz}
  \Phi(t) = \rme^{\phi(t)} - 1
\end{equation}
for a increasing non negative convex function $\phi$ that satisfies
$\phi(0)=0$ (see Section \ref{sect:concentration_inequalities}), then
the previous implies bounds of the form
\begin{equation*}
  \prob{|X|\geq t}\leq \rme^{-\phi(t/\tau)}.
\end{equation*}
It is also true that an inequality of this form implies a bound on the
Orlicz norm induced by a function $\Phi$ of the form of Equation
\eqref{eq:phi_orlicz} in certain situations. In those situations, Orlicz
norms characterize tail behavior of random variables. We explore two
cases. Since we are interested in studying successions of random
variables such as $E_n(h)$, we introduce the parameter $n$ in the
following definitions. With this in mind, define
\begin{equation*}%\label{eq:def_phi_p}
  \Phi_{p}(t) = \rme^{t^p}-1 = \rme^{\phi_p(t)}-1
\end{equation*}
and
\begin{equation*}%\label{eq:def_phi_bern}
  \Phi_{\text{Bern}}^L(t) = \rme^{\paren{\frac{\sqrt{1 + 2Lt} - 1}{L}}^2}-1 = \rme^{\phi_{\text{Bern}}^L(t)}-1
\end{equation*}
The converse holds true for $\phi_p$ \citep[Lemma
2.2.1]{van_der_vaart_weak_1996} and for $\phi_{\text{Bern}}^L$
\citep[Lemma 2]{van_de_geer_bernsteinorlicz_2013}. We write down the
results in the following lemma
\begin{lemma}\label{lem:tail_and_orlicz}
  Let $\{X_n\}$ be a succession of random variables. The following
  holds.
  \begin{itemize}
  \item If $\prob{n^{1/p}|X_n|\geq t}\leq C\rme^{-\phi_p(t/\tau)}$, then
    $\norm{n^{1/p}X_n}_{\Phi_p}\leq (1+C)^{1/p}\tau$
  \item
    $\prob{n^{1/2}|X_n|\geq t}\leq 2\rme^{-\phi_{\text{Bern}}^{L/\sqrt{n}}(t/\tau)}$,
    then $\norm{n^{1/2}X_n}_{\Phi_{\text{Bern}}^{\sqrt{3}L/\sqrt{n}}}\leq \sqrt{3}\tau$
  \end{itemize}
\end{lemma}

If $X_1,\dots,X_N$ is a collection of random variables for which
$\norm{X_i}_{\Phi}\leq \tau$. It is easy to bound the expectation of
$\max_{i}X_i$ through Jensen's inequality
\begin{equation*}
  \Phi(\bbP[\max_{i}|X_i|/ \tau]) \leq \bbP[\max_{i}\Phi(|X_i| / \tau)]\leq
  \sum_i \bbP[\Phi(|X_i| / \tau)] \leq N
\end{equation*}
so that
\begin{equation*}
  \bbP[\max_{i}|X_i|] \leq \tau \Phi^{-1}(N).
\end{equation*}
On the other hand, with a little more work, it is possible to prove
that if there is a constant $c$ such that $\Phi(x)\Phi(y)\Phi(cxy)$
remains bounded as $x,y\to\infty$ (which is the case for $\Phi_p$ and
$\Phi_{\text{Bern}}^L$) then
\begin{equation*}
  \norm{\max_{i}\Phi(|X_i| / \tau)}_{\Phi}\lesssim \tau\Phi^{-1}(N).
\end{equation*}

Consider now an uncountable family of random variables
$\{X_t\}_{t\in T}$. Through a series successive approximations in
finite subsets of $T$ the last equations can be used to obtain an
bound for $\norm{\sup_{t\in T}|X_t|}_{\Phi}$.
\begin{theorem}\label{thm:chaining}
  Let $\Phi$ be a convex, non decreasing, nonzero function with
  $\Phi(0)$ and
  $$\limsup_{x,y\to\infty}\Phi(x)\Phi(y)/\Phi(cxy)<\infty$$ for some
  constant $c$. Let $\{X_t\}_{t\in T}$ be a separable stochastic
  process on the semi metric space $(T,d)$ with
  \begin{equation*}
    \norm{X_s - X_t}_{\Phi}\leq Cd(s,t)
  \end{equation*}
  for every $s,t\in T$ and a constant
  $C$. Then there exists a constant $K$ depending only on $\Phi$ such
  that for $t_0\in T$
  \begin{equation*}
    \norm{\sup_t|X_t|}_{\Phi} \leq \norm{X_{t_0}}_{\Phi} +
    K\int_{0}^{\mathrm{diam} T}\Phi^{-1}(N(d, T, \epsilon)) \rmd \epsilon
  \end{equation*}
  where $\mathrm{diam}T$ is the diameter of $T$.
\end{theorem}

The following lemma will be useful.

\begin{lemma}\label{lem:lipschitz_orlicz}
  let $(T,d)$ be a semi metric space and $\{X_t\}_{t\in T}$ a
  stochastic process with almost surely Lipschitz continuous paths,
  that is,
  \begin{equation*}
    |X_s - X_t|\leq Ld(s,t)
  \end{equation*}
  almost surely for every $s,t\in T$. If $\norm{X_t}_{\Phi}<\infty$
  for each $t$, then
  \begin{equation*}
    \norm{X_s - X_t}_{\Phi}\leq Cd(s,t).
  \end{equation*}
  for some constant $C>0$.
\end{lemma}
\begin{proof}
  It is easy to check from the definition of the Orlicz norm
  $\norm{\cdot}_{\Phi}$ (see Equation \eqref{eq:def_orlicz_norm}) that
  \begin{align*}
    \norm{X_s - X_t}_{\Phi}
    &= \inf\bracks{K:\bbP[\Phi(|X_s - X_t|/K)]\leq 1}\\
    &\leq \inf\bracks{K:\Phi(Ld(s,t)/K)\leq 1}\\
    &= \frac{L}{\Phi^{-1}(1)}d(s,t)
  \end{align*}
  so that taking $C = L/\Phi^{-1}(1)$ we obtain the
  result.
\end{proof}

\begin{corollary}\label{thm:special_chaining}
  Let $\{X\}_{t\in T}$ be a stochastic process. Assume that there is a
  semi metric $d$ on $T$ and that $X$ has almost surely Lipschitz
  continuous paths. Assume that $X$ satisfies an inequality
  of the form
  \begin{equation}
    \prob{n^{1/p}|X| \geq  t } \lesssim \rme^{-n\phi(t/\tau)}
  \end{equation}
  as in Lemma \ref{lem:tail_and_orlicz}. Assume that there is a $t_o$
  such that $X_{t_0}=0$ almost surely and define
  $\Phi(x) = \rme^{\phi(x)} -1$. In the case that
  $\phi = \phi_{\text{Bern}}^{L}$ then
  \begin{equation*}
    \norm{n^{1/2}\sup_{t\in T}|X_t|}_{\Phi_{\text{Bern}}^{L/\sqrt{n}}}
    \lesssim
    \int_{0}^{\mathrm{diam} \calH} \paren{\phi_{\text{Bern}}^{L/\sqrt{n}}}^{-1}
    \paren{\log(N(d, \calH, \epsilon) + 1)} \rmd \epsilon.
  \end{equation*}
  In the case that $\phi=\phi_p$, then
  \begin{equation*}
    \norm{n^{1/p}\sup_{t\in T}|X_t|}_{\Phi_p}
    \lesssim
    \int_{0}^{\mathrm{diam} \calH} \paren{\phi_p}^{-1}
    \paren{\log(N(d, \calH, \epsilon) + 1)} \rmd \epsilon.
  \end{equation*}
\end{corollary}
\begin{proof}
  We prove the case for $\phi_p$, the other case is similar.  From
  Lemma \ref{lem:tail_and_orlicz} it follows that
  \begin{equation*}
    \norm{n^{1/p}|X|}_{\Phi_p}\lesssim\tau
  \end{equation*}
  for each $h\in\calH$. With this in mind, from Lemma
  \ref{lem:lipschitz_orlicz} it follows that
  \begin{equation*}
    \norm{n^{1/p}(X_t-X_s)}_{\Phi_n}\leq Cd(h_1,h_2)
  \end{equation*}
  for each $s,t\in T$ and some constant $C>0$. Thus, we can apply
  Theorem \ref{thm:chaining} by noticing that that the Lipschitz
  continuity of $t\mapsto X_t$ makes it separable.
\end{proof}

This implies rates of order $n^{-1/p}$ for $\phi_p$ and of order
$n^{-1/2}$ for $\phi_{\text{Bern}}^L$ as long as the integral
converges. This is the case for $\phi_p$ because, if all the
conditions in the previous theorem are satisfied, the Orlicz norm
\begin{equation*}
  \norm{n^{1/p}\sup_{t\in T}|X_t|}_{\Phi_p} \leq \tau
\end{equation*}
for some finite $\tau>0$ involving the integral of the metric entropy
given that it converges. Then, by our previous considerations
\begin{equation*}
  \prob{n^{1/p}\sup_{t\in T}|X_t|\geq t}\leq \rme^{-\phi_p(t/\tau)} =
\rme^{-(t/\tau)^{p}},
\end{equation*}
which means that with probability higher than $1-\delta$ the
inequality
\begin{equation*}
  \sup_{t\in T}|X_t| \leq \frac{\tau}{n^{1/p}}\paren{\log\frac{1}{\delta}}^{1/p}
\end{equation*}
holds, a  $n^{-1/p}$ rate. Similarly, a $n^{-1/2}$ rate holds for
$\phi_{\text{Bern}}^L$.

% \subsection{Rates Based on Symmetrization}
% Given that the two-sided empirical process converges to zero, that is,
% \begin{equation}
%   \sup_{h\in\calH}\abs{L(h) - L_n(h)} \to 0
% \end{equation}
% the study of its rate of convergence also gives rates of convergence
% for the expected excess loss. This is consequece of the straighforward
% inequality
% \begin{equation}\label{eq:ep_geq_regret}
%   L(h_n^*) - \inf_{h\in H} L(h) \leq 2\sup_{h\in\calH}\abs{L(h) - L_n(h)},
% \end{equation}
% (See Lemma 8.2 in \citet{gyorfi_probabilistic_1996}), which holds true
% when the expected loss is bounded, that is, when
% \begin{equation}
%   \sup_{h\in\calH}|L(h)| < \infty.
% \end{equation}
% This means that even though The Key Theorem (See Theorem
% \ref{thm:key_theorem}) does not provide tools for obtaining rates of
% convergence of the expected excess loss, the study of two sided
% empirical processes does.

% The key observation is that because of the two-sidedness of the
% empirical process and the iid, there is a lot of simmetry to be
% exploited. If we denote $L'_n(h)$ the empirical loss calculated with a
% random iid sample $Z'_1,\dots,Z'_n$ independent of $Z_1,\dots,Z_n$
% (which is used to calculate $L_n(h)$) then it is possible to prove
% \citep[See][Chapter 2, Theorem 8]{pollard_convergence_1984} that
% \begin{theorem}[Symmetrization Lemma]
%   Let $\delta>0$. Suppose that for all $h\in\calH$
%   \begin{equation}
%     \prob{|L(h)- L_n(h)|>\delta/2}\leq \frac{1}{2}
%   \end{equation}
% Then
% \begin{equation}
%   \prob{\sup_{h\in\calH}\abs{L(h) - L_n(h)}>\delta}\leq
%   2\prob{\sup_{h\in\calH}\abs{L'_n(h) - L_n(h)}>\frac{\delta}{2}}
% \end{equation}
% \end{theorem}

% This symmetrization lemma has as a corollary that,
% \begin{equation}
%   \prob{\sup_{h\in\calH}\abs{L(h) - L_n(h)}>\delta}\leq
%   4\prob{\sup_{h\in\calH}\abs{L^\sigma_n(h)}>\frac{\delta}{4}}
% \end{equation}
% where the $L^{\sigma}_n(h)$ is the
% simmetrized empirical average
% \begin{equation}
%   L^{\sigma}_n(h) = \frac{1}{n}\sum_{i=1}^{n}\sigma_i\ell_i(h)
% \end{equation}
% and $\sigma_1,\dots,\sigma_n$ are independent (Rademacher) random
% variables that take the values $+1$ or $-1$ with equal probability.

% \citet{vapnik_necessary_1991} gave a distribution-free characterization
% of the the convergence of the two-sided empirical process in the case
% that the loss $\ell$ is uniformly bounded over both $\calZ$ and
% $\calH$. The result is based on their celebrated inequality, which
% states that for any $\epsilon>0$
% \begin{equation}\label{eq:vc_inequality}
%   \prob{\sup_{h\in\calH}\abs{L(h) - L_n(h)} \geq \epsilon}\leq
%   8(n+1)^V\rme^{-n\epsilon^2 /32},
% \end{equation}
% where $V$ is the so-called Vapnik-Chervonenkis (VC) dimension of the
% hypothesis class. $V$ depends only on combinatorial properties of the
% hypotheses class $\calH$, making the bound independent of the data
% distribution $\bbP$. Thus the finiteness of $V$ is sufficient for the
% uniform convergence of the empirical process and thus for the
% consistency of ERM. In the case that $V$ is infinite it is possible to
% build distributions for which the empirical process does not converge
% to zero in the so called No Free Lunch Theorem (See also
% \citet{gyorfi_probabilistic_1996}).

% Note that together with Equation \eqref{eq:ep_geq_regret}, Equation
% \eqref{eq:vc_inequality} leads to the conclusion that if the VC
% diminesion of the Hypotheses class $\calH$ is finite, then the regret
% converges to zero at a
% \begin{equation}
%   \paren{\frac{V\log n}{n}}^{1/2}
% \end{equation}
% rate. Nevertheless faster rates are possible as it is shown in the
% next example.


\end{appendices}


\chapter*{Popular Summary}
\addcontentsline{toc}{chapter}{Popular Summary}

Many problems in statistics, pattern recognition, and machine learning
can be formulated in the following way. We are given a set of data
$Z_1,\dots,Z_n$ in which each data point is produced by a unique
mechanism independently from the rest. We have a set of hypotheses
$\calH$ from which we want to choose one. And we have a notion of what
it means for a decision to be bad given a data point, encoded on a
function $\ell$, called \textit{loss function}. This function depends
on which data point we observe and which hypothesis we choose, that
is, $\ell = \ell(h,Z)$. The goal is then to choose a hypotheses such
that the expected loss $L(h)$ (over the data) is small. Examples of
these problems include prototypical machine learning and statistical
problems such as classification, regression and density estimation. In
these problems, hypotheses with low expected losses are better at
making predictions on future data.

The expected loss of each hypothesis is, however, usually
unknown. Perhaps the most intuitive idea is to minimize an empirical
estimate of it. In order to estimate the expected loss using the data,
one can use the empirical average $\hat{L}_n(h)$ of the loss function
(over the data)
\begin{equation*}
  \hat{L}_n(h) = \frac{1}{n}\sum_{i=1}^n\ell(h,Z_i).
\end{equation*}
We then choose an element $h^*_n$ that minimizes $\hat{L}_n(h)$ and
use it as our best guess. This technique is called Empirical Risk
Minimization and, remarkably, it has proven to be very useful. Since
also the lowest expected loss is unknown, for any estimate $\hat{h}_n$
the main quantity of interest is how far it is from optimal, that is,
\begin{equation*}
  L(\hat{h}_n) - \inf_{h'\in\calH}L(h'),
\end{equation*}
called expected excess loss. Notably, and in agreement with our
intuition, choosing elements $\hat{h}_n$ based on data that have a
small $\hat{L}_n(\hat{h}_n)$ can and does (in many situations) lead to
small values of the expected excess loss.

In this work we investigated how the expected excess loss of
data-based estimates $\hat{h}_n$ decreases to zero for different
choices of $\hat{h}_n$ as the number of data points $n$ increases. In
that case, it is known that under weak conditions the expected excess
loss decreases at a rate of $n^{-1/2}$, which we called \textit{slow
  rate}. We considered situations in which this rate can be improved
to be $n^{-1}$, which we called \textit{fast rate}. These conditions
are well known when losses are bounded; we investigated them in the
unbounded case. Achieving fast rates means that for decreasing the
expected excess loss by a factor of 100, $\sim 100$ times more data
points are needed, while $\sim 10000$ times more are needed in the
slow case.





\bibliographystyle{plainnat}
\bibliography{Thesis}

%\printindex




\end{document}
