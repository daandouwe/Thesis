
@inproceedings{pollard_empirical_1990,
	title = {Empirical processes: theory and applications},
	shorttitle = {Empirical processes},
	booktitle = {{NSF}-{CBMS} regional conference series in probability and statistics},
	publisher = {JSTOR},
	author = {Pollard, David},
	year = {1990},
	pages = {i--86},
	file = {Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/WRJRHDHR/4153175.html:text/html}
}

@book{pollard_convergence_1984,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Convergence of {Stochastic} {Processes}},
	isbn = {978-1-4612-9758-1},
	url = {//www.springer.com/us/book/9781461297581},
	abstract = {A more accurate title for this book might be: An Exposition of Selected Parts of Empirical Process Theory, With Related Interesting Facts About Weak Convergence, and Applications to Mathematical Statistics. The high points are Chapters II and VII, which describe some of the developments inspired by Richard Dudley's 1978 paper. There I explain the combinatorial ideas and approximation methods that are needed to prove maximal inequalities for empirical processes indexed by classes of sets or classes of functions. The material is somewhat arbitrarily divided into results used to prove consistency theorems and results used to prove central limit theorems. This has allowed me to put the easier material in Chapter II, with the hope of enticing the casual reader to delve deeper. Chapters III through VI deal with more classical material, as seen from a different perspective. The novelties are: convergence for measures that don't live on borel a-fields; the joys of working with the uniform metric on D[O, IJ; and finite-dimensional approximation as the unifying idea behind weak convergence. Uniform tightness reappears in disguise as a condition that justifies the finite-dimensional approximation. Only later is it exploited as a method for proving the existence of limit distributions. The last chapter has a heuristic flavor. I didn't want to confuse the martingale issues with the martingale facts.},
	language = {en},
	urldate = {2018-02-26},
	publisher = {Springer-Verlag},
	author = {Pollard, D.},
	year = {1984},
	file = {Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/HW33L3DS/9781461297581.html:text/html}
}

@article{van_erven_fast_2015,
	title = {Fast rates in statistical and online learning.},
	volume = {16},
	journal = {Journal of Machine Learning Research},
	author = {Van Erven, Tim and Grünwald, Peter D. and Mehta, Nishant A. and Reid, Mark D. and Williamson, Robert C.},
	year = {2015},
	pages = {1793--1861},
	file = {Fulltext:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/K8GSGMTE/Van Erven et al. - 2015 - Fast rates in statistical and online learning..pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/P79ZWHV7/Van Erven et al. - 2015 - Fast rates in statistical and online learning..pdf:application/pdf}
}

@article{grunwald_fast_2016,
	title = {Fast {Rates} for {General} {Unbounded} {Loss} {Functions}: from {ERM} to {Generalized} {Bayes}},
	shorttitle = {Fast {Rates} for {General} {Unbounded} {Loss} {Functions}},
	url = {http://arxiv.org/abs/1605.00252},
	abstract = {We present new excess risk bounds for general unbounded loss functions including log loss and squared loss, where the distribution of the losses may be heavy-tailed. The bounds hold for general estimators, but they are optimized when applied to \${\textbackslash}eta\$-generalized Bayesian, MDL, and ERM estimators. When applied with log loss, the bounds imply convergence rates for generalized Bayesian inference under misspecification in terms of a generalization of the Hellinger metric as long as the learning rate \${\textbackslash}eta\$ is set correctly. For general loss functions, our bounds rely on two separate conditions: the \$v\$-GRIP (generalized reversed information projection) conditions, which control the lower tail of the excess loss; and the newly introduced witness condition, which controls the upper tail. The parameter \$v\$ in the \$v\$-GRIP conditions determines the achievable rate and is akin to the exponent in the well-known Tsybakov margin condition and the Bernstein condition for bounded losses, which the \$v\$-GRIP conditions generalize; favorable \$v\$ in combination with small model complexity leads to \${\textbackslash}tilde\{O\}(1/n)\$ rates. The witness condition allows us to connect the excess risk to an 'annealed' version thereof, by which we generalize several previous results connecting Hellinger and R{\textbackslash}'enyi divergence to KL divergence.},
	urldate = {2018-02-26},
	journal = {arXiv:1605.00252 [cs, stat]},
	author = {Grünwald, Peter D. and Mehta, Nishant A.},
	month = may,
	year = {2016},
	note = {arXiv: 1605.00252},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	annote = {Comment: 70 pages},
	file = {arXiv\:1605.00252 PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/WZEXKSPR/Grünwald y Mehta - 2016 - Fast Rates for General Unbounded Loss Functions f.pdf:application/pdf;arXiv.org Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/DGZHS3BS/1605.html:text/html}
}

@book{gyorfi_probabilistic_1996,
	title = {A probabilistic theory of pattern recognition},
	publisher = {Springer-Verlag},
	author = {Gyorfi, L. Devroye L. and Lugosi, Gabor and Devroye, L.},
	year = {1996},
	file = {Fulltext:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/5JB433AB/Gyorfi et al. - 1996 - A probabilistic theory of pattern recognition.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/4G64KKXE/Gyorfi et al. - 1996 - A probabilistic theory of pattern recognition.pdf:application/pdf}
}

@inproceedings{vapnik_uniform_1968,
	title = {On the uniform convergence of relative frequencies of events to their probabilities},
	volume = {9},
	booktitle = {Soviet {Math}. {Dokl}},
	author = {Vapnik, Vladimir N. and Chervonenkis, Alexey Ya},
	year = {1968},
	pages = {915--918}
}

@article{grunwald_tight_2017,
	title = {A {Tight} {Excess} {Risk} {Bound} via a {Unified} {PAC}-{Bayesian}-{Rademacher}-{Shtarkov}-{MDL} {Complexity}},
	url = {http://arxiv.org/abs/1710.07732},
	abstract = {We present a novel notion of complexity that interpolates between and generalizes some classic existing complexity notions in learning theory: for estimators like empirical risk minimization (ERM) with arbitrary bounded losses, it is upper bounded in terms of data-independent Rademacher complexity; for generalized Bayesian estimators, it is upper bounded by the data-dependent information complexity (also known as stochastic or PAC-Bayesian, \${\textbackslash}mathrm\{KL\}({\textbackslash}text\{posterior\} {\textbackslash}operatorname\{{\textbackslash}{\textbar}\} {\textbackslash}text\{prior\})\$ complexity. For (penalized) ERM, the new complexity reduces to (generalized) normalized maximum likelihood (NML) complexity, i.e. a minimax log-loss individual-sequence regret. Our first main result bounds excess risk in terms of the new complexity. Our second main result links the new complexity via Rademacher complexity to \$L\_2(P)\$ entropy, thereby generalizing earlier results of Opper, Haussler, Lugosi, and Cesa-Bianchi who did the log-loss case with \$L\_{\textbackslash}infty\$. Together, these results recover optimal bounds for VC- and large (polynomial entropy) classes, replacing localized Rademacher complexity by a simpler analysis which almost completely separates the two aspects that determine the achievable rates: 'easiness' (Bernstein) conditions and model complexity.},
	urldate = {2018-03-19},
	journal = {arXiv:1710.07732 [cs, stat]},
	author = {Grünwald, Peter D. and Mehta, Nishant A.},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.07732},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	annote = {Comment: 38 pages},
	file = {arXiv\:1710.07732 PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/UV33CQQU/Grünwald y Mehta - 2017 - A Tight Excess Risk Bound via a Unified PAC-Bayesi.pdf:application/pdf;arXiv.org Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/LWVEAWWJ/1710.html:text/html}
}

@book{van_der_vaart_weak_1996,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Weak {Convergence} and {Empirical} {Processes}: {With} {Applications} to {Statistics}},
	isbn = {978-0-387-94640-5},
	shorttitle = {Weak {Convergence} and {Empirical} {Processes}},
	url = {//www.springer.com/la/book/9780387946405},
	abstract = {This book tries to do three things. The first goal is to give an exposition of certain modes of stochastic convergence, in particular convergence in distribution. The classical theory of this subject was developed mostly in the 1950s and is well summarized in Billingsley (1968). During the last 15 years, the need for a more general theory allowing random elements that are not Borel measurable has become well established, particularly in developing the theory of empirical processes. Part 1 of the book, Stochastic Convergence, gives an exposition of such a theory following the ideas of J. Hoffmann-J!1Jrgensen and R. M. Dudley. A second goal is to use the weak convergence theory background devel­ oped in Part 1 to present an account of major components of the modern theory of empirical processes indexed by classes of sets and functions. The weak convergence theory developed in Part 1 is important for this, simply because the empirical processes studied in Part 2, Empirical Processes, are naturally viewed as taking values in nonseparable Banach spaces, even in the most elementary cases, and are typically not Borel measurable. Much of the theory presented in Part 2 has previously been scattered in the journal literature and has, as a result, been accessible only to a relatively small number of specialists. In view of the importance of this theory for statis­ tics, we hope that the presentation given here will make this theory more accessible to statisticians as well as to probabilists interested in statistical applications.},
	language = {en},
	urldate = {2018-05-03},
	publisher = {Springer-Verlag},
	author = {Van der Vaart, A. W. and Wellner, Jon},
	year = {1996},
	file = {Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/FWCCWHXD/9780387946405.html:text/html}
}

@book{boucheron_concentration_2013,
	title = {Concentration {Inequalities}: {A} {Nonasymptotic} {Theory} of {Independence}},
	isbn = {978-0-19-953525-5},
	shorttitle = {Concentration {Inequalities}},
	abstract = {Concentration inequalities for functions of independent random variables is an area of probability theory that has witnessed a great revolution in the last few decades, and has applications in a wide variety of areas such as machine learning, statistics, discrete mathematics, and high-dimensional geometry. Roughly speaking, if a function of many independent random variables does not depend too much on any of the variables then it is concentrated in the sense that with high probability, it is close to its expected value. This book offers a host of inequalities to illustrate this rich theory in an accessible way by covering the key developments and applications in the field. The authors describe the interplay between the probabilistic structure (independence) and a variety of tools ranging from functional inequalities to transportation arguments to information theory. Applications to the study of empirical processes, random projections, random matrix theory, and threshold phenomena are also presented. A self-contained introduction to concentration inequalities, it includes a survey of concentration of sums of independent random variables, variance bounds, the entropy method, and the transportation method. Deep connections with isoperimetric problems are revealed whilst special attention is paid to applications to the supremum of empirical processes. Written by leading experts in the field and containing extensive exercise sections this book will be an invaluable resource for researchers and graduate students in mathematics, theoretical computer science, and engineering.},
	language = {en},
	publisher = {OUP Oxford},
	author = {Boucheron, Stéphane and Lugosi, Gábor and Massart, Pascal},
	month = feb,
	year = {2013},
	note = {Google-Books-ID: koNqWRluhP0C},
	keywords = {Mathematics / Discrete Mathematics, Mathematics / Probability \& Statistics / General, Technology \& Engineering / Engineering (General)}
}

@article{bartlett_empirical_2006,
	title = {Empirical minimization},
	volume = {135},
	issn = {0178-8051, 1432-2064},
	url = {https://link.springer.com/article/10.1007/s00440-005-0462-3},
	doi = {10.1007/s00440-005-0462-3},
	abstract = {We investigate the behavior of the empirical minimization algorithm using various methods. We first analyze it by comparing the empirical, random, structure and the original one on the class, either in an additive sense, via the uniform law of large numbers, or in a multiplicative sense, using isomorphic coordinate projections. We then show that a direct analysis of the empirical minimization algorithm yields a significantly better bound, and that the estimates we obtain are essentially sharp. The method of proof we use is based on Talagrand's concentration inequality for empirical processes.},
	language = {en},
	number = {3},
	urldate = {2018-05-12},
	journal = {Probability Theory and Related Fields},
	author = {Bartlett, Peter L. and Mendelson, Shahar},
	month = jul,
	year = {2006},
	pages = {311--334},
	file = {Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/DSBYIEUX/s00440-005-0462-3.html:text/html}
}

@article{van_de_geer_bernsteinorlicz_2013,
	title = {The {Bernstein}–{Orlicz} norm and deviation inequalities},
	volume = {157},
	issn = {0178-8051, 1432-2064},
	url = {https://link.springer.com/article/10.1007/s00440-012-0455-y},
	doi = {10.1007/s00440-012-0455-y},
	abstract = {We introduce two new concepts designed for the study of empirical processes. First, we introduce a new Orlicz norm which we call the Bernstein–Orlicz norm. This new norm interpolates sub-Gaussian and sub-exponential tail behavior. In particular, we show how this norm can be used to simplify the derivation of deviation inequalities for suprema of collections of random variables. Secondly, we introduce chaining and generic chaining along a tree. These simplify the well-known concepts of chaining and generic chaining. The supremum of the empirical process is then studied as a special case. We show that chaining along a tree can be done using entropy with bracketing. Finally, we establish a deviation inequality for the empirical process for the unbounded case.},
	language = {en},
	number = {1-2},
	urldate = {2018-05-12},
	journal = {Probability Theory and Related Fields},
	author = {Van de Geer, Sara and Lederer, Johannes},
	month = oct,
	year = {2013},
	pages = {225--250},
	file = {Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/8FD67SMC/s00440-012-0455-y.html:text/html}
}

@article{zhang_e-entropy_2006,
	title = {From ɛ-entropy to {KL}-entropy: {Analysis} of minimum information complexity density estimation},
	volume = {34},
	issn = {0090-5364, 2168-8966},
	shorttitle = {From ɛ-entropy to {KL}-entropy},
	url = {https://projecteuclid.org/euclid.aos/1169571794},
	doi = {10.1214/009053606000000704},
	abstract = {We consider an extension of ɛ-entropy to a KL-divergence based complexity measure for randomized density estimation methods. Based on this extension, we develop a general information-theoretical inequality that measures the statistical complexity of some deterministic and randomized density estimators. Consequences of the new inequality will be presented. In particular, we show that this technique can lead to improvements of some classical results concerning the convergence of minimum description length and Bayesian posterior distributions. Moreover, we are able to derive clean finite-sample convergence bounds that are not obtainable using previous approaches.},
	language = {en},
	number = {5},
	urldate = {2018-05-14},
	journal = {The Annals of Statistics},
	author = {Zhang, Tong},
	month = oct,
	year = {2006},
	mrnumber = {MR2291497},
	zmnumber = {1106.62005},
	keywords = {Bayesian posterior distribution, density estimation, minimum description length},
	pages = {2180--2210},
	file = {Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/9XZYY99R/1169571794.html:text/html}
}

@article{zhang_information-theoretic_2006,
	title = {Information-theoretic upper and lower bounds for statistical estimation},
	volume = {52},
	issn = {0018-9448},
	doi = {10.1109/TIT.2005.864439},
	abstract = {In this paper, we establish upper and lower bounds for some statistical estimation problems through concise information-theoretic arguments. Our upper bound analysis is based on a simple yet general inequality which we call the information exponential inequality. We show that this inequality naturally leads to a general randomized estimation method, for which performance upper bounds can be obtained. The lower bounds, applicable for all statistical estimators, are obtained by original applications of some well known information-theoretic inequalities, and approximately match the obtained upper bounds for various important problems. Moreover, our framework can be regarded as a natural generalization of the standard minimax framework, in that we allow the performance of the estimator to vary for different possible underlying distributions according to a predefined prior},
	number = {4},
	journal = {IEEE Transactions on Information Theory},
	author = {Zhang, Tong},
	month = apr,
	year = {2006},
	keywords = {Additives, Bayesian methods, general randomized estimation method, Gibbs algorithm, Helium, Information analysis, information theory, information-theoretic inequality, lower bound, lower bound analysis, minimax, minimax techniques, Minimax techniques, PAC-Bayes, Pattern recognition, Probability, random processes, Random variables, randomized estimatin, standard minimax framework, statistical analysis, statistical estimation, Statistical learning, Upper bound, upper bound analysis},
	pages = {1307--1321},
	file = {IEEE Xplore Abstract Record:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/PX7NVTZJ/1614067.html:text/html;IEEE Xplore Full Text PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/IXBW43LY/Zhang - 2006 - Information-theoretic upper and lower bounds for s.pdf:application/pdf}
}

@article{tsybakov_optimal_2004,
	title = {Optimal aggregation of classifiers in statistical learning},
	volume = {32},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1079120131},
	doi = {10.1214/aos/1079120131},
	abstract = {Classification can be considered as nonparametric estimation of sets, where the risk is defined by means of a specific distance between sets associated with misclassification error. It is shown that the rates of convergence of classifiers depend on two parameters: the complexity of the class of candidate sets and the margin parameter. The dependence is explicitly given, indicating that optimal fast rates approaching O(n−1)O(n−1)O(n{\textasciicircum}\{-1\}) can be attained, where n is the sample size, and that the proposed classifiers have the property of robustness to the margin. The main result of the paper concerns optimal aggregation of classifiers: we suggest a classifier that automatically adapts both to the complexity and to the margin, and attains the optimal fast rates, up to a logarithmic factor.},
	language = {en},
	number = {1},
	urldate = {2018-05-28},
	journal = {The Annals of Statistics},
	author = {Tsybakov, Alexander B.},
	month = feb,
	year = {2004},
	mrnumber = {MR2051002},
	zmnumber = {1105.62353},
	keywords = {aggregation of classifiers, Classification, complexity of classes of sets, empirical processes, margins, optimal rates, statistical learning},
	pages = {135--166},
	file = {Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/D9V8Z9Q9/1079120131.html:text/html}
}

@article{vapnik_overview_1999,
	title = {An overview of statistical learning theory},
	volume = {10},
	number = {5},
	journal = {IEEE transactions on neural networks},
	author = {Vapnik, Vladimir Naumovich},
	year = {1999},
	pages = {988--999},
	file = {Fulltext:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/KB46SLRP/Vapnik - 1999 - An overview of statistical learning theory.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/6ART8XSD/788640.html:text/html}
}

@book{vapnik_nature_2000,
	address = {New York},
	edition = {2},
	series = {Information {Science} and {Statistics}},
	title = {The {Nature} of {Statistical} {Learning} {Theory}},
	isbn = {978-0-387-98780-4},
	url = {//www.springer.com/br/book/9780387987804},
	abstract = {The aim of this book is to discuss the fundamental ideas which lie behind the statistical theory of learning and generalization. It considers learning as a general problem of function estimation based on empirical data. Omitting proofs and technical details, the author concentrates on discussing the main results of learning theory and their connections to fundamental problems in statistics. These include: * the setting of learning problems based on the model of minimizing the risk functional from empirical data * a comprehensive analysis of the empirical risk minimization principle including necessary and sufficient conditions for its consistency * non-asymptotic bounds for the risk achieved using the empirical risk minimization principle * principles for controlling the generalization ability of learning machines using small sample sizes based on these bounds * the Support Vector methods that control the generalization ability when estimating function using small sample size. The second edition of the book contains three new chapters devoted to further development of the learning theory and SVM techniques. These include: * the theory of direct method of learning based on solving multidimensional integral equations for density, conditional probability, and conditional density estimation * a new inductive principle of learning. Written in a readable and concise style, the book is intended for statisticians, mathematicians, physicists, and computer scientists. Vladimir N. Vapnik is Technology Leader AT\&T Labs-Research and Professor of London University. He is one of the founders of},
	language = {en},
	urldate = {2018-05-28},
	publisher = {Springer-Verlag},
	author = {Vapnik, Vladimir},
	year = {2000},
	file = {Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/NTDW3IH8/9780387987804.html:text/html}
}

@article{vapnik_necessary_1991,
	title = {The necessary and sufficient conditions for consistency of the method of empirical risk},
	volume = {1},
	number = {3},
	journal = {Pattern Recognition and Image Analysis},
	author = {Vapnik, Vladimir N. and Chervonenkis, A. Ja},
	year = {1991},
	pages = {284--305},
	file = {Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/AURZJ24A/Vapnik y Chervonenkis - 1991 - The necessary and sufficient conditions for consis:}
}

@article{cortes_support-vector_1995,
	title = {Support-vector networks},
	volume = {20},
	number = {3},
	journal = {Machine learning},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	year = {1995},
	pages = {273--297},
	file = {Fulltext:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/UKKX2KQN/Cortes y Vapnik - 1995 - Support-vector networks.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/EF52NL7A/BF00994018.html:text/html}
}

@article{breiman_random_2001,
	title = {Random forests},
	volume = {45},
	number = {1},
	journal = {Machine learning},
	author = {Breiman, Leo},
	year = {2001},
	pages = {5--32},
	file = {Fulltext:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/M2ZIBLAA/Breiman - 2001 - Random forests.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/8BZA8AXT/A1010933404324.html:text/html}
}

@article{grunwald_inconsistency_2017,
	title = {Inconsistency of {Bayesian} inference for misspecified linear models, and a proposal for repairing it},
	volume = {12},
	number = {4},
	journal = {Bayesian Analysis},
	author = {Grünwald, Peter and Van Ommen, Thijs},
	year = {2017},
	pages = {1069--1103},
	file = {Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/FI5B8FKY/1510974325.html:text/html}
}

@inproceedings{vovk_aggregating_1990,
	address = {San Francisco, CA, USA},
	series = {{COLT} '90},
	title = {Aggregating {Strategies}},
	isbn = {978-1-55860-146-8},
	url = {http://dl.acm.org/citation.cfm?id=92571.92672},
	urldate = {2018-05-28},
	booktitle = {Proceedings of the {Third} {Annual} {Workshop} on {Computational} {Learning} {Theory}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Vovk, Volodimir G.},
	year = {1990},
	pages = {371--386}
}

@article{audibert_fast_2009,
	title = {Fast learning rates in statistical inference through aggregation},
	volume = {37},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1245332827},
	doi = {10.1214/08-AOS623},
	abstract = {We develop minimax optimal risk bounds for the general learning task consisting in predicting as well as the best function in a reference set up to the smallest possible additive term, called the convergence rate. When the reference set is finite and when n denotes the size of the training data, we provide minimax convergence rates of the form with tight evaluation of the positive constant C and with exact 0{\textless}v≤1, the latter value depending on the convexity of the loss function and on the level of noise in the output distribution. The risk upper bounds are based on a sequential randomized algorithm, which at each step concentrates on functions having both low risk and low variance with respect to the previous step prediction function. Our analysis puts forward the links between the probabilistic and worst-case viewpoints, and allows to obtain risk bounds unachievable with the standard statistical learning approach. One of the key ideas of this work is to use probabilistic inequalities with respect to appropriate (Gibbs) distributions on the prediction function space instead of using them with respect to the distribution generating the data. The risk lower bounds are based on refinements of the Assouad lemma taking particularly into account the properties of the loss function. Our key example to illustrate the upper and lower bounds is to consider the Lq-regression setting for which an exhaustive analysis of the convergence rates is given while q ranges in [1; +∞[.},
	language = {EN},
	number = {4},
	urldate = {2018-06-27},
	journal = {The Annals of Statistics},
	author = {Audibert, Jean-Yves},
	month = aug,
	year = {2009},
	mrnumber = {MR2533466},
	zmnumber = {1360.62167},
	keywords = {Statistical learning, aggregation, convex loss, excess risk, fast rates of convergence, L\_q-regression, lower bounds in VC-classes, minimax lower bounds},
	pages = {1591--1646},
	file = {Full Text PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/ICUAN7CB/Audibert - 2009 - Fast learning rates in statistical inference throu.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/HIZAGFQH/1245332827.html:text/html}
}

@article{mcallester_pac-bayesian_2013,
	title = {A {PAC}-{Bayesian} {Tutorial} with {A} {Dropout} {Bound}},
	url = {http://arxiv.org/abs/1307.2118},
	abstract = {This tutorial gives a concise overview of existing PAC-Bayesian theory focusing on three generalization bounds. The first is an Occam bound which handles rules with finite precision parameters and which states that generalization loss is near training loss when the number of bits needed to write the rule is small compared to the sample size. The second is a PAC-Bayesian bound providing a generalization guarantee for posterior distributions rather than for individual rules. The PAC-Bayesian bound naturally handles infinite precision rule parameters, \$L\_2\$ regularization, \{{\textbackslash}em provides a bound for dropout training\}, and defines a natural notion of a single distinguished PAC-Bayesian posterior distribution. The third bound is a training-variance bound --- a kind of bias-variance analysis but with bias replaced by expected training loss. The training-variance bound dominates the other bounds but is more difficult to interpret. It seems to suggest variance reduction methods such as bagging and may ultimately provide a more meaningful analysis of dropouts.},
	urldate = {2018-07-04},
	journal = {arXiv:1307.2118 [cs]},
	author = {McAllester, David},
	month = jul,
	year = {2013},
	note = {arXiv: 1307.2118},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1307.2118 PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/WDW2SAZE/McAllester - 2013 - A PAC-Bayesian Tutorial with A Dropout Bound.pdf:application/pdf;arXiv.org Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/MSE4LYRD/1307.html:text/html}
}

@article{mcallester_concentration_2003,
	title = {Concentration inequalities for the missing mass and for histogram rule error},
	volume = {4},
	number = {Oct},
	journal = {Journal of Machine Learning Research},
	author = {McAllester, David and Ortiz, Luis},
	year = {2003},
	pages = {895--911},
	file = {Fulltext:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/CN8STHM4/McAllester y Ortiz - 2003 - Concentration inequalities for the missing mass an.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/SL5PUQ2N/mcallester03a.html:text/html}
}

@incollection{meyer_sur_1978,
	title = {Sur le {Lemme} de {La} {Vallee} {Poussin} et un theoreme de {Bismut}},
	booktitle = {Séminaire de {Probabilités} {XII}},
	publisher = {Springer},
	author = {Meyer, Paul-André},
	year = {1978},
	pages = {770--774},
	file = {Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/VQWX6W6W/Meyer - 1978 - Sur le Lemme de La Vallee Poussin et un theoreme d.pdf:application/pdf}
}

@article{van_de_geer_new_1987,
	title = {A {New} {Approach} to {Least}-{Squares} {Estimation}, with {Applications}},
	volume = {15},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176350362},
	doi = {10.1214/aos/1176350362},
	abstract = {The regression model y=g(x)+εy=g(x)+ε{\textbackslash}mathbf\{y\} = g({\textbackslash}mathbf\{x\}) + {\textbackslash}mathbf\{{\textbackslash}varepsilon\} and least-squares estimation are studied in a general context. By making use of empirical process theory, it is shown that entropy conditions on the class 𝒢G{\textbackslash}mathscr\{G\} of possible regression functions imply L2L2L{\textasciicircum}2-consistency of the least-squares estimator ĝ ng{\textasciicircum}n{\textbackslash}hat\{{\textbackslash}mathbf\{g\}\}\_n of ggg. This result is applied in parametric and nonparametric regression.},
	language = {EN},
	number = {2},
	urldate = {2018-07-24},
	journal = {The Annals of Statistics},
	author = {Van de Geer, Sara},
	month = jun,
	year = {1987},
	mrnumber = {MR888427},
	zmnumber = {0625.62046},
	keywords = {Consistency, empirical measure, entropy, uniform convergence},
	pages = {587--602},
	file = {Full Text PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/H4L9KFDD/Geer - 1987 - A New Approach to Least-Squares Estimation, with A.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/V7RKJFCF/1176350362.html:text/html}
}

@article{lederer_new_2014,
	title = {New concentration inequalities for suprema of empirical processes},
	volume = {20},
	issn = {1350-7265},
	url = {https://projecteuclid.org/euclid.bj/1411134452},
	doi = {10.3150/13-BEJ549},
	abstract = {While effective concentration inequalities for suprema of empirical processes exist under boundedness or strict tail assumptions, no comparable results have been available under considerably weaker assumptions. In this paper, we derive concentration inequalities assuming only low moments for an envelope of the empirical process. These concentration inequalities are beneficial even when the envelope is much larger than the single functions under consideration.},
	language = {EN},
	number = {4},
	urldate = {2018-07-24},
	journal = {Bernoulli},
	author = {Lederer, Johannes and Van de Geer, Sara},
	month = nov,
	year = {2014},
	mrnumber = {MR3263097},
	zmnumber = {1355.60026},
	keywords = {empirical processes, chaining, concentration inequalities, deviation inequalities, rate of convergence},
	pages = {2020--2038},
	file = {Full Text PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/FSCDM2MZ/Lederer y Geer - 2014 - New concentration inequalities for suprema of empi.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/BR9MDIXK/1411134452.html:text/html}
}

@phdthesis{audibert_theorie_2004,
	title = {Théorie statistique de l'apprentissage : une approche {PAC}-{Bayésienne}},
	shorttitle = {Théorie statistique de l'apprentissage},
	url = {http://www.theses.fr/2004PA066003},
	urldate = {2018-07-30},
	school = {Paris 6},
	author = {Audibert, Jean-Yves},
	month = jan,
	year = {2004},
	file = {Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/D5MJ3H7I/2004PA066003.html:text/html}
}

@article{haussler_bounds_1994,
	title = {Bounds on the sample complexity of {Bayesian} learning using information theory and the {VC} dimension},
	volume = {14},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1007/BF00993163},
	doi = {10.1007/BF00993163},
	abstract = {In this paper we study a Bayesian or average-case model of concept learning with a twofold goal: to provide more precise characterizations of learning curve (sample complexity) behavior that depend on properties of both the prior distribution over concepts and the sequence of instances seen by the learner, and to smoothly unite in a common framework the popular statistical physics and VC dimension theories of learning curves. To achieve this, we undertake a systematic investigation and comparison of two fundamental quantities in learning and information theory: the probability of an incorrect prediction for an optimal learning algorithm, and the Shannon information gain. This study leads to a new understanding of the sample complexity of learning in several existing models.},
	language = {en},
	number = {1},
	urldate = {2018-07-30},
	journal = {Machine Learning},
	author = {Haussler, David and Kearns, Michael and Schapire, Robert E.},
	month = jan,
	year = {1994},
	pages = {83--113},
	file = {Full Text PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/KBZVB4KM/Haussler et al. - 1994 - Bounds on the sample complexity of Bayesian learni.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/Q2HKCQKV/BF00993163.html:text/html}
}

@article{haussler_mutual_1997,
	title = {Mutual information, metric entropy and cumulative relative entropy risk},
	volume = {25},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1030741081},
	doi = {10.1214/aos/1030741081},
	abstract = {Assume Pθ:θϵΘPθ:θϵΘ\{P\_\{{\textbackslash}theta\}: {\textbackslash}theta {\textbackslash}epsilon {\textbackslash}Theta\} is a set of probability distributions with a common dominating measure on a complete separable metric space Y. A state θ∗ϵΘθ∗ϵΘ{\textbackslash}theta{\textasciicircum}* {\textbackslash}epsilon {\textbackslash}Theta is chosen by Nature. A statistician obtains n independent observations Y1,…,YnY1,…,YnY\_1, {\textbackslash}dots, Y\_n from Y distributed according to Pθ∗Pθ∗P\_\{{\textbackslash}theta{\textasciicircum}*\}. For each time t between 1 and n, based on the observations Y1,…,Yt−1Y1,…,Yt−1Y\_1, {\textbackslash}dots, Y\_\{t-1\}, the statistician produces an estimated distribution P̂ tP{\textasciicircum}t{\textbackslash}hat\{P\}\_t for Pθ∗Pθ∗P\_\{{\textbackslash}theta{\textasciicircum}*\} and suffers a loss L(Pθ∗,P̂ t)L(Pθ∗,P{\textasciicircum}t)L(P\_\{{\textbackslash}theta{\textasciicircum}*\}, {\textbackslash}hat\{P\}\_t). The cumulative risk for the statistician is the average total loss up to time n. Of special interest in information theory, data compression, mathematical finance, computational learning theory and statistical mechanics is the special case when the loss L(Pθ∗,P̂ t)L(Pθ∗,P{\textasciicircum}t)L(P\_\{{\textbackslash}theta{\textasciicircum}*\}, {\textbackslash}hat\{P\}\_t) is the relative entropy between the true distribution Pθ∗Pθ∗P\_\{{\textbackslash}theta{\textasciicircum}*\} and the estimated distribution P̂ tP{\textasciicircum}t{\textbackslash}hat\{P\}\_t. Here the cumulative Bayes risk from time 1 to n is the mutual information between the random parameter Θ∗Θ∗{\textbackslash}Theta{\textasciicircum}* and the observations Y1,…,YnY1,…,YnY\_1, {\textbackslash}dots, Y\_n. New bounds on this mutual information are given in terms of the Laplace transform of the Hellinger distance between pairs of distributions indexed by parameters in ΘΘ{\textbackslash}Theta. From these, bounds on the cumulative minimax risk are given in terms of the metric entropy of ΘΘ{\textbackslash}Theta with respect to the Hellinger distance. The assumptions required for these bounds are very general and do not depend on the choice of the dominating measure. They apply to both finite- and infinite-dimensional ΘΘ{\textbackslash}Theta. They apply in some cases where Y is infinite dimensional, in some cases where Y is not compact, in some cases where the distributions are not smooth and in some parametric cases where asymptotic normality of the posterior distribution fails.},
	language = {en},
	number = {6},
	urldate = {2018-07-30},
	journal = {The Annals of Statistics},
	author = {Haussler, David and Opper, Manfred},
	month = dec,
	year = {1997},
	mrnumber = {MR1604481},
	zmnumber = {0920.62007},
	keywords = {density estimation, Bayes risk, Hellinger distance, Kullback-Leibler distance, metric entropy, minimax risk, Mutual information, relative entropy},
	pages = {2451--2492},
	file = {Full Text PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/KJIBFBFV/Haussler y Opper - 1997 - Mutual information, metric entropy and cumulative .pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/6W2S3V57/1030741081.html:text/html}
}

@book{cesa-bianchi_prediction_2006,
	title = {Prediction, learning, and games},
	publisher = {Cambridge university press},
	author = {Cesa-Bianchi, Nicolo and Lugosi, Gábor},
	year = {2006},
	file = {Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/DFIYKA49/books.html:text/html}
}

@article{boucheron_theory_2005,
	title = {Theory of {Classification}: a {Survey} of {Some} {Recent} {Advances}},
	volume = {9},
	issn = {1292-8100, 1262-3318},
	shorttitle = {Theory of {Classification}},
	url = {https://www.cambridge.org/core/journals/esaim-probability-and-statistics/article/theory-of-classification-a-survey-of-some-recent-advances/42A9912D17169A650AB06244820464BC},
	doi = {10.1051/ps:2005018},
	abstract = {The last few years have witnessed important new developments in
the theory and practice of pattern classification. We intend to
survey some of the main new ideas that have led to these
recent results.},
	language = {en},
	urldate = {2018-07-31},
	journal = {ESAIM: Probability and Statistics},
	author = {Boucheron, Stéphane and Bousquet, Olivier and Lugosi, Gábor},
	month = nov,
	year = {2005},
	keywords = {empirical processes, Pattern recognition, concentration inequalities, {\textless}div class="kwd"{\textgreater}{\textless}/div{\textgreater}, model selection., statistical learning theory},
	pages = {323--375},
	file = {Full Text PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/DSBN6G2H/Boucheron et al. - 2005 - Theory of Classification a Survey of Some Recent .pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/67V4SIU7/42A9912D17169A650AB06244820464BC.html:text/html}
}

@inproceedings{mcallester_pac-bayesian_1998,
	address = {New York, NY, USA},
	series = {{COLT}' 98},
	title = {Some {PAC}-{Bayesian} {Theorems}},
	isbn = {978-1-58113-057-7},
	url = {http://doi.acm.org/10.1145/279943.279989},
	doi = {10.1145/279943.279989},
	urldate = {2018-07-31},
	booktitle = {Proceedings of the {Eleventh} {Annual} {Conference} on {Computational} {Learning} {Theory}},
	publisher = {ACM},
	author = {McAllester, David A.},
	year = {1998},
	pages = {230--234}
}

@article{gine_limit_1984,
	title = {Some {Limit} {Theorems} for {Empirical} {Processes}},
	volume = {12},
	issn = {0091-1798, 2168-894X},
	url = {https://projecteuclid.org/euclid.aop/1176993138},
	doi = {10.1214/aop/1176993138},
	abstract = {In this paper we provide a general framework for the study of the central limit theorem (CLT) for empirical processes indexed by uniformly bounded families of functions ℱF{\textbackslash}mathscr\{F\}. From this we obtain essentially all known results for the CLT in this case; we improve Dudley's (1982) theorem on entropy with bracketing and Kolcinskii's (1981) CLT under random entropy conditions. One of our main results is that a combinatorial condition together with the existence of the limiting Gaussian process are necessary and sufficient for the CLT for a class of sets (modulo a measurability condition). The case of unbounded ℱF{\textbackslash}mathscr\{F\} is also considered; a general CLT as well as necessary and sufficient conditions for the law of large numbers are obtained in this case. The results for empiricals also yield some new CLT's in C[0,1]C[0,1]C{\textbackslash}lbrack 0, 1{\textbackslash}rbrack and D[0,1]D[0,1]D{\textbackslash}lbrack 0, 1{\textbackslash}rbrack.},
	language = {EN},
	number = {4},
	urldate = {2018-08-02},
	journal = {The Annals of Probability},
	author = {Giné, Evarist and Zinn, Joel},
	month = nov,
	year = {1984},
	mrnumber = {MR757767},
	zmnumber = {0553.60037},
	keywords = {empirical processes, metric entropy, Central limit theorems, functional Donsker classes, Gaussian processes, laws of large numbers},
	pages = {929--989},
	file = {Full Text PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/SXZHLECF/Gine y Zinn - 1984 - Some Limit Theorems for Empirical Processes.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/7SIL7GG7/1176993138.html:text/html}
}

@book{talagrand_upper_2014,
	address = {Berlin Heidelberg},
	series = {Ergebnisse der {Mathematik} und ihrer {Grenzgebiete}. 3. {Folge} / {A} {Series} of {Modern} {Surveys} in {Mathematics}},
	title = {Upper and {Lower} {Bounds} for {Stochastic} {Processes}: {Modern} {Methods} and {Classical} {Problems}},
	isbn = {978-3-642-54074-5},
	shorttitle = {Upper and {Lower} {Bounds} for {Stochastic} {Processes}},
	url = {//www.springer.com/la/book/9783642540745},
	abstract = {The book develops modern methods and in particular the "generic chaining" to bound stochastic processes. This methods allows in particular to get optimal bounds for Gaussian and Bernoulli processes. Applications are given to stable processes, infinitely divisible processes, matching theorems, the convergence of random Fourier series, of orthogonal series, and to functional analysis. The complete solution of a number of classical problems is given in complete detail, and an ambitious program for future research is laid out.},
	language = {en},
	urldate = {2018-08-02},
	publisher = {Springer-Verlag},
	author = {Talagrand, Michel},
	year = {2014},
	file = {Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/8KKHVZT9/9783642540745.html:text/html}
}

@book{vapnik_statistical_1998,
	title = {Statistical learning theory. 1998},
	volume = {3},
	publisher = {Wiley, New York},
	author = {Vapnik, Vladimir},
	year = {1998}
}

@article{bahr_inequalities_1965,
	title = {Inequalities for the \$r\$th {Absolute} {Moment} of a {Sum} of {Random} {Variables}, \$1 {\textbackslash}leqq r {\textbackslash}leqq 2\$},
	volume = {36},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/euclid.aoms/1177700291},
	doi = {10.1214/aoms/1177700291},
	abstract = {Let X1,X2,⋯,XnX1,X2,⋯,XnX\_1, X\_2, {\textbackslash}cdots, X\_n be a sequence of random variables (r.v.'s) and put Sm=∑mν=1Xν,1≦m≦nSm=∑ν=1mXν,1≦m≦nS\_m = {\textbackslash}sum{\textasciicircum}m\_\{{\textbackslash}nu = 1\} X\_{\textbackslash}nu, 1 {\textbackslash}leqq m {\textbackslash}leqq n. It is well-known that E{\textbar}Sn{\textbar}r≦nr−1∑ν=1nE{\textbar}Xν{\textbar}rr{\textgreater}1,((1))((1))E{\textbar}Sn{\textbar}r≦nr−1∑ν=1nE{\textbar}Xν{\textbar}rr{\textgreater}1,{\textbackslash}begin\{equation*\}{\textbackslash}tag\{(1)\}E{\textbar}S\_n{\textbar}{\textasciicircum}r {\textbackslash}leqq n{\textasciicircum}\{r - 1\} {\textbackslash}sum{\textasciicircum}n\_\{{\textbackslash}nu = 1\} E{\textbar}X\_{\textbackslash}nu{\textbar}{\textasciicircum}r{\textbackslash}quad r {\textgreater} 1,{\textbackslash}end\{equation*\} E{\textbar}Sn{\textbar}r≦∑nν=1E{\textbar}X{\textbar}nu{\textbar}r,r≦1.E{\textbar}Sn{\textbar}r≦∑ν=1nE{\textbar}X{\textbar}nu{\textbar}r,r≦1.E{\textbar}S\_n{\textbar}{\textasciicircum}r {\textbackslash}leqq {\textbackslash}sum{\textasciicircum}n\_\{{\textbackslash}nu = 1\} E{\textbar}X\_{\textbar}nu{\textbar}{\textasciicircum}r,{\textbackslash}quad r {\textbackslash}leqq 1. However, if the r.v.'s satisfy the relations E(Xm+1∣Sm)=0a.s.1≦m≦n−1,(2)(2)E(Xm+1∣Sm)=0a.s.1≦m≦n−1,{\textbackslash}begin\{equation*\}{\textbackslash}tag\{2\}E(X\_\{m + 1\} {\textbackslash}mid S\_m) = 0 {\textbackslash}text\{a.s.\}{\textbackslash}quad 1 {\textbackslash}leqq m {\textbackslash}leqq n - 1,{\textbackslash}end\{equation*\} it is possible to improve the first inequality considerably. The case r{\textgreater}2r{\textgreater}2r {\textgreater} 2 with independent r.v.'s will be treated elsewhere by one of the authors, von Bahr. If r=2r=2r = 2, we have, under (2), ES2n=∑ν=1nEX2ν.(3)(3)ESn2=∑ν=1nEXν2.{\textbackslash}begin\{equation*\}{\textbackslash}tag\{3\}ES{\textasciicircum}2\_n = {\textbackslash}sum{\textasciicircum}n\_\{{\textbackslash}nu = 1\} EX{\textasciicircum}2\_{\textbackslash}nu.{\textbackslash}end\{equation*\} In the case 1≦r≦21≦r≦21 {\textbackslash}leqq r {\textbackslash}leqq 2, we will show that under (2) E{\textbar}Sn{\textbar}r≦C(r,n)∑ν=1ne{\textbar}Xν{\textbar}r,((4))((4))E{\textbar}Sn{\textbar}r≦C(r,n)∑ν=1ne{\textbar}Xν{\textbar}r,{\textbackslash}begin\{equation*\}{\textbackslash}tag\{(4)\}E{\textbar}S\_n{\textbar}{\textasciicircum}r {\textbackslash}leqq C(r, n) {\textbackslash}sum{\textasciicircum}n\_\{{\textbackslash}nu = 1\}e{\textbar}X\_{\textbackslash}nu{\textbar}{\textasciicircum}r,{\textbackslash}end\{equation*\} where C(r,n)C(r,n)C(r, n) is a bounded function of rrr and nnn. In Theorem 2 we show that (4) is true with C(r,n)=2C(r,n)=2C(r, n) = 2. If the distribution of each Xm+1Xm+1X\_\{m + 1\} conditioned by SmSmS\_m is symmetric about zero, one can put C(r,n)=1C(r,n)=1C(r, n) = 1 (Theorem 1). Further, if the r.v.'s satisfy the following conditions E(Xi∣Rmi)=0a.s.1≦i≦m+1≦n,((5))((5))E(Xi∣Rmi)=0a.s.1≦i≦m+1≦n,{\textbackslash}begin\{equation*\}{\textbackslash}tag\{(5)\}E(X\_i {\textbackslash}mid R\_\{mi\}) = 0{\textbackslash}text\{a.s.\}{\textbackslash}quad 1 {\textbackslash}leqq i {\textbackslash}leqq m + 1 {\textbackslash}leqq n,{\textbackslash}end\{equation*\} where Rmi=∑m+1ν=1,ν≠iXνRmi=∑ν=1,ν≠im+1XνR\_\{mi\} = {\textbackslash}sum{\textasciicircum}\{m + 1\}\_\{{\textbackslash}nu = 1, {\textbackslash}nu {\textbackslash}neq i\} X\_{\textbackslash}nu it is possible to put C(r,n)=2−n−1C(r,n)=2−n−1C(r, n) = 2 - n{\textasciicircum}\{-1\}. The conditions (2) and (5) are fulfilled if the r.v.'s are independent and have zero means. In this case, however, it is possible to make C(r,n)C(r,n)C(r, n) dependent on rrr, so that C(r,n)→1C(r,n)→1C(r, n) {\textbackslash}rightarrow 1 as r→2r→2r {\textbackslash}rightarrow 2. It is possible to show by an example, that (4) is not generally true with C(r,n)=1C(r,n)=1C(r, n) = 1 even in this case. If 1≦r{\textless}s≦21≦r{\textless}s≦21 {\textbackslash}leqq r {\textless} s {\textbackslash}leqq 2 and E{\textbar}Xν{\textbar}s{\textless}∞,1≦ν≦nE{\textbar}Xν{\textbar}s{\textless}∞,1≦ν≦nE{\textbar}X\_{\textbackslash}nu{\textbar}{\textasciicircum}s {\textless} {\textbackslash}infty, 1 {\textbackslash}leqq {\textbackslash}nu {\textbackslash}leqq n, it is generally better not to use (4) directly, but to use it together with E{\textbar}Sn{\textbar}r≦(E{\textbar}Sn{\textbar}s)r/sE{\textbar}Sn{\textbar}r≦(E{\textbar}Sn{\textbar}s)r/sE{\textbar}S\_n{\textbar}{\textasciicircum}r {\textbackslash}leqq (E{\textbar}S\_n{\textbar}{\textasciicircum}s){\textasciicircum}\{r/s\}, so that E{\textbar}Sn{\textbar}r≦(C(s,n)∑nν=1E{\textbar}Xν{\textbar}s)r/sE{\textbar}Sn{\textbar}r≦(C(s,n)∑ν=1nE{\textbar}Xν{\textbar}s)r/sE{\textbar}S\_n{\textbar}{\textasciicircum}r {\textbackslash}leqq (C(s, n) {\textbackslash}sum{\textasciicircum}n\_\{{\textbackslash}nu = 1\} E{\textbar}X\_{\textbackslash}nu{\textbar}{\textasciicircum}s){\textasciicircum}\{r/s\}. The case r{\textless}1r{\textless}1r {\textless} 1 is by (1) trivial.},
	language = {EN},
	number = {1},
	urldate = {2018-08-03},
	journal = {The Annals of Mathematical Statistics},
	author = {Bahr, Bengt von and Esseen, Carl-Gustav},
	month = feb,
	year = {1965},
	mrnumber = {MR170407},
	zmnumber = {0134.36902},
	pages = {299--303},
	file = {Full Text PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/BTKDFQSP/Bahr y Esseen - 1965 - Inequalities for the \$r\$th Absolute Moment of a Su.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/YTNUU85K/1177700291.html:text/html}
}

@book{rao_theory_1991,
	title = {Theory of {Orlicz} spaces},
	publisher = {M. Dekker New York},
	author = {Rao, Malempati Madhusudana and Ren, Zhong Dao},
	year = {1991}
}

@inproceedings{mendelson_learning_2014,
	title = {Learning without concentration},
	booktitle = {Conference on {Learning} {Theory}},
	author = {Mendelson, Shahar},
	year = {2014},
	pages = {25--39},
	file = {Fulltext:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/LFIZDTKF/Mendelson - 2014 - Learning without concentration.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/6IVRNSQ9/Mendelson - 2014 - Learning without concentration.pdf:application/pdf}
}

@article{cortes_relative_2013,
	title = {Relative {Deviation} {Learning} {Bounds} and {Generalization} with {Unbounded} {Loss} {Functions}},
	url = {http://arxiv.org/abs/1310.5796},
	abstract = {We present an extensive analysis of relative deviation bounds, including detailed proofs of two-sided inequalities and their implications. We also give detailed proofs of two-sided generalization bounds that hold in the general case of unbounded loss functions, under the assumption that a moment of the loss is bounded. These bounds are useful in the analysis of importance weighting and other learning tasks such as unbounded regression.},
	urldate = {2018-08-10},
	journal = {arXiv:1310.5796 [cs]},
	author = {Cortes, Corinna and Greenberg, Spencer and Mohri, Mehryar},
	month = oct,
	year = {2013},
	note = {arXiv: 1310.5796},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1310.5796 PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/M3PDNWHU/Cortes et al. - 2013 - Relative Deviation Learning Bounds and Generalizat.pdf:application/pdf;arXiv.org Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/UASLKKU6/1310.html:text/html}
}

@article{diestel_uniform_1991,
	title = {Uniform integrability: an introduction},
	shorttitle = {Uniform integrability},
	author = {Diestel, Joe},
	year = {1991},
	file = {Fulltext:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/EMSYIPRD/Diestel - 1991 - Uniform integrability an introduction.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/NKL93A5A/Diestel - 1991 - Uniform integrability an introduction.pdf:application/pdf}
}

@article{talagrand_sharper_1994,
	title = {Sharper {Bounds} for {Gaussian} and {Empirical} {Processes}},
	volume = {22},
	issn = {0091-1798, 2168-894X},
	url = {https://projecteuclid.org/euclid.aop/1176988847},
	doi = {10.1214/aop/1176988847},
	abstract = {Under natural conditions on a class ℱF{\textbackslash}mathscr\{F\} of functions on a probability space, near optimal bounds are given for the probabilities P(supf∈ℱ{\textbar}∑i≤nf(Xi)−nE(f){\textbar}≥Mn‾√)P(supf∈F{\textbar}∑i≤nf(Xi)−nE(f){\textbar}≥Mn)P{\textbackslash}big({\textbackslash}sup\_\{f{\textbackslash}in{\textbackslash}mathscr\{F\}\}{\textbar}{\textbackslash}sum\_\{i{\textbackslash}leq n\} f(X\_i) - nE(f){\textbar} {\textbackslash}geq M{\textbackslash}sqrt n{\textbackslash}big). The method is a variation of this author's method to study the tail probability of the supremum of a Gaussian process.},
	language = {EN},
	number = {1},
	urldate = {2018-08-14},
	journal = {The Annals of Probability},
	author = {Talagrand, M.},
	month = jan,
	year = {1994},
	mrnumber = {MR1258865},
	zmnumber = {0798.60051},
	keywords = {isoperimetric inequalities, tail probabilities, Uniform approximation},
	pages = {28--76},
	file = {Full Text PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/WVUYDMX6/Talagrand - 1994 - Sharper Bounds for Gaussian and Empirical Processe.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/HADREJ7K/1176988847.html:text/html}
}

@article{mammen_smooth_1999,
	title = {Smooth discrimination analysis},
	volume = {27},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1017939240},
	doi = {10.1214/aos/1017939240},
	abstract = {Discriminant analysis for two data sets in ℝdRd{\textbackslash}mathbb\{R\}{\textasciicircum}d with probability densities fff and ggg can be based on the estimation of the set G=\{x:f(x)≥g(x)\}G=\{x:f(x)≥g(x)\}G = {\textbackslash}\{x: f(x) {\textbackslash}geq g(x){\textbackslash}\}. We consider applications where it is appropriate to assume that the region GGG has a smooth boundary or belongs to another nonparametric class of sets. In particular, this assumption makes sense if discrimination is used as a data analytic tool. Decision rules based on minimization of empirical risk over the whole class of sets and over sieves are considered. Their rates of convergence are obtained. We show that these rules achieve optimal rates for estimation of GGG and optimal rates of convergence for Bayes risks. An interesting conclusion is that the optimal rates for Bayes risks can be very fast, in particular, faster than the “parametric” root-nnn rate. These fast rates cannot be guaranteed for plug-in rules.},
	language = {en},
	number = {6},
	urldate = {2018-08-14},
	journal = {The Annals of Statistics},
	author = {Mammen, Enno and Tsybakov, Alexandre B.},
	month = dec,
	year = {1999},
	mrnumber = {MR1765618},
	zmnumber = {0961.62058},
	keywords = {optimal rates, Bayes risk, Discrimination analysis, empirical risk, sieves},
	pages = {1808--1829},
	file = {Full Text PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/2R9R8PUQ/Mammen y Tsybakov - 1999 - Smooth discrimination analysis.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/44PUPGE9/1017939240.html:text/html}
}

@book{van_der_vaart_asymptotic_2000,
	title = {Asymptotic statistics},
	volume = {3},
	publisher = {Cambridge university press},
	author = {Van der Vaart, Aad W.},
	year = {2000},
	file = {Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/ZFS57S4Y/books.html:text/html}
}

@article{wong_probability_1995,
	title = {Probability {Inequalities} for {Likelihood} {Ratios} and {Convergence} {Rates} of {Sieve} {MLES}},
	volume = {23},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176324524},
	doi = {10.1214/aos/1176324524},
	abstract = {Let Y1,…,YnY1,…,YnY\_1,{\textbackslash}ldots, Y\_n be independent identically distributed with density p0p0p\_0 and let ℱF{\textbackslash}mathscr\{F\} be a space of densities. We show that the supremum of the likelihood ratios ∏ni=1p(Yi)/p0(Yi)∏i=1np(Yi)/p0(Yi){\textbackslash}prod{\textasciicircum}n\_\{i=1\} p(Y\_i)/p\_0(Y\_i), where the supremum is over p∈ℱp∈Fp {\textbackslash}in {\textbackslash}mathscr\{F\} with ‖p1/2−p1/20‖2≥ε‖p1/2−p01/2‖2≥ε{\textbackslash}{\textbar}p{\textasciicircum}\{1/2\} - p{\textasciicircum}\{1/2\}\_0{\textbackslash}{\textbar}\_2 {\textbackslash}geq {\textbackslash}varepsilon, is exponentially small with probability exponentially close to 1. The exponent is proportional to nε2nε2n{\textbackslash}varepsilon{\textasciicircum}2. The only condition required for this to hold is that εε{\textbackslash}varepsilon exceeds a value determined by the bracketing Hellinger entropy of ℱF{\textbackslash}mathscr\{F\}. A similar inequality also holds if we replace ℱF{\textbackslash}mathscr\{F\} by ℱnFn{\textbackslash}mathscr\{F\}\_n and p0p0p\_0 by qnqnq\_n, where qnqnq\_n is an approximation to p0p0p\_0 in a suitable sense. These results are applied to establish rates of convergence of sieve MLEs. Furthermore, weak conditions are given under which the "optimal" rate εnεn{\textbackslash}varepsilon\_n defined by H(εn,ℱ)=nε2nH(εn,F)=nεn2H({\textbackslash}varepsilon\_n, {\textbackslash}mathscr\{F\}) = n{\textbackslash}varepsilon{\textasciicircum}2\_n, where H(⋅,ℱ)H(⋅,F)H({\textbackslash}cdot, {\textbackslash}mathscr\{F\}) is the Hellinger entropy of ℱF{\textbackslash}mathscr\{F\}, is nearly achievable by sieve estimators.},
	language = {EN},
	number = {2},
	urldate = {2018-09-07},
	journal = {The Annals of Statistics},
	author = {Wong, Wing Hung and Shen, Xiaotong},
	month = apr,
	year = {1995},
	mrnumber = {MR1332570},
	zmnumber = {0829.62002},
	keywords = {Hellinger distance, bracketing metric entropy, exponential inequality, Kullback-Leibler number},
	pages = {339--362},
	file = {Full Text PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/WW6JPCQG/Wong y Shen - 1995 - Probability Inequalities for Likelihood Ratios and.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/ZUX36BFV/1176324524.html:text/html}
}

@article{onshuus_metric_2015,
	title = {Metric {Entropy} estimation using o-minimality {Theory}},
	journal = {arXiv preprint arXiv:1511.07098},
	author = {Onshuus, Alf and Quiroz, Adolfo J.},
	year = {2015},
	file = {Fulltext:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/QMMFABJF/Onshuus y Quiroz - 2015 - Metric Entropy estimation using o-minimality Theor.pdf:application/pdf;Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/3DEFCPVF/1511.html:text/html}
}

@article{dudley_metric_1974,
	title = {Metric entropy of some classes of sets with differentiable boundaries},
	volume = {10},
	number = {3},
	journal = {Journal of Approximation Theory},
	author = {Dudley, Richard M.},
	year = {1974},
	pages = {227--236},
	file = {Fulltext:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/DE5I4WZQ/Dudley - 1974 - Metric entropy of some classes of sets with differ.pdf:application/pdf}
}

@article{van_erven_pac-bayes_2014,
	title = {{PAC}-{Bayes} {Mini}-tutorial: {A} {Continuous} {Union} {Bound}},
	shorttitle = {{PAC}-{Bayes} {Mini}-tutorial},
	url = {http://arxiv.org/abs/1405.1580},
	abstract = {When I first encountered PAC-Bayesian concentration inequalities they seemed to me to be rather disconnected from good old-fashioned results like Hoeffding's and Bernstein's inequalities. But, at least for one flavour of the PAC-Bayesian bounds, there is actually a very close relation, and the main innovation is a continuous version of the union bound, along with some ingenious applications. Here's the gist of what's going on, presented from a machine learning perspective.},
	urldate = {2018-09-07},
	journal = {arXiv:1405.1580 [stat]},
	author = {van Erven, Tim},
	month = may,
	year = {2014},
	note = {arXiv: 1405.1580},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1405.1580 PDF:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/IECSG4NY/van Erven - 2014 - PAC-Bayes Mini-tutorial A Continuous Union Bound.pdf:application/pdf;arXiv.org Snapshot:/home/muriel/.zotero/zotero/eg7b4pml.default/zotero/storage/DW7WRDK6/1405.html:text/html}
}

@book{cover_elements_2006,
	address = {New York, NY, USA},
	series = {Wiley {Series} in {Telecommunications} and {Signal} {Processing}},
	title = {Elements of {Information} {Theory}},
	isbn = {978-0-471-24195-9},
	publisher = {Wiley-Interscience},
	author = {Cover, Thomas M. and Thomas, Joy A.},
	year = {2006}
}