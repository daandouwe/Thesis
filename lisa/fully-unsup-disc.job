#!/bin/bash
#SBATCH -N 1
#SBATCH -p normal
#SBATCH -J fully-unsup-disc
#SBATCH -o lisa/out/fully-unsup-disc.out
#SBATCH -t 3-00:00:00

# send an e-mail when the job starts
echo "Job $SLURM_JOB_NAME started at `date`" | mail $USER -s "Started job $SLURM_JOB_NAME"

# write sterr and stout of each experiment here
OUTPUT_DIR=${HOME}/thesis/lisa/out/${SLURM_JOB_NAME}
mkdir -p ${OUTPUT_DIR}

# always run from the main directory
cd ${HOME}/thesis

source lisa/lisa-setup.sh

# create supervised vocab
python src/main.py build @src/configs/vocab/supervised.txt

# we can run training runs in parallel, each on a different processor (16 in total)
# with a different seed, and we tell each script which seed
# all other arguments are passed to the parser
for seed in {1..10}; do

  # do not conflate the timestamped foldernames
  sleep 5

  lisa/train.sh ${seed} ${OUTPUT_DIR} \
      --dynet-autobatch 1 \
      --dynet-mem 4000 \
      --max-time $((3 * 23 * 3600)) \
      --model-path-base models/fully-unsup-disc \
      --model-type fully-unsup-disc \
      @src/configs/vocab/supervised.txt \
      @src/configs/data/supervised.txt \
      @src/configs/training/adam.txt \
      @src/configs/baseline/mlp.txt \
      --num-samples 1 \
      --batch-size 1 \
      --num-dev-samples 50 \
      --num-test-samples 100 \
      &

done

# this waits until all sub-jobs finish
wait

echo "Jobs finished"
echo "Job $SLURM_JOB_NAME ended at `date`" | mail $USER -s "Ended job $SLURM_JOB_NAME"

sleep 300
