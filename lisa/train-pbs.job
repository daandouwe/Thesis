#PBS -S /bin/bash
#PBS -lnodes=1
#PBS -lwalltime=48:00:00
#PBS -qgpu

module load eb
module load Python/3.6.3-foss-2017b
module load CUDA/9.0.176
module load cuDNN/7.0.5-CUDA-9.0.176

# send an e-mail when the job starts
echo "Job $PBS_JOBID started at `date`" | mail $USER -s "Started Job $PBS_JOBID"

# enter virtual environment
source ${HOME}/envs/parser/bin/activate

# change to directory of the parser
# EDIT
cd ${HOME}/git/parser

# we can run 4 training runs in parallel, each on a different GPU (0,1,2,3)
# we tell each script which GPU to use (GPU IDs start at 0) and provide a name
# all other arguments are passed to the parser
lisa/train.sh 0 baseline --dim 512 --pos_emb_dim 28 --num_layers 3 --lab_mlp_dim 128 --lr 0.001 --lr_decay 0.90 --lr_decay_steps 2500 --word_dropout_p 0.0 --pos_dropout_p 0.0 &
lisa/train.sh 1 baseline --dim 512 --pos_emb_dim 28 --num_layers 3 --lab_mlp_dim 128 --lr 0.001 --lr_decay 0.95 --lr_decay_steps 2500 --word_dropout_p 0.0 --pos_dropout_p 0.0 &
lisa/train.sh 2 baseline --dim 512 --pos_emb_dim 60 --num_layers 3 --lab_mlp_dim 128 --lr 0.001 --lr_decay 0.90 --lr_decay_steps 2500 --word_dropout_p 0.0 --pos_dropout_p 0.0 &
lisa/train.sh 3 baseline --dim 512 --pos_emb_dim 60 --num_layers 3 --lab_mlp_dim 128 --lr 0.001 --lr_decay 0.95 --lr_decay_steps 2500 --word_dropout_p 0.0 --pos_dropout_p 0.0 &

# this waits until all sub-jobs finish
wait

echo "Jobs finished"
echo "Job $PBS_JOBID $JOB_NAME ended at `date`" | mail $USER -s "Ended Job $PBS_JOBID $JOB_NAME"

sleep 300
