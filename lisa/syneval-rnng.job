#!/bin/bash
#SBATCH -N 1
#SBATCH -p normal
#SBATCH -J syneval-rnng
#SBATCH -o lisa/out/syneval-rnng.out
#SBATCH -t 5-00:00:00

module load Python/3.6.3-foss-2017b

# as not to tred on each other's toes
export MKL_NUM_THREADS=1

# the strategy here is different than for the lm syneval:
# the model is so slow that we subsample the test-set and instead
# evaluatue the same models repeatedly on random subsets.
GEN_PATH="models/gen-rnng_seed=2_dev=90.92"
PROP_PATH="models/disc-rnng_seed=1_dev=88.06"

# send an e-mail when the job starts
echo "Job $SLURM_JOB_NAME started at `date`." | mail $USER -s "Started job $SLURM_JOB_NAME"

# write sterr and stout of each experiment here
OUTPUT_DIR=${HOME}/thesis/lisa/out/${SLURM_JOB_NAME}
mkdir -p ${OUTPUT_DIR}

# always run from the main directory
cd ${HOME}/thesis

# evaluate the same model on different subsamples
for seed in {1..15}; do
  python src/main.py syneval \
      --dynet-autobatch 1 \
      --dynet-mem 3000 \
      --model-type gen-rnng \
	    --checkpoint ${GEN_PATH} \
	    --proposal-model ${PROP_PATH} \
	    --indir data/syneval/data/converted \
	    --num-samples 50 \
      --alpha 0.8 \
	    --syneval-max-lines 1000 \
      --numpy-seed ${seed} \
      &
done

# this waits until all sub-jobs finish
wait

echo "Jobs finished"
echo "Job $SLURM_JOB_NAME ended at `date`" | mail $USER -s "Ended job $SLURM_JOB_NAME"

sleep 300
